<?xml version="1.0" encoding="UTF-8"?><rss xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title><![CDATA[Untitled RSS Feed]]></title>
        <description><![CDATA[Untitled RSS Feed]]></description>
        <link>https://spion.github.io</link>
        <generator>RSS for Node</generator>
        <lastBuildDate>Tue, 19 Dec 2017 15:03:15 GMT</lastBuildDate>
        <atom:link href="https://spion.github.io/rss.xml" rel="self" type="application/rss+xml"/>
        <pubDate>Tue, 19 Dec 2017 15:03:10 GMT</pubDate>
        <item>
            <title><![CDATA[Machine learning ethics]]></title>
            <description><![CDATA[<p>Today I found and watched one of the most important videos on machine learning published this year</p>
<blockquote>
<p>We&#39;re building a dystopia just to make people click on ads
<a href="https://www.youtube.com/watch?v=iFTWM7HV2UI&amp;app=desktop">https://www.youtube.com/watch?v=iFTWM7HV2UI&amp;app=desktop</a></p>
</blockquote>
<p>Go watch it first before reading ahead! I could not possibly summarise it without doing it a
disservice.</p>
<p>What struck me most was the following quote:</p>
<blockquote>
<p>Having interviewed people who worked at Facebook, I&#39;m convinced that nobody there really
understands how it [the machine learning system] works.</p>
</blockquote>
<p>The important question is, howcome nobody understands how a machine learning system works? You
would think, its because the system is very complex, its hard for any one person to understand it
fully. Thats not the problem.</p>
<p>The problem is fundamental to machine learning systems.</p>
<p>A machine learning system is a program that is given a target goal, a list of actions,
a history of results and the personal information of the user.</p>
<p>The goal could be e.g. to maximise the time the user stays on a video sharing website. More
generally, a value function is given by the users that measures the desireability of a certain
outcome or behaviour (it could include multiple things like number of product bought, mnumber of
ads clicked or viewed, etc)</p>
<p>The list of possible actions, could be a list of videos it can show in the sidebar on the right.</p>
<p>The result is the input to the value function. How long did a certain user stay on the website
after they were shown certain content on the website? How many ads did they click? How many
products did they buy?</p>
<p>The personal history of a user could be, for example, which videos they&#39;ve watched in the past
few months, their gender, age and so on (many companies have a lot more).</p>
<p>Based on these results, the system learns how to tailor its actions (the videos it shows) so that
it causes that particular person in that particular situation to stay longer on the website.</p>
<p>At the beginning it will try random things. After several iterations, it will find things that
&quot;stick&quot; i.e. maximise the value. There are sophisticated techniques to get unstuck from local
maximums too, in order to find even bigger maximums.</p>
<p>Once trained and ran to determine actions in a certain situation, it will do some calculations and
conclude: &quot;well, when I encountered a situation like this other times, I tried these 5 options, and
one of those maximised that value I&#39;m told to maximise in most of the cases, so I&#39;ll do that&quot;.</p>
<p>Sure, there are ways to ask some ML systems why they made a decision after the fact, and they can
elaborate the variables that had the most effect. But before the algorithm gets the training data,
you <em>don&#39;t</em> know what it will decide - nobody does!</p>
<p>Remember there are thousands of people visiting this site every day, so the algorithm can try a
lot of stuff in a very short time. After a while it will start noticing certain patterns. For
example, it seems that people who generally watch cat videos will stay a lot longer if they are
given cat videos in their suggestion box.  Moreover, that will happen even if situations when they
are watching something else, like academic lecture material.</p>
<p>This raises a very important question - is the system behaving in an ethical manner? Is it ethical
to show cat videos to a person trying to study and nudge them towards wasting their time? Even that
is a fairly benign example. There are far worse examples mentioned in the TED talk above.</p>
<p>The root of the problem is the value function. Our systems are often blisfully unaware of any side
effects their decision may cause and blatantly disregard basic rules of behaviour that we take for
granted. They have no other values than the value they&#39;re maximizing. For them, the end justifies
the means. If the value function is maximized by manipulating people, preying on their insecurities,
making them scared, angry or sad - all of that is unimportant. If they find that the most effective
way to keep a person on the same page is to show them something that will render them unconscious,
they would happily do that.</p>
<p>So how do we make these systems ethical?</p>
<p>The first challenge is technical, and its the easiest one. How do we come up with a value function
that encodes additional basic values of of human ethics? Its easy as pie! You take a bunch of
ethicists, give them various situations and ask them to rate actions as ethical/unethical. Then once
you have enough data, you train the value function so that the system can learn some basic humanity.
All done. (If only things were that easy!)</p>
<p>The second challenge is a business one. How far are you willing to reduce your value maximisation
to be ethical? What to do if your competitor doesn&#39;t do that? What are the ethics of putting a
number on how much ethics you&#39;re willing to sacrifice for profits? (Spoiler alert: they&#39;re not
great)</p>
<p>One way to solve that is to have regulations for ethical behaviour of machine learning systems.
Such systems could be held responsible for unethical actions. If those actions are reported by
people, investigated by experts and found true in court, the company owning the ML system is held
liable. Unethical behaviour of machine learning systems shouldn&#39;t be <em>too</em> difficult to spot,
although getting evidence might prove difficult. Public pressure and exposure of companies seems
to help too. Perhaps we could make a machine learning systems that detects unethical behaviour and
call it the ML police. Citizens could agree to install the ML police add-on to help monitor
and aggregate behaviour of online ML systems. (If these suggestions look silly, its because they
are)</p>
<p>The third challenge is philosophical. Until now, philosophers were content with &quot;there is no right
answer, but there have been many thoughts on what exactly is ethical&quot;. They better get their act
together, because we&#39;ll need them to come up with a definite, quantifiable answer real soon.</p>
<p>On the more optimistic side, I hope that any generally agreed upon &quot;standard&quot; ethical system will
be better than none.</p>
]]></description>
            <link>https://spion.github.io/posts/machine-learning-ethics.html</link>
            <guid isPermaLink="true">https://spion.github.io/posts/machine-learning-ethics.html</guid>
            <pubDate>Tue, 19 Dec 2017 00:00:00 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[JavaScript isn't cancer]]></title>
            <description><![CDATA[<p>The last few days, I&#39;ve been thinking about what leads so many people to hate JavaScript.</p>
<p>JS is so quirky and unclean! Thats supposed to be the primary reason, but after working with a few other dynamic languages, I don&#39;t buy it. JS actually has a fairly small amount of quirks compared to other dynamic languages.</p>
<p>Just think about PHP&#39;s named functions, which are always in the global scope. Except when they are in namespaces (oh hi another concept), and then its kinda weird because <a href="https://stackoverflow.com/questions/13435051/relative-nested-namespaces-in-php">namespaces can be relative</a>. There are no first class named functions, but function expressions can be assigned to variables. Which must be prefixed with <code>$</code>. There are no real modules, or proper nestable scope - at least not for functions, which are always global. But nested functions only exist once the outer function is called!</p>
<p>In Ruby, blocks are like lambdas except when they are not, and you can pass a block explicitly or yield to the first block implicitly. But there are also lambdas, which are different. Modules are uselessly global, cannot be parameterised over other modules (without resorting to meta programming), and there are several ways to nest them: if you don&#39;t nest them lexically, <a href="https://cirw.in/blog/constant-lookup.html">the lookup rules become different</a>. And there are classes, with private variables, which are prefixed with <code>@</code>. I really don&#39;t get that sigil fetish.</p>
<p>The above examples are only scratching the surface.</p>
<p>And which are the most often cited problems of JavaScript? Implicit conversions (the wat talk), no large ints and hard to understand prototypical inheritance and <code>this</code> keyword. That doesn&#39;t look any worse than the above lists! Plus, the language (pre ES6) is very minimalistic. It has freeform records with prototypes, and closures with lexical scope. Thats it!</p>
<p>So this supposed &quot;quirkiness&quot; of JavaScript doesn&#39;t seem like a satisfactory explanation. There must be something else going on here, and I think I finally realized what that is.</p>
<p>JavaScript is seen as a &quot;low status&quot; language. A 10 day accident, a silly toy language for the browser that ought to be simple and easy to learn. To an extent this is true, largely thanks to the fact that there are very few distinct concepts to be learned.</p>
<p>However, those few concepts combine together into a package with a really good power-to-weight ratio. Additionally, the simplicity ensures that the language is malleable towards even more power (e.g. you can extend it with a type system and then you can <em>idiomatically</em> approximate some capabilities of algebraic sum types, like <a href="https://goo.gl/IkiZqx">making illegal states unrepresentable</a>).</p>
<p>The emphasis above is on <em>idiomatically</em> for a reason. This sort of extension is somehow perfectly normal in JavaScript. If you took Ruby and used its dictionary type to add a comparable feature, it has significantly lower likelyhood of being accepted by developers. Why? Because Ruby has standard ways of doing things. You should be using objects and classes, not hashes, to model most of your data. (*)</p>
<p>That was not the case with the simple pre-ES6 JavaScript. There was no module system to organize code. No classes system to hierarhically organize blueprints of things that hold state. Lack of basic standard library items, such as maps, sets, iterables, streams, promises. Lack of functions to manipulate existing data structures (dictionaries and arrays).</p>
<p>Combine sufficient power, simplicity/malleability, and the lack of the basic facilities. Add to this the fact that its the basic option in the browser, the most popular platform. What do you get? You get a TON of people working in it to extend it in various different ways. And they invent a TON of stuff!</p>
<p>We ended up with several popular module systems (object based namespaces, CommonJS, AMD, ES6, the angular module system, etc) as well as many package managers to manage these modules (npm, bower, jspm, ...). We also got many object/inheritance systems: plain objects, pure prototype extension, simulating classes, <a href="https://github.com/stampit-org/stampit">&quot;composable object factories&quot;</a>, and so on and so forth. Heck, a while ago every other library used to implement its own class system! <small>(That is, until CoffeeScript came and gave the definite answer on how to implement classes on top of prototypes. This is interesting, and I&#39;ll come back to it later.)</small></p>
<p>This creates dissonance with the language&#39;s simplicity. JavaScript is this simple browser language that was supposed to be easy, so why is it so hard? Why are there so many things built on top of it and how the heck do I choose which one to use? I hate it. Why do I hate it? Probably its all these silly quirks that it has! Just look at its implicit conversions and lack of number types other than doubles!</p>
<p>It doesn&#39;t matter that many languages are much worse. A great example of the reverse phenomenon is C++. Its a <a href="http://yosefk.com/c++fqa/">complete abomination</a>, far worse than JavaScript - a Frankenstein in the languages domain. But its seen as &quot;high status&quot;, so it has many apologists that will come to defend its broken design: &quot;Yeah, C++ is a serious language, you need grown-up pants to use it&quot;. Unfortunately JS has no such luck: its status as a hack-together glue for the web pages seems to have been forever cemented in people&#39;s heads.</p>
<p>So how do we fix this? You might not realize it, but this is already being fixed as we speak! Remember how CoffeeScript slowed down the prolification of custom object systems? Browsers and environments are quickly implementing ES6, which standardizes a huge percentage of what used to be the JS wild west. We now have the standard way to do modules, the standard way to do classes, the standard way to do basic procedural async (Promises; async/await). The standard way to do bundling will probably be no-bundling: <a href="https://esdiscuss.org/topic/fwd-are-es6-modules-in-browsers-going-to-get-loaded-level-by-level#content-4">HTTP2 push + ES6 modules will &quot;just work&quot;</a>!</p>
<p>Finally, I believe the people who think that JavaScript will always be transpiled are wrong. As ES6+ features get implemented in major browsers, more and more people will find the overhead of ES.Next to ES transpilers isn&#39;t worth it. This process will stop entirely at some point as the basics get fully covered.</p>
<p>At this point, I&#39;m hoping several things will happen. We&#39;ll finally get those big integers and number types that Brendan Eich has been promising. We&#39;ll have some more stuff on top of <a href="https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/SharedArrayBuffer">SharedArrayBuffer</a> to enable easier shared memory parallelism, perhaps even <a href="https://facebook.github.io/immutable-js/">immutable datastructures</a> that are <a href="https://developer.mozilla.org/en-US/docs/Web/API/Web_Workers_API/Using_web_workers#Passing_data_by_transferring_ownership_(transferable_objects)">transferable objects</a>. The wat talk will be obsolete: obviously, you&#39;d be using a static analysis tool such as <a href="https://flowtype.org/">Flow</a> or TypeScript to deal with that; the fact that the browser ignores those type annotations and does its best to interpret what you meant will be irrelevant. async/await will be implemented in all browsers as the de-facto way to do async control flow; perhaps even <a href="https://github.com/tc39/proposal-async-iteration">async iterators</a> too. We&#39;ll also have widly accepted standard libraries for <a href="https://github.com/whatwg/streams">data</a> and event streams.</p>
<p>Will JavaScript finally gain the status it deserves then? Probably. But at what cost? JavaScript is big enough now that there is less space for new inventions. And its fun to invent new things and read about other people&#39;s inventions!</p>
<p>On the other hand, maybe then we&#39;ll be able to focus on the stuff we&#39;re actually building instead.</p>
<p><small>(*) Or metaprogramming, but then everyone has to agree on the same metaprogramming. In JS,
everyone uses records, and they probably use a tag field to discriminate them already: its a small step to add types for that.</small></p>
]]></description>
            <link>https://spion.github.io/posts/javascript-is-not-cancer.html</link>
            <guid isPermaLink="true">https://spion.github.io/posts/javascript-is-not-cancer.html</guid>
            <pubDate>Thu, 06 Oct 2016 00:00:00 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[ES7 async functions - a step in the wrong direction]]></title>
            <description><![CDATA[<p>Async functions are a new feature scheduled to become a part of ES7. They build
on top of previous capabilities made available by ES6 (promises), letting you
write async code as though it were synchronous. At the moment, they&#39;re a
<a href="https://github.com/lukehoban/ecmascript-asyncawait">stage 1 proposal for ES7</a> and supported by babel /
regenerator.</p>
<p>When generator functions were first made available in node, I was
<a href="https://spion.github.io/posts/analysis-generators-and-other-async-patterns-node.html">very excited</a>. Finally, a way to write asynchronous JavaScript that
doesn&#39;t descend into callback hell! At the time, I was unfamiliar with promises
and the language power you get back by simply having async computations be
first class values, so it seemed to me that generators are the best solution
available.</p>
<p>Turns out, they aren&#39;t. And the same limitations apply for async functions.</p>
<h3 id="predicates-in-catch-statements">Predicates in catch statements</h3>
<p>With generators, thrown errors bubble up the function chain until a catch
statement is encountered, much like in other languages that support exceptions.
On one hand, this is convenient, but on the other, you never know what you&#39;re
catching once you write a catch statement.</p>
<p>JavaScript catch doesn&#39;t support any mechanism to filter errors. This
limitation isn&#39;t too hard to get around: we can write a function <code>guard</code></p>
<pre><code class="lang-js">function guard(e, predicate) {
  if (!predicate(e)) throw e;
}
</code></pre>
<p>and then use it to e.g. only filter &quot;not found&quot; errors when downloading an
image</p>
<pre><code class="lang-js">try {
    await downloadImage(url);
} catch (e) { guard(e, e =&gt; e.code == 404);
    handle404(...);
}
</code></pre>
<p>But that only gets us so far. What if we want to have a second error handler?
We must resort to using <code>if-then-else</code>, making sure that we don&#39;t forget to
rethrow the error at the end</p>
<pre><code class="lang-js">try {
    await downloadImage(url);
} catch (e) {
    if (e.code == 404)  {
        handle404(...)
    } else if (e.code == 401) {
        handle401(...);
    } else {
        throw e;
    }
}
</code></pre>
<p>Since promises are a userland library, restrictions like the above do not
apply. We can write our own promise implementation that demands the use of a
predicate filter:</p>
<pre><code class="lang-js">downloadImage(url)
.catch(e =&gt; e.code == 404, e =&gt; {
    handle404(...);
})
.catch(e =&gt; e.code == 401, e =&gt; {
    handle401(...)
})
</code></pre>
<p>Now if we want all errors to be caught, we have to say it explicitly:</p>
<pre><code class="lang-js">asyncOperation()
.catch(e =&gt; true, e =&gt; {
    handleAllErrors(...)
});
</code></pre>
<p>Since these constructs are not built-in language features but a DSL built on
top of higher order functions, we can impose any restrictions we like instead
of waiting on TC39 to fix the language.</p>
<h3 id="cannot-use-higher-order-functions">Cannot use higher order functions</h3>
<p>Because generators and async-await are shallow, you cannot use <code>yield</code> or
<code>await</code> within lambdas passed to higher order functions.</p>
<p>This is <a href="https://github.com/tc39/ecmascript-asyncawait/issues/7">better explained here</a> - The example given
there is</p>
<pre><code class="lang-js">async function renderChapters(urls) {
  urls.map(getJSON).forEach(j =&gt; addToPage((await j).html));
}
</code></pre>
<p>and will not work, because you&#39;re not allowed to use await from within a nested
function. The following will work, but will execute in parallel:</p>
<pre><code class="lang-js">async function renderChapters(urls) {
  urls.map(getJSON).forEach(async j =&gt; addToPage((await j).html));
}
</code></pre>
<p>To understand why, you need to read <a href="http://calculist.org/blog/2011/12/14/why-coroutines-wont-work-on-the-web/">this article</a>. In short:
its much harder to implement deep coroutines so browser vendors probably wont
do it.</p>
<p>Besides being very unintuitive, this is also limiting. Higher order functions
are succint and powerful, yet we cannot <em>really</em> use them inside async
functions. To get sequential execution we have to resort to the clumsy built
in for loops which often force us into writing ceremonial, stateful code.</p>
<h3 id="arrow-functions-give-us-more-power-than-ever-before">Arrow functions give us more power than ever before</h3>
<p>Functional DSLs were very powerful even before JS had short lambda syntax. But
with arrow functions, things get even cleaner. The amount of code one needs to
write can be reduced greatly thanks to short lambda syntax and higher order
functions. Lets take the motivating example from the async-await proposal</p>
<pre><code class="lang-js">function chainAnimationsPromise(elem, animations) {
    var ret = null;
    var p = currentPromise;
    for(var anim of animations) {
        p = p.then(function(val) {
            ret = val;
            return anim(elem);
        })
    }
    return p.catch(function(e) {
        /* ignore and keep going */
    }).then(function() {
        return ret;
    });
}
</code></pre>
<p>With bluebird&#39;s <code>Promise.reduce</code>, this becomes</p>
<pre><code class="lang-js">function chainAnimationsPromise(elem, animations) {
  return Promise.reduce(animations,
      (lastVal, anim) =&gt; anim(elem).catch(_ =&gt; Promise.reject(lastVal)),
      Promise.resolve(null))
  .catch(lastVal =&gt; lastVal);
}
</code></pre>
<p>In short: functional DSLs are now more powerful than built in constructs,
even though (admittedly) they may take some getting used to.</p>
<hr>
<p>But this is not why async functions are a step in the wrong direction. The
problems above are not unique to async functions. The same problems apply to
generators: async functions merely inherit them as they&#39;re very similar.</p>
<p>Async functions also go another step backwards.</p>
<h2 id="loss-of-generality-and-power">Loss of generality and power</h2>
<p>Despite their shortcomings, generator based coroutines have one redeeming
quality: they allow you to redefine the coroutine execution engine. This is
extremely powerful, and I will demonstrate by giving the following example:</p>
<p>Lets say we were given the task to write the save function for an issue
tracker. The issue author can specify the issue&#39;s title and text, as well
as any other issues that are blocking the solution of the newly entered issue.</p>
<p>Our initial implementation is simple:</p>
<pre><code class="lang-js">async function saveIssue(data, blockers) {
    let issue = await Issues.insert(data);
    for (let blockerId of blockers) {
      await BlockerIssues.insert({blocker: blockerId, blocks: issue.id});
    }
}

Issues.insert = async function(data) {
    return db.query(&quot;INSERT ... VALUES&quot;, data).execWithin(db.pool);
}

BlockerIssue.insert = async function(data) {
    return db.query(&quot;INSERT .... VALUES&quot;, data).execWithin(db.pool);
}
</code></pre>
<p><code>Issue</code> and <code>BlockerIssues</code> are references to the corresponding tables in an
SQL database. Their <code>insert</code> methods return a promise that indicate whether
the query has been completed. The query is executed by a connection pool.</p>
<p>But then, we run into a problem. We don&#39;t want to partially save the issue if
some of the data was not inserted successfuly. We want the entire save
operation to be atomic. Fortunately, SQL databases support this via
transactions, and our database library has a transaction abstraction. So we
change our code:</p>
<pre><code class="lang-js">async function saveIssue(data, blockers) {
    let tx = db.beginTransaction();
    let issue = await Issue.insert(tx, data);
    for (let blockerId of blockers) {
      await BlockerIssues.insert(tx, {blocker: blockerId, blocks: issue.id});
    }
}

Issues.insert = async function(tx, data) {
    return db.query(&quot;INSERT ... VALUES&quot;, data).execWithin(tx);
}

BlockerIssue.insert = async function(tx, data) {
    return db.query(&quot;INSERT .... VALUES&quot;, data).execWithin(tx);
}
</code></pre>
<p>Here, we changed the code in two ways. Firstly, we created a transaction within
the saveIssue function. Secondly, we changed both insert methods to take this
transaction as an argument.</p>
<p>Immediately we can see that this solution doesn&#39;t scale very well. What if
we need to use <code>saveIssue</code> as a part of a larger transaction? Then it has to
take a transaction as an argument. Who will create the transactions? The top
level service. What if the top level service becomes a part of a larger
service? Then we need to change the code again.</p>
<p>We can reduce the extent of this problem by writing a base class that
automatically initializes a transaction if one is not passed via the
constructor, and then have <code>Issues</code>, <code>BlockerIssue</code> etc inherit from this
class.</p>
<pre><code>
class Transactionable {
    constructor(tx) {
        this.transaction = tx || db.beginTransaction();
    }
}
class IssueService extends Transactionable {
    async saveIssue(data, blockers) {
        issues = new Issues(this.transaction);
        blockerIssues = new BlockerIssues(this.transaction);
        ...
    }
}
class Issues extends Transactionable { ... }
class BlockerIssues extends Transactionable { ... }
// etc
</code></pre><p>Like many OO solutions, this only spreads the problem across the plate to make
it look smaller but doesn&#39;t solve it.</p>
<h2 id="generators-are-better">Generators are better</h2>
<p>Generators let us define the execution engine. The iteration is driven by the
function that consumes the generator, which decides what to do with the yielded
values. What if instead of only allowing promises, our engine let us also:</p>
<ol>
<li>Specify additional options which are accessible from within</li>
<li>Yield queries. These will be run in the transaction specified in the options
above</li>
<li>Yield other generator iterables: These will be run with the same engine and
options</li>
<li>Yield promises: These will be handled normally</li>
</ol>
<p>Lets take the original code and simplify it:</p>
<pre><code class="lang-js">
function* saveIssue(data, blockers) {
    let issue = yield Issues.insert(data);
    for (var blockerId of blockers) {
      yield BlockerIssues.insert({blocker: blockerId, blocks: issue.id});
    }
}

Issues.insert = function* (data) {
    return db.query(&quot;INSERT ... VALUES&quot;, data)
}

BlockerIssue.insert = function* (data) {
    return db.query(&quot;INSERT .... VALUES&quot;, data)
}
</code></pre>
<p>From our http handler, we can now write</p>
<pre><code class="lang-js">var myengine = require(&#39;./my-engine&#39;);

app.post(&#39;/issues/save&#39;, function(req, res) {
  myengine.run(saveIssue(data, blockers), {tx: db.beginTransaction()})
});
</code></pre>
<p>Lets implement this engine:</p>
<pre><code class="lang-js">function run(iterator, options) {
    function id(x) { return x; }
    function iterate(value) {
        var next = iterator.next(value)
        var request = next.value;
        var nextAction = next.done ? id : iterate;

        if (isIterator(request)) {
            return run(request, options).then(nextAction)
        }
        else if (isQuery(request)) {
            return request.execWithin(options.tx).then(nextAction)
        }
        else if (isPromise(request)) {
            return request.then(nextAction);
        }
    }
    return iterate()
}
</code></pre>
<p>The best part of this change is that we did not have to change the original
code at all. We didn&#39;t have to add the transaction parameter to every function,
to take care to properly propagate it everywhere and to properly create the
transaction. All we needed to do is just change our execution engine.</p>
<p>And we can add much more! We can <code>yield</code> a request to get the current user
if any, so we don&#39;t have to thread that through our code. Infact we can
implement <a href="https://github.com/othiym23/node-continuation-local-storage">continuation local storage</a> with only a few lines of code.</p>
<p>Async generators are often given as a reason why we need async functions. If
yield is already being used as await, how can we get both working at the same
time without adding a new keyword? Is that even possible?</p>
<p>Yes. Here is a simple proof-of-concept.
<a href="https://github.com/spion/async-generators">github.com/spion/async-generators</a>.
All we needed to do is change the execution engine to support a mechanism
to distinguish between awaited and yielded values.</p>
<p>Another example worth exploring is a query optimizer that supports aggregate
execution of queries. If we replace <code>Promise.all</code> with our own implementaiton
caled <code>parallel</code>, then we can add support for non-promise arguments.</p>
<p>Lets say we have the following code to notify owners of blocked issues in
parallel when an issue is resolved:</p>
<pre><code class="lang-js">let blocked = yield BlockerIssues.where({blocker: blockerId})
let owners  = yield engine.parallel(blocked.map(issue =&gt; issue.getOwner()))

for (let owner of owners) yield owner.notifyResolved(issue)
</code></pre>
<p>Instead of returning an SQL based query, we can have <code>getOwner()</code> return data
about the query:</p>
<pre><code class="lang-js">{table: &#39;users&#39;, id: issue.user_id}
</code></pre>
<p>and have <code>engine</code> optimize the execution of parallel queries, by sending
a single query per table rather then per item.</p>
<pre><code class="lang-js">if (isParallelQuery(query)) {
    var results = _(query.items).groupBy(&#39;table&#39;)
      .map((items, t) =&gt; db.query(`select * from ${t} where id in ?`,
                                  items.map(it =&gt; it.id))
                .execWithin(options.tx)).toArray();
    Promise.all(results)
        .then(results =&gt; results.sort(byOrderOf(query.items)))
        .then(runNext)
}
</code></pre>
<p>And voila, we&#39;ve just implemented a query optimizer. It will fetch all issue
owners with a single query. If we add an SQL parser into the mix, it should
be possible to rewrite real SQL queries.</p>
<p>We can do something similar on the client too with GraphQL queries by aggregating
multiple individual queries.</p>
<p>And if we add support for iterators, the optimization becomes deep:
we would be able to aggregate queries that are several layers within other
generator functions,  In the above example, <code>getOwner()</code> could be another
generatator which produces a query for the user as a first result. Our
implementation of <code>parallel</code> will run all those getOwner() iterators and
consolidate their first queries into a single query. All this is done without
those functions knowing anything about it (thus, without breaking modularity).</p>
<p>Async functions cant let us do any of this. All we get is a single execution
engine that only knows how to await promises. To make matters worse, thanks
to the unfortunately short-sighted <a href="https://esdiscuss.org/topic/a-challenge-problem-for-promise-designers-was-re-futures">recursive thenable assimilation</a>
design decision, we can&#39;t simply create our own thenable that will support
the above extra features. If we try to do that, we will be
<a href="https://github.com/Reactive-Extensions/RxJS/issues/470">unable to safely use it with Promises</a>.
We&#39;re stuck with what we get by default in async functions, and
thats it.</p>
<p>Generators are JavaScript&#39;s programmable semicolons. Lets not take away that
power by taking away the programmability. Lets drop async/await and write our
own interpreters.</p>
]]></description>
            <link>https://spion.github.io/posts/es7-async-await-step-in-the-wrong-direction.html</link>
            <guid isPermaLink="true">https://spion.github.io/posts/es7-async-await-step-in-the-wrong-direction.html</guid>
            <pubDate>Sun, 23 Aug 2015 00:00:00 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[Why I am switching to promises]]></title>
            <description><![CDATA[<p>I&#39;m switching my node code from callbacks to promises. The reasons aren&#39;t
merely aesthetical, they&#39;re rather practical:</p>
<p><a name="throw-crash"></a></p>
<h3 id="throw-catch-vs-throw-crash">Throw-catch vs throw-crash</h3>
<p>We&#39;re all human. We make mistakes, and then JavaScript <code>throw</code>s an error. How
do callbacks punish that mistake? They crash your process!</p>
<blockquote>
<p>But spion, why don&#39;t you use domains?</p>
</blockquote>
<p>Yes, I could do that. I could <a href="http://nodejs.org/api/domain.html#domain_warning_don_t_ignore_errors">crash my process gracefully</a>
instead of letting it just crash. But its still a crash no matter what
lipstick you put on it. It still results with an inoperative worker. With
thousands of requests, 0.5% hitting a throwing path means over 50 process
shutdowns and most likely denial of service.</p>
<p>And guess what a user that hits an error does? Starts repeatedly refreshing
the page, thats what. The horror!</p>
<p>Promises are throw-safe. If an error is thrown in one of the <code>.then</code>
callbacks, only that single promise chain will die. I can also attach error or
&quot;finally&quot; handlers to do any clean up if necessary - transparently! The
process will happily continue to serve the rest of my users.</p>
<p>For more info see <a href="https://github.com/joyent/node/issues/5114">#5114</a> and <a href="https://github.com/joyent/node/issues/5149">#5149</a>. To find out how promises
can solve this, see <a href="https://github.com/petkaantonov/bluebird/issues/51">bluebird #51</a></p>
<h3 id="-if-err-return-callback-err-"><code>if (err) return callback(err)</code></h3>
<p>That line is haunting me in my dreams now. What happened to the
<a href="dry">DRY principle</a>?</p>
<p>I understand that its important to explicitly handle all errors. But I don&#39;t
 believe its important to explicitly <em>bubble them up</em> the callback chain. If
I don&#39;t deal with the error here, thats because I can&#39;t deal with the error
there - I simply don&#39;t have enough context.</p>
<blockquote>
<p>But spion, why don&#39;t you wrap your calbacks?</p>
</blockquote>
<p>I guess I could do that and lose the callback stack when generating a
<code>new Error()</code>. Or since I&#39;m already wrapping things, why not wrap the
entire thing with promises, rely on longStackSupport, and handle errors at
my discretion?</p>
<p>Also, what happened to the <a href="dry">DRY principle</a>?</p>
<h3 id="promises-are-now-part-of-es6">Promises are now part of ES6</h3>
<p>Yes, they will become a part of the language. New DOM APIs will be using them
too. jQuery already switched to promise...ish things. Angular utilizes promises
everywhere (even in the templates). Ember uses promises. The list goes on.</p>
<p>Browser libraries already switched. I&#39;m switching too.</p>
<h3 id="containing-zalgo">Containing Zalgo</h3>
<p>Your promise library prevents you from <a href="http://blog.izs.me/post/59142742143/designing-apis-for-asynchrony">releasing Zalgo</a>. You
can&#39;t release Zalgo with promises. Its impossible for a promise to result with
the release of the Zalgo-beast. <a href="http://promisesaplus.com/#point-87">Promises are Zalgo-safe (see section
3.1)</a>.</p>
<h3 id="callbacks-getting-called-multiple-times">Callbacks getting called multiple times</h3>
<p>Promises solve that too. Once the operation is complete and the promise is
resolved (either with a result or with an error), it cannot be resolved again.</p>
<h3 id="promises-can-do-your-laundry">Promises can do your laundry</h3>
<p>Oops, unfortunately, promises wont do that. You still need to do it manually.</p>
<h2 id="but-you-said-promises-are-slow-">But you said promises are slow!</h2>
<p>Yes, I know I wrote that. But I was wrong. A month after I wrote
<a href="/posts/analysis-generators-and-other-async-patterns-node.html">the giant comparison of async patterns</a>, Petka Antonov wrote
<a href="https://github.com/petkaantonov/bluebird">Bluebird</a>. Its a wicked fast promise library, and here are the
charts to prove it:</p>
<p>Time to complete (ms)</p>
<div id="perf-time-promises" class="plot">
</div>

<p style="text-align:right">Parallel requests</p>

<script type="text/javascript">

window.perfTimePromises =

[ { label: 'callbacks-async-waterfall.js',
    data:
     [ [ '200', 19 ],
       [ '500', 31 ],
       [ '1000', 88 ],
       [ '2000', 146 ],
       [ '5000', 400 ],
       [ '10000', 881 ],
       [ '20000', 1826 ] ] },
  { label: 'promises-bluebird-generator.js',
    data:
     [ [ '200', 13 ],
       [ '500', 16 ],
       [ '1000', 34 ],
       [ '2000', 69 ],
       [ '5000', 202 ],
       [ '10000', 392 ],
       [ '20000', 891 ] ] },
  { label: 'promises-bluebird.js',
    data:
     [ [ '200', 12 ],
       [ '500', 20 ],
       [ '1000', 43 ],
       [ '2000', 96 ],
       [ '5000', 278 ],
       [ '10000', 538 ],
       [ '20000', 1021 ] ] },
  { label: 'callbacks-flattened.js',
    data:
     [ [ '200', 20 ],
       [ '500', 39 ],
       [ '1000', 53 ],
       [ '2000', 99 ],
       [ '5000', 166 ],
       [ '10000', 342 ],
       [ '20000', 654 ] ] },
  { label: 'callbacks-generator-suspend.js',
    data:
     [ [ '200', 16 ],
       [ '500', 18 ],
       [ '1000', 36 ],
       [ '2000', 69 ],
       [ '5000', 227 ],
       [ '10000', 490 ],
       [ '20000', 968 ] ] },
  { label: 'thunks-generator-gens.js',
    data:
     [ [ '200', 35 ],
       [ '500', 46 ],
       [ '1000', 71 ],
       [ '2000', 111 ],
       [ '5000', 272 ],
       [ '10000', 569 ],
       [ '20000', 1000 ] ] } ];

window.addEventListener('load', function() {
    $.plot("#perf-time-promises", window.perfTimePromises,
        {legend: { position: 'nw' }});
});

</script>


<p>Memory usage (MB)</p>
<div id="perf-mem-promises" class="plot">
</div>

<p style="text-align:right">Parallel requests</p>

<script type="text/javascript">

window.perfMemPromises =

[ { label: 'callbacks-async-waterfall.js',
    data:
     [ [ '200', 1.76953125 ],
       [ '500', 5.5390625 ],
       [ '1000', 9.8515625 ],
       [ '2000', 20.81640625 ],
       [ '5000', 55.79296875 ],
       [ '10000', 70.703125 ],
       [ '20000', 153.83203125 ] ] },
  { label: 'promises-bluebird-generator.js',
    data:
     [ [ '200', 0.80859375 ],
       [ '500', 1.02734375 ],
       [ '1000', 3.08203125 ],
       [ '2000', 6.8046875 ],
       [ '5000', 18.91015625 ],
       [ '10000', 41.828125 ],
       [ '20000', 60.2734375 ] ] },
  { label: 'promises-bluebird.js',
    data:
     [ [ '200', 0.78515625 ],
       [ '500', 1.1171875 ],
       [ '1000', 5 ],
       [ '2000', 10.41015625 ],
       [ '5000', 33.3046875 ],
       [ '10000', 57.0546875 ],
       [ '20000', 105.953125 ] ] },
  { label: 'callbacks-flattened.js',
    data:
     [ [ '200', 0.28515625 ],
       [ '500', 1.5703125 ],
       [ '1000', 2.91015625 ],
       [ '2000', 6.15234375 ],
       [ '5000', 18.015625 ],
       [ '10000', 19.671875 ],
       [ '20000', 59.41015625 ] ] },
  { label: 'callbacks-generator-suspend.js',
    data:
     [ [ '200', 0.984375 ],
       [ '500', 1.03125 ],
       [ '1000', 3.8671875 ],
       [ '2000', 7.33203125 ],
       [ '5000', 21.640625 ],
       [ '10000', 44.99609375 ],
       [ '20000', 65.28125 ] ] },
  { label: 'thunks-generator-gens.js',
    data:
     [ [ '200', 1.03125 ],
       [ '500', 2.14453125 ],
       [ '1000', 4.86328125 ],
       [ '2000', 9.8125 ],
       [ '5000', 23.09375 ],
       [ '10000', 41.83984375 ],
       [ '20000', 57.10546875 ] ] } ]


window.addEventListener('load', function() {
    $.plot('#perf-mem-promises', perfMemPromises, {legend: { position: 'nw' },
        yaxis: {min: 0}});
});

</script>

<p>And now, a table containing many patterns, 10 000 parallel requests, 1 ms per
I/O op. Measure ALL the things!</p>
<table>
<thead>
<tr>
<th style="text-align:left">file</th>
<th style="text-align:right">time(ms)</th>
<th style="text-align:right">memory(MB)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">callbacks-original.js</td>
<td style="text-align:right">316</td>
<td style="text-align:right">34.97</td>
</tr>
<tr>
<td style="text-align:left">callbacks-flattened.js</td>
<td style="text-align:right">335</td>
<td style="text-align:right">35.10</td>
</tr>
<tr>
<td style="text-align:left">callbacks-catcher.js</td>
<td style="text-align:right">355</td>
<td style="text-align:right">30.20</td>
</tr>
<tr>
<td style="text-align:left">promises-bluebird-generator.js</td>
<td style="text-align:right">364</td>
<td style="text-align:right">41.89</td>
</tr>
<tr>
<td style="text-align:left">dst-streamline.js</td>
<td style="text-align:right">441</td>
<td style="text-align:right">46.91</td>
</tr>
<tr>
<td style="text-align:left">callbacks-deferred-queue.js</td>
<td style="text-align:right">455</td>
<td style="text-align:right">38.10</td>
</tr>
<tr>
<td style="text-align:left">callbacks-generator-suspend.js</td>
<td style="text-align:right">466</td>
<td style="text-align:right">45.20</td>
</tr>
<tr>
<td style="text-align:left">promises-bluebird.js</td>
<td style="text-align:right">512</td>
<td style="text-align:right">57.45</td>
</tr>
<tr>
<td style="text-align:left">thunks-generator-gens.js</td>
<td style="text-align:right">517</td>
<td style="text-align:right">40.29</td>
</tr>
<tr>
<td style="text-align:left">thunks-generator-co.js</td>
<td style="text-align:right">707</td>
<td style="text-align:right">47.95</td>
</tr>
<tr>
<td style="text-align:left">promises-compose-bluebird.js</td>
<td style="text-align:right">710</td>
<td style="text-align:right">73.11</td>
</tr>
<tr>
<td style="text-align:left">callbacks-generator-genny.js</td>
<td style="text-align:right">801</td>
<td style="text-align:right">67.67</td>
</tr>
<tr>
<td style="text-align:left">callbacks-async-waterfall.js</td>
<td style="text-align:right">989</td>
<td style="text-align:right">89.97</td>
</tr>
<tr>
<td style="text-align:left">promises-bluebird-spawn.js</td>
<td style="text-align:right">1227</td>
<td style="text-align:right">66.98</td>
</tr>
<tr>
<td style="text-align:left">promises-kew.js</td>
<td style="text-align:right">1578</td>
<td style="text-align:right">105.14</td>
</tr>
<tr>
<td style="text-align:left">dst-stratifiedjs-compiled.js</td>
<td style="text-align:right">2341</td>
<td style="text-align:right">148.24</td>
</tr>
<tr>
<td style="text-align:left">rx.js</td>
<td style="text-align:right">2369</td>
<td style="text-align:right">266.59</td>
</tr>
<tr>
<td style="text-align:left">promises-when.js</td>
<td style="text-align:right">7950</td>
<td style="text-align:right">240.11</td>
</tr>
<tr>
<td style="text-align:left">promises-q-generator.js</td>
<td style="text-align:right">21828</td>
<td style="text-align:right">702.93</td>
</tr>
<tr>
<td style="text-align:left">promises-q.js</td>
<td style="text-align:right">28262</td>
<td style="text-align:right">712.93</td>
</tr>
<tr>
<td style="text-align:left">promises-compose-q.js</td>
<td style="text-align:right">59413</td>
<td style="text-align:right">778.05</td>
</tr>
</tbody>
</table>
<p>Promises are not slow. At least, not anymore. Infact, bluebird generators
are almost as fast as regular callback code (they&#39;re also the fastest
generators as of now). And bluebird promises are definitely at least two times
faster than <code>async.waterfall</code>.</p>
<p>Considering that bluebird wraps the underlying callback-based libraries <strong>and</strong>
makes your own callbacks exception-safe, this is really amazing.
<code>async.waterfall</code> doesn&#39;t do this. exceptions still crash your process.</p>
<h2 id="what-about-stack-traces-">What about stack traces?</h2>
<p>Bluebird has them behind a flag that slows it down about 5 times. They&#39;re
even longer than Q&#39;s <code>longStackSupport</code>: bluebird can give you the entire event
chain. Simply enable the flag in development mode, and you&#39;re suddenly in
debugging nirvana. It may even be viable to turn them on in production!</p>
<h2 id="what-about-the-community-">What about the community?</h2>
<p>This is a valid point. Mikeal said it: If you write a library based on
promises, <a href="http://www.youtube.com/watch?v=GaqxIMLLOu8">nobody is going to use it</a>.</p>
<p>However, both bluebird and Q give you <code>promise.nodeify</code>. With it, you can
write a library with a dual API that can both take callbacks and return
promises:</p>
<pre><code class="lang-js">module.exports = function fetch(itemId, callback) {
    return locate(itemId).then(function(location) {
        return getFrom(location, itemId);
    }).nodeify(callback);
}
</code></pre>
<p>And now my library is not imposing promises on you. Infact, my library is even
friendlier to the community: if I make a dumb mistake that causes an exception
to be thrown in the library, the exception will be passed as an error to
your callback instead of crashing your process. Now I don&#39;t have to fear the
wrath of angry library users expecting zero downtime on their production
servers. Thats always a plus, right?</p>
<h2 id="what-about-generators-">What about generators?</h2>
<p>To use generators with callbacks you have two options</p>
<ol>
<li>use a resumer style library like <a href="https://github.com/jmar777/suspend">suspend</a> or <a href="https://github.com/jmar777/genny">genny</a></li>
<li>wrap callback-taking functions to become thunk returning functions.</li>
</ol>
<p>Since #1 is proving to be unpopular, and #2 already involves wrapping, why not
just <code>s/thunk/promise/g</code> in #2 and use generators with promises?</p>
<p><a name="promises-tutorial"></a></p>
<h2 id="but-promises-are-unnecessarily-complicated-">But promises are unnecessarily complicated!</h2>
<p>Yes, the terminology used to explain promises can often be confusing. But
promises themselves are pretty simple - they&#39;re basically like lightweight
streams for single values.</p>
<p>Here is a straight-forward guide that uses known principles and analogies from
node (remember, the focus is on simplicity, not correctness):</p>
<p><strong>Edit (2014-01-07)</strong>: I decided to re-do this tutorial into a series of short
articles called <a href="http://promise-nuggets.github.io/">promise nuggets</a>. The
content is CC0 so feel free to fork, modify, improve or send pull requests.
The old tutorial will remain available within this article.</p>
<p>Promises are objects that have a <code>then</code> method. Unlike node functions, which
take a single callback, the <code>then</code> method of a promise can take two
callbacks: a success callback and an error callback. When one of these two
callbacks returns a value or throws an exception, <code>then</code> must behave in a way
that enables stream-like chaining and simplified error handling. Lets explain
that behavior of <code>then</code> through examples:</p>
<p>Imagine that node&#39;s <code>fs</code> was wrapped to work in this manner. This is pretty
easy to do - bluebird already lets you do something like that with
<a href="https://github.com/petkaantonov/bluebird/blob/master/API.md#promisepromisifyobject-target---object"><code>promisify()</code></a>. Then this code:</p>
<pre><code class="lang-js">fs.readFile(file, function(err, res) {
    if (err) handleError();
    doStuffWith(res);
});
</code></pre>
<p>will look like this:</p>
<pre><code class="lang-js">fs.readFile(file).then(function(res) {
    doStuffWith(res);
}, function(err) {
    handleError();
});
</code></pre>
<p>Whats going on here? <code>fs.readFile(file)</code> starts a file reading operation. That
operation is not yet complete at the point when readFile returns. This means
we can&#39;t return the file content. But we can still return something: we can
return the reading operation itself. And that operation is represented with a
promise.</p>
<p>This is sort of like a single-value stream:</p>
<pre><code class="lang-js">net.connect(port).on(&#39;data&#39;, function(res) {
    doStuffWith(res);
}).on(&#39;error&#39;, function(err) {
    hadnleError();
});
</code></pre>
<p>So far, this doesn&#39;t look that different from regular node callbacks -
except that you use a second callback for the error (which isn&#39;t necessarily
better). So when does it get better?</p>
<p>Its better because you can attach the callback later if you want. Remember,
<code>fs.readFile(file)</code> <em>returns</em> a promise now, so you can put that in a var,
or return it from a function:</p>
<pre><code class="lang-js">var filePromise = fs.readFile(file);
// do more stuff... even nest inside another promise, then
filePromise.then(function(res) { ... });
</code></pre>
<p>Yup, the second callback is optional. We&#39;re going to see why later.</p>
<p>Okay, that&#39;s still not much of an improvement. How about this then? You can
attach more than one callback to a promise if you like:</p>
<pre><code class="lang-js">filePromise.then(function(res) { uploadData(url, res); });
filePromise.then(function(res) { saveLocal(url, res); });
</code></pre>
<p>Hey, this is beginning to look more and more like streams - they too can be
piped to multiple destinations. But unlike streams, you can attach more
callbacks and get the value even <em>after</em> the file reading operation completes.</p>
<p>Still not good enough?</p>
<p>What if I told you... that if you return something from inside a .then()
callback, then you&#39;ll get a promise for that thing on the outside?</p>
<p>Say you want to get a line from a file. Well, you can get a promise for that
line instead:</p>
<pre><code class="lang-js">
var filePromise = fs.readFile(file)

var linePromise = filePromise.then(function(data) {
    return data.toString().split(&#39;\n&#39;)[line];
});

var beginsWithHelloPromise = linePromise.then(function(line) {
    return /^hello/.test(line);
});
</code></pre>
<p>Thats pretty cool, although not terribly useful - we could just put both sync
operations in the first <code>.then()</code> callback and be done with it.</p>
<p>But guess what happens when you return a <em>promise</em> from within a <code>.then</code>
callback. You get a promise for a promise outside of <code>.then()</code>?  Nope,
you just get the same promise!</p>
<pre><code class="lang-js">function readProcessAndSave(inPath, outPath) {
    // read the file
    var filePromise = fs.readFile(inPath);
    // then send it to the transform service
    var transformedPromise = filePromise.then(function(content) {
        return service.transform(content);
    });
    // then save the transformed content
    var writeFilePromise = transformedPromise.then(function(transformed) {
        return fs.writeFile(otherPath, transformed)
    });
    // return a promise that &quot;succeeds&quot; when the file is saved.
    return writeFilePromise;
}
readProcessAndSave(file, url, otherPath).then(function() {
    console.log(&quot;Success!&quot;);
}, function(err) {
    // This function will catch *ALL* errors from the above
    // operations including any exceptions thrown inside .then
    console.log(&quot;Oops, it failed.&quot;, err);
});
</code></pre>
<p>Now its easier to understand chaining: at the end of every function passed
to a <code>.then()</code> call, simply return a promise.</p>
<p>Lets make our code even shorter:</p>
<pre><code class="lang-js">function readProcessAndSave(file, url, otherPath) {
    return fs.readFile(file)
        .then(service.transform)
        .then(fs.writeFile.bind(fs, otherPath));
}
</code></pre>
<p>Mind = blown! Notice how I don&#39;t have to manually propagate errors. They will
automatically get passed with the returned promise.</p>
<p>What if we want to read, process, then upload, then also save locally?</p>
<pre><code class="lang-js">function readUploadAndSave(file, url, otherPath) {
    var content;
    // read the file and transform it
    return fs.readFile(file)
    .then(service.transform)
    .then(function(vContent)
        content = vContent;
        // then upload it
        return uploadData(url, content);
    }).then(function() { // after its uploaded
        // save it
        return fs.writeFile(otherPath, content);
    });
}
</code></pre>
<p>Or just nest it if you prefer the closure.</p>
<pre><code class="lang-js">function readUploadAndSave(file, url, otherPath) {
    // read the file and transform it
    return fs.readFile(file)
        .then(service.transform)
        .then(function(content)
            return uploadData(url, content).then(function() {
                // after its uploaded, save it
                return fs.writeFile(otherPath, content);
            });
        });
}
</code></pre>
<p>But hey, you can also upload and save in parallel!</p>
<pre><code class="lang-js">function readUploadAndSave(file, url, otherPath) {
    // read the file and transform it
    return fs.readFile(file)
        .then(service.transform)
        .then(function(content) {
            // create a promise that is done when both the upload
            // and file write are done:
            return Promise.join(
                uploadData(url, content),
                fs.writeFile(otherPath, content));
        });
}
</code></pre>
<p>No, these are not &quot;conveniently chosen&quot; functions. Promise code really is that
short in practice!</p>
<p>Similarly to how in a <code>stream.pipe</code> chain the last stream is returned, in
promise pipes the promise returned from the last <code>.then</code> callback is returned.</p>
<p>Thats all you need, really. The rest is just converting callback-taking
functions to promise-returning functions and using the stuff above to do your
control flow.</p>
<p>You can also return values in case of an error. So for example, to write a
<code>readFileOrDefault</code> (which returns a default value if for example the file
doesn&#39;t exist) you would simply return the default value from the error
callback:</p>
<pre><code class="lang-js">function readFileOrDefault(file, defaultContent) {
    return fs.readFile(file).then(function(fileContent) {
        return fileContent;
    }, function(err) {
        return defaultContent;
    });
}
</code></pre>
<p>You can also throw exceptions within both callbacks passed to <code>.then</code>. The
user of the returned promise can catch those errors by adding the second
.then handler</p>
<p>Now how about <code>configFromFileOrDefault</code> that reads and parses a JSON config
file, falls back to a default config if the file doesn&#39;t exist, but reports
JSON parsing errors? Here it is:</p>
<pre><code class="lang-js">function configFromFileOrDefault(file, defaultConfig) {
    // if fs.readFile fails, a default config is returned.
    // if JSON.parse throws, this promise propagates that.
    return fs.readFile(file).then(JSON.parse,
           function ifReadFails() {
               return defaultConfig;
           });
    // if we want to catch JSON.parse errors, we need to chain another
    // .then here - this one only captures errors from fs.readFile(file)
}
</code></pre>
<p>Finally, you can make sure your resources are released in all cases, even
when an error or exception happens:</p>
<pre><code class="lang-js">var result = doSomethingAsync();

return result.then(function(value) {
    // clean up first, then return the value.
    return cleanUp().then(function() { return value; })
}, function(err) {
    // clean up, then re-throw that error
    return cleanUp().then(function() { throw err; });
})
</code></pre>
<p>Or you can do the same using <code>.finally</code> (from both Bluebird and Q):</p>
<pre><code class="lang-js">var result = doSomethingAsync();
return result.finally(cleanUp);
</code></pre>
<p>The same promise is still returned, but only after <code>cleanUp</code> completes.</p>
<h2 id="but-what-about-async-async-">But what about <a href="https://github.com/caolan/async">async</a>?</h2>
<p>Since promises are actual values, most of the tools in async.js become
unnecessary and you can just use whatever you&#39;re using for regular values, like
your regular <code>array.map</code> / <code>array.reduce</code> functions, or just plain for loops.
That, and a couple of promise array tools like <code>.all</code>, <code>.spread</code> and <code>.some</code></p>
<p>You already have async.waterfall and async.auto with .then and .spread
chaining:</p>
<pre><code class="lang-js">files.getLastTwoVersions(filename)
    .then(function(items) {
        // fetch versions in parallel
        var v1 = versions.get(items.last),
            v2 = versions.get(items.previous);
        return [v1, v2];
    })
    .spread(function(v1, v2) {
        // both of these are now complete.
        return diffService.compare(v1.blob, v2.blob)
    })
    .then(function(diff) {
        // voila, diff is ready. Do something with it.
    });
</code></pre>
<p><code>async.parallel</code> / <code>async.map</code> are straightforward:</p>
<pre><code class="lang-js">// download all items, then get their names
var pNames = ids.map(function(id) {
    return getItem(id).then(function(result) {
        return result.name;
    });
});
// wait for things to complete:
Promise.all(pNames).then(function(names) {
    // we now have all the names.
});
</code></pre>
<p>What if you want to wait for the current item to download first (like
<code>async.mapSeries</code> and <code>async.series</code>)? Thats also pretty straightforward: just
wait for the current download to complete, then start the next download, then
extract the item name, and thats exactly what you say in the code:</p>
<pre><code class="lang-js">// start with current being an &quot;empty&quot; already-fulfilled promise
var current = Promise.fulfilled();
var namePromises = ids.map(function(id) {
    // wait for the current download to complete, then get the next
    // item, then extract its name.
    current = current
        .then(function() { return getItem(id); })
        .then(function(item) { return item.name; });
    return current;
});
Promise.all(namePromises).then(function(names) {
    // use all names here.
});
</code></pre>
<p>The only thing that remains is mapLimit - which is a bit harder to write - but
still not that hard:</p>
<pre><code class="lang-js">var queued = [], parallel = 3;
var namePromises = ids.map(function(id) {
    // How many items must download before fetching the next?
    // The queued, minus those running in parallel, plus one of
    // the parallel slots.
    var mustComplete = Math.max(0, queued.length - parallel + 1);
    // when enough items are complete, queue another request for an item
    return Promise.some(queued, mustComplete)
        .then(function() {
            var download = getItem(id);
            queued.push(download);
            return download;
        }).then(function(item) {
            // after that new download completes, get the item&#39;s name.
            return item.name;
        });
  });
Promise.all(namePromises).then(function(names) {
    // use all names here.
});
</code></pre>
<p>That covers most of async.</p>
<h2 id="what-about-early-returns-">What about early returns?</h2>
<p>Early returns are a pattern used throughout both sync and async code. Take this
hypothetical sync example:</p>
<pre><code>function getItem(key) {
    var item;
    // early-return if the item is in the cache.
    if (item = cache.get(key)) return item;
    // continue to get the item from the database. cache.put returns the item.
    item = cache.put(database.get(key));

    return item;
}
</code></pre><p>If we attempt to write this using promises, at first it looks impossible:</p>
<pre><code class="lang-js">function getItem(key) {
    return cache.get(key).then(function(item) {
        // early-return if the item is in the cache.
        if (item) return item;
        return database.get(item)
    }).then(function(putOrItem) {
        // what do we do here to avoid the unnecessary cache.put ?
    })
}
</code></pre>
<p>How can we solve this?</p>
<p>We solve it by remembering that the callback variant looks like this:</p>
<pre><code class="lang-js">function getItem(key, callback) {
    cache.get(key, function(err, res) {
        // early-return if the item is in the cache.
        if (res) return callback(null, res);
        // continue to get the item from the database
        database.get(key, function(err, res) {
            if (err) return callback(err);
            // cache.put calls back with the item
            cache.put(key, res, callback);
        })
    })
}
</code></pre>
<p>The promise version can do pretty much the same - just nest the rest
of the chain inside the first callback.</p>
<pre><code class="lang-js">function getItem(key) {
    return cache.get(key).then(function(res) {
        // early return if the item is in the cache
        if (res) return res;
        // continue the chain within the callback.
        return database.get(key)
            .then(cache.put);
    });
}
</code></pre>
<p>Or alternatively, if a cache miss results with an error:</p>
<pre><code class="lang-js">function getItem(key) {
    return cache.get(key).catch(function(err) {
        return database.get(key).then(cache.put);
    });
}
</code></pre>
<p>That means that early returns are just as easy as with callbacks, and sometimes
even easier (in case of errors)</p>
<h2 id="what-about-streams-">What about streams?</h2>
<p>Promises can work very well with streams. Imagine a <code>limit</code> stream that allows
at most 3 promises resolving in parallel, backpressuring otherwise, processing
items from leveldb:</p>
<pre><code class="lang-js">originalSublevel.createReadStream().pipe(limit(3, function(data) {
    return convertor(data.value).then(function(converted) {
        return {key: data.key, value: converted};
    });
})).pipe(convertedSublevel.createWriteStream());
</code></pre>
<p>Or how about stream pipelines that are safe from errors without attaching
error handlers to all of them?</p>
<pre><code class="lang-js">pipeline(original, limiter, converted).then(function(done) {

}, function(streamError) {

})
</code></pre>
<p>Looks awesome. I definitely want to explore that.</p>
<h2 id="the-future-">The future?</h2>
<p>In ES7, promises will become monadic (by getting flatMap and unit).
Also, we&#39;re going to get generic syntax sugar for monads. Then, it trully wont
matter what style you use - stream, promise or thunk - as long as it also
implements the monad functions. That is, except for callback-passing style -
it wont be able to join the party because it doesn&#39;t produce values.</p>
<p>I&#39;m just kidding, of course. I don&#39;t know if thats going to happen. Either way,
promises are useful and practical and will remain useful and practical in the
future.</p>
]]></description>
            <link>https://spion.github.io/posts/why-i-am-switching-to-promises.html</link>
            <guid isPermaLink="true">https://spion.github.io/posts/why-i-am-switching-to-promises.html</guid>
            <pubDate>Mon, 07 Oct 2013 00:00:00 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[Closures are unavoidable in node]]></title>
            <description><![CDATA[<p>A couple of weeks ago I wrote a <a href="/posts/analysis-generators-and-other-async-patterns-node.html">giant comparison of node.js async code
patterns</a> that
mostly focuses on the new generators feature in EcmaScript 6 (Harmony)</p>
<p>Among other implementations there were two callback versions: <a href="//github.com/spion/async-compare/blob/blog/examples/original.js">original.js</a>,
which contains nested callbacks, and <a href="//github.com/spion/async-compare/blob/blog/examples/flattened.js">flattened.js</a>, which flattens the nesting a
little bit. Both make extensive use of JavaScript closures: every time
the benchmarked function is invoked, a lot of closures are created.</p>
<p>Then <a href="http://blog.trevnorris.com/2013/08/long-live-callbacks.html">Trevor Norris wrote</a>
that we should be avoiding closures when writing performance-sensitive code,
hinting that my benchmark may be an example of &quot;doing it wrong&quot;</p>
<p>I decided to try and write two more flattened variants. The idea is to
minimize performance loss and memory usage by avoiding the creation of closures.</p>
<p>You can see the code here: <strong><a href="//github.com/spion/async-compare/blob/blog/examples/flattened-class.js">flattened-class.js</a> and <a href="//github.com/spion/async-compare/blob/blog/examples/flattened-noclosure.js">flattened-noclosure.js</a></strong></p>
<p>Of course, this made complexity skyrocket. Lets see what it did for performance.</p>
<p>These are the results for 50 000 parallel invocations of the upload function,
with simulated I/O operations that always take 1ms. Note: suspend is currently
the fastest generator based library</p>
<table>
<thead>
<tr>
<th style="text-align:left">file</th>
<th style="text-align:right">time(ms)</th>
<th style="text-align:right">memory(MB)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><a href="//github.com/spion/async-compare/blob/blog/examples/flattened-class.js">flattened-class.js</a></td>
<td style="text-align:right">1398</td>
<td style="text-align:right">106.58</td>
</tr>
<tr>
<td style="text-align:left"><a href="//github.com/spion/async-compare/blob/blog/examples/flattened.js">flattened.js</a></td>
<td style="text-align:right">1453</td>
<td style="text-align:right">110.19</td>
</tr>
<tr>
<td style="text-align:left"><a href="//github.com/spion/async-compare/blob/blog/examples/flattened-noclosure.js">flattened-noclosure.js</a></td>
<td style="text-align:right">1574</td>
<td style="text-align:right">102.28</td>
</tr>
<tr>
<td style="text-align:left"><a href="//github.com/spion/async-compare/blob/blog/examples/original.js">original.js</a></td>
<td style="text-align:right">1749</td>
<td style="text-align:right">124.96</td>
</tr>
<tr>
<td style="text-align:left"><a href="//github.com/spion/async-compare/blob/blog/examples/suspend.js">suspend.js</a></td>
<td style="text-align:right">2701</td>
<td style="text-align:right">144.66</td>
</tr>
</tbody>
</table>
<p>No performance gains. Why?</p>
<p>Because this kind of code requires that results from previous callbacks are
passed to the next callback. And unfortunately, in node this means creating
closures.</p>
<p>There really is no other option. Node core functions only take callback
functions. This means we <em>have</em> to create a closure: its the only mechanism in
JS that allows you to include context together with a function.</p>
<p>And yeah, <code>bind</code> also creates a closure:</p>
<pre><code>function bind(fn, ctx) {
    return function bound() {
        return fn.apply(ctx, arguments);
    }
}
</code></pre><p>Notice how <code>bound</code> is a closure over ctx and fn.</p>
<p>Now, if node core functions were also able to take a context argument, things
could have been different. For example, instead of writing:</p>
<pre><code>fs.readFile(f, bind(this.afterFileRead, this));
</code></pre><p>if we were able to write:</p>
<pre><code>fs.readFile(f, this.afterFileRead, this);
</code></pre><p>then we would be able to write code that avoids closures and
<a href="//github.com/spion/async-compare/blob/blog/examples/flattened-class.js">flattened-class.js</a> could have been much faster.</p>
<p>But we can&#39;t do that.</p>
<p>What if we could though? Lets fork
<a href="https://github.com/joyent/node/blob/blog/lib/timers.js">timers.js</a> from
node core and find out:</p>
<p>I added context passing support to the <code>Timeout</code> class. The result was
<a href="//github.com/spion/async-compare/blob/blog/lib/timers-ctx.js">timers-ctx.js</a>
which in turn resulted with <a href="//github.com/spion/async-compare/blob/blog/examples/flattened-class-ctx.js">flattened-class-ctx.js</a></p>
<p>And here is how it performs:</p>
<table>
<thead>
<tr>
<th style="text-align:left">file</th>
<th style="text-align:right">time(ms)</th>
<th style="text-align:right">memory(MB)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><a href="//github.com/spion/async-compare/blob/blog/examples/flattened-class-ctx.js">flattened-class-ctx.js</a></td>
<td style="text-align:right">929</td>
<td style="text-align:right">59.57</td>
</tr>
<tr>
<td style="text-align:left"><a href="//github.com/spion/async-compare/blob/blog/examples/flattened-class.js">flattened-class.js</a></td>
<td style="text-align:right">1403</td>
<td style="text-align:right">106.57</td>
</tr>
<tr>
<td style="text-align:left"><a href="//github.com/spion/async-compare/blob/blog/examples/flattened.js">flattened.js</a></td>
<td style="text-align:right">1452</td>
<td style="text-align:right">110.19</td>
</tr>
<tr>
<td style="text-align:left"><a href="//github.com/spion/async-compare/blob/blog/examples/original.js">original.js</a></td>
<td style="text-align:right">1743</td>
<td style="text-align:right">125.02</td>
</tr>
<tr>
<td style="text-align:left"><a href="//github.com/spion/async-compare/blob/blog/examples/suspend.js">suspend.js</a></td>
<td style="text-align:right">2834</td>
<td style="text-align:right">145.34</td>
</tr>
</tbody>
</table>
<p>Yeah. That shaved off a couple of 100s of miliseconds more.</p>
<p>Is it worth it?</p>
<table>
<thead>
<tr>
<th style="text-align:left">name</th>
<th style="text-align:right">tokens</th>
<th style="text-align:right">complexity</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><a href="//github.com/spion/async-compare/blob/blog/examples/suspend.js">suspend.js</a></td>
<td style="text-align:right">331</td>
<td style="text-align:right">1.10</td>
</tr>
<tr>
<td style="text-align:left"><a href="//github.com/spion/async-compare/blob/blog/examples/original.js">original.js</a></td>
<td style="text-align:right">425</td>
<td style="text-align:right">1.41</td>
</tr>
<tr>
<td style="text-align:left"><a href="//github.com/spion/async-compare/blob/blog/examples/flattened.js">flattened.js</a></td>
<td style="text-align:right">477</td>
<td style="text-align:right">1.58</td>
</tr>
<tr>
<td style="text-align:left"><a href="//github.com/spion/async-compare/blob/blog/examples/flattened-class-ctx.js">flattened-class-ctx.js</a></td>
<td style="text-align:right">674</td>
<td style="text-align:right">2.23</td>
</tr>
</tbody>
</table>
<p>Maybe, maybe not. You decide.</p>
]]></description>
            <link>https://spion.github.io/posts/closures-are-unavoidable-in-node.html</link>
            <guid isPermaLink="true">https://spion.github.io/posts/closures-are-unavoidable-in-node.html</guid>
            <pubDate>Fri, 23 Aug 2013 00:00:00 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[Analysis of generators and other async patterns in node]]></title>
            <description><![CDATA[<p>Table of contents:</p>
<ul>
<li><a href="#a-gentle-introduction-to-generators">A gentle introduction to generators</a></li>
<li><a href="#the-analysis">The analysis</a></li>
<li><a href="#the-examples">The examples</a></li>
<li><a href="#complexity">Complexity</a></li>
<li><a href="#performance-time-and-memory">Performance (time and memory)</a></li>
<li><a href="#debuggability">Debuggability</a><ul>
<li><a href="#source-maps-support">Source maps support</a></li>
<li><a href="#stack-trace-accuracy">Stack trace accuracy</a></li>
</ul>
</li>
<li><a href="#conclusion">Conclusion</a></li>
</ul>
<p>Async coding patterns are the subject of never-ending debates for us node.js
developers. Everyone has their own favorite method or pet library as well as
strong feelings and opinions on all the other methods and libraries. Debates
can be heated: sometimes social pariahs may be declared or grave rolling
may be induced.</p>
<p>The reason for this is that JavaScript never had any continuation
mechanism to allow code to pause and resume across the event loop boundary.</p>
<p>Until now.</p>
<p><a name="a-gentle-introduction-to-generators"></a></p>
<h3 id="a-gentle-introduction-to-generators">A gentle introduction to generators</h3>
<p><small>If you know how generators work, you can <a href="#skip">skip this</a>
    and continue to the analysis</small></p>
<p>Generators are a new feature of ES6. Normally they would be used for iteration.
Here is a generator that generates Fibonacci numbers. The example is taken from
the <a href="http://wiki.ecmascript.org/doku.php?id=harmony:generators">ECMAScript harmony wiki</a>:</p>
<pre><code class="lang-js">function* fibonacci() {
    let [prev, curr] = [0, 1];
    for (;;) {
        [prev, curr] = [curr, prev + curr];
        yield curr;
    }
}
</code></pre>
<p>And here is how we iterate through this generator:</p>
<pre><code class="lang-js">for (n of fibonacci()) {
    // truncate the sequence at 1000
    if (n &gt; 1000) break;
    console.log(n);
}
</code></pre>
<p>What happens behind the scene?</p>
<p>Generator functions are actually constructors of iterators. The returned
iterator object has a <code>next()</code> method. We can invoke that method manually:</p>
<pre><code class="lang-js">var seq = fibonacci();
console.log(seq.next()); // 1
console.log(seq.next()); // 2 etc.
</code></pre>
<p>When <code>next</code> is invoked, it starts the execution of the generator. The generator
runs until it encounters a <code>yield</code> expression. Then it pauses and the execution
goes back to the code that called <code>next</code></p>
<p>So in a way, <code>yield</code> works similarly to <code>return</code>. But there is a big difference.
If we call <code>next</code> on the generator again, the generator will resume from the
point where it left off - from the last <code>yield</code> line.</p>
<p>In our example, the generator will resume to the top of the endless  <code>for</code> loop
and calculate the next Fibonacci pair.</p>
<p>So how would we use this to write async code?</p>
<p>A great thing about the <code>next()</code> method is that it can also send values to the
generator. Let&#39;s write a simple number generator that also collects the stuff
it receives. When it gets two things it prints them using <code>console.log</code>:</p>
<pre><code class="lang-js">function* numbers() {
    var stuffIgot = [];
    for (var k = 0; k &lt; 2; ++k) {
        var itemReceived = yield k;
        stuffIgot.push(itemReceived);
    }
    console.log(stuffIgot);
}
</code></pre>
<p>This generator gives us 3 numbers using yield. Can we give something back?</p>
<p>Let&#39;s give two things to this generator:</p>
<pre><code class="lang-js">var iterator = numbers();
// Cant give anything the first time: need to get to a yield first.
console.log(iterator.next()); // logs 0
console.log(iterator.next(&#39;present&#39;)); // logs 1
fs.readFile(&#39;file.txt&#39;, function(err, resultFromAnAsyncTask) {
    console.log(iterator.next(resultFromAnAsyncTask)); // logs 2
});
</code></pre>
<p>The generator will log the string <code>&#39;present&#39;</code> and the contents of <code>file.txt</code></p>
<p>Uh-oh.</p>
<p>Seems that we can keep the generator paused across the event loop boundary.</p>
<p>What if instead of numbers, we yielded some files to be read?</p>
<pre><code class="lang-js">function* files() {
    var results = [];
    for (var k = 0; k &lt; files.length; ++k)
        results.push(yield files[k]);
    return results;
}
</code></pre>
<p>We could process those file reading tasks asynchronously.</p>
<pre><code class="lang-js">var iterator = files();
function process(iterator, sendValue) {
    var fileTask = iterator.next(sendValue);
    fs.readFile(fileTask, function(err, res) {
        if (err) iterator.throw(err);
        else process(iterator, res);
    });
}
process(iterator);
</code></pre>
<p>But from the generator&#39;s point of view, everything seems to be happening
synchronously: it gives us the file using <code>yield</code>, then it waits to be resumed,
then it receives the contents of the file and makes a push to the results
array.</p>
<p>And there is also <code>generator.throw()</code>. It causes an exception to be thrown
from inside the generator. How cool is that?</p>
<p>With <code>next</code> and <code>throw</code> combined together, we can easily run async code. Here
is an example from one of the earliest ES6 async generators library
<a href="http://taskjs.org/">task.js</a>.</p>
<pre><code class="lang-js">spawn(function* () {
    var data = yield $.ajax(url);
    $(&#39;#result&#39;).html(data);
    var status = $(&#39;#status&#39;).html(&#39;Download complete.&#39;);
    yield status.fadeIn().promise();
    yield sleep(2000);
    status.fadeOut();
});
</code></pre>
<p>This generator yields promises, which causes it to suspend execution. The <code>spawn</code>
function that runs the generator takes those promises and waits until they&#39;re
fulfilled. Then it resumes the generator by sending it the resulting value.</p>
<p>When used in this form, generators look a lot like classical threads. You spawn
a thread, it issues blocking I/O calls using <code>yield</code>, then the code resumes
execution from the point it left off.</p>
<p>There is one very important difference though. While threads can be suspended
involuntarily at any point by the operating systems, generators have to
willingly suspend themselves using <code>yield</code>. This means that there is no danger
of variables changing under our feet, except after a <code>yield</code>.</p>
<p>Generators go a step further with this: it&#39;s impossible to suspend execution
without using the <code>yield</code> keyword. In fact, if you want to call another
generator you will have to write <code>yield* anotherGenerator(args)</code>. This means
that suspend points are always visible in the code, just like they are when
using callbacks.</p>
<p>Amazing stuff! So what does this mean? What is the reduction of code complexity?
What are the performance characteristics of code using generators? Is debugging
easy? What about environments that don&#39;t have ES6 support?</p>
<p>I decided to do a big comparison of all existing node async code patterns and
find the answers to these questions.</p>
<p><a name="skip"></a><a name="the-analysis"></a></p>
<h3 id="the-analysis">The analysis</h3>
<p>For the analysis, I took <code>file.upload</code>, a typical CRUD method extracted from
<a href="http://doxbee.com">DoxBee</a> called when uploading files. It executes multiple
queries to the database: a couple of selects, some inserts and one update.
Lots of mixed sync / async action.</p>
<p>It looks like this:</p>
<pre><code>function upload(stream, idOrPath, tag, done) {
    var blob = blobManager.create(account);
    var tx = db.begin();
    function backoff(err) {
        tx.rollback();
        return done(new Error(err));
    }
    blob.put(stream, function (err, blobId) {
        if (err) return done(err);
        self.byUuidOrPath(idOrPath).get(function (err, file) {
            if (err) return done(err);
            var previousId = file ? file.version : null;
            var version = {
                userAccountId: userAccount.id,
                date: new Date(),
                blobId: blobId,
                creatorId: userAccount.id,
                previousId: previousId
            };
            version.id = Version.createHash(version);
            Version.insert(version).execWithin(tx, function (err) {
                if (err) return backoff(err);
                if (!file) {
                    var splitPath = idOrPath.split(&#39;/&#39;);
                    var fileName = splitPath[splitPath.length - 1];
                    var newId = uuid.v1();
                    self.createQuery(idOrPath, {
                        id: newId,
                        userAccountId: userAccount.id,
                        name: fileName,
                        version: version.id
                    }, function (err, q) {
                        if (err) return backoff(err);
                        q.execWithin(tx, function (err) {
                            afterFileExists(err, newId);
                        });

                    })
                }
                else return afterFileExists(null, file.id);
            });
            function afterFileExists(err, fileId) {
                if (err) return backoff(err);
                FileVersion.insert({fileId: fileId,versionId: version.id})
                    .execWithin(tx, function (err) {
                        if (err) return backoff(err);
                        File.whereUpdate({id: fileId}, {
                            version: version.id
                        }).execWithin(tx, function (err) {
                            if (err) return backoff(err);
                            tx.commit(done);
                        });
                })
            }
        });
    });
}
</code></pre><p>Slightly pyramidal code full of callbacks.</p>
<p>This is how it looks like when written with generators:</p>
<pre><code>var genny = require(&#39;genny&#39;);
module.exports = genny.fn(function* upload(resume, stream, idOrPath, tag) {
    var blob = blobManager.create(account);
    var tx = db.begin();
    try {
        var blobId = yield blob.put(stream, resume());
        var file = yield self.byUuidOrPath(idOrPath).get(resume());
        var previousId = file ? file.version : null;
        var version = {
            userAccountId: userAccount.id,
            blobId: blobId,
            creatorId: userAccount.id,
            previousId: previousId
        };
        version.id = Version.createHash(version);
        yield Version.insert(version).execWithin(tx, resume());
        if (!file) {
            var splitPath = idOrPath.split(&#39;/&#39;);
            var fileName = splitPath[splitPath.length - 1];
            var newId = uuid.v1();
            var file = {
                id: newId,
                userAccountId: userAccount.id,
                name: fileName,
                version: version.id
            }
            var q = yield self.createQuery(idOrPath, file, resume());
            yield q.execWithin(tx, resume());
        }
        yield FileVersion.insert({fileId: file.id, versionId: version.id})
            .execWithin(tx, resume());
        yield File.whereUpdate({id: file.id}, {version: version.id})
            .execWithin(tx, resume());
        yield tx.commit(resume());
    } catch (e) {
        tx.rollback();
        throw e;
    }
});
</code></pre><p>Shorter, very straight-forward code and absolutely no nesting of callback
functions. Awesome.</p>
<p>Yet subjective adjectives are not very convincing. I want to have a measure of
complexity, a number that tells me what I&#39;m actually saving.</p>
<p>I also want to know what the performance characteristics are - how much time
and memory would it take to execute a thousand of parallel invocations of this
method? What about 2000 or 3000?</p>
<p>Also, what happens if an exception is thrown? Do I get a complete stack trace
like in the original version?</p>
<p>I also wanted to compare the results with other alternatives, such as fibers,
streamlinejs and promises (without generators).</p>
<p>So I wrote a lot of different versions of this method, and I will share my
personal impressions before giving you the results of the analysis</p>
<p><a name="the-examples"></a></p>
<h3 id="the-examples">The examples</h3>
<h4 id="-original-js-github-com-spion-async-compare-blob-blog-examples-original-js-"><a href="//github.com/spion/async-compare/blob/blog/examples/original.js">original.js</a></h4>
<p>The original solution, presented above. Vanilla callbacks. Slightly pyramidal.
I consider it acceptable, if a bit mediocre.</p>
<h4 id="-flattened-js-github-com-spion-async-compare-blob-blog-examples-flattened-js-"><a href="//github.com/spion/async-compare/blob/blog/examples/flattened.js">flattened.js</a></h4>
<p>Flattened variant of the original via named functions. Taking the advice from
<a href="http://callbackhell.com/">callback hell</a>, I flattened the pyramid a little
bit. As I did that, I found that while the pyramid shrunk, the code actually
grew.</p>
<h4 id="-catcher-js-github-com-spion-async-compare-blob-blog-examples-catcher-js-"><a href="//github.com/spion/async-compare/blob/blog/examples/catcher.js">catcher.js</a></h4>
<p>I noticed that the first two vanilla solutions had a lot of common error
handling code everywhere. So I wrote a tiny library called catcher.js which
works very much like node&#39;s <code>domain.intercept</code>. This reduced the complexity
and the number of lines further, but the pyramidal looks remained.</p>
<h4 id="-async-js-github-com-spion-async-compare-blob-blog-examples-async-js-"><a href="//github.com/spion/async-compare/blob/blog/examples/async.js">async.js</a></h4>
<p>Uses the waterfall function from <a href="//github.com/caolan/async">caolan&#39;s async</a>.
Very similar to flattened.js but without the need to handle errors at every
step.</p>
<h4 id="-flattened-class-js-github-com-spion-async-compare-blob-blog-examples-flattened-class-js-flattened-noclosure-js-github-com-spion-async-compare-blob-blog-examples-flattened-noclosure-js-flattened-class-ctx-js-github-com-spion-async-compare-blob-blog-examples-flattened-class-ctx-js-"><a href="//github.com/spion/async-compare/blob/blog/examples/flattened-class.js">flattened-class.js</a>, <a href="//github.com/spion/async-compare/blob/blog/examples/flattened-noclosure.js">flattened-noclosure.js</a>, <a href="//github.com/spion/async-compare/blob/blog/examples/flattened-class-ctx.js">flattened-class-ctx.js</a></h4>
<p>See <a href="/posts/closures-are-unavoidable-in-node.html">this post</a> for details</p>
<p><a name="promises-failure"></a></p>
<h4 id="-promises-js-github-com-spion-async-compare-blob-blog-examples-extra-promises-js-"><a href="//github.com/spion/async-compare/blob/blog/examples-extra/promises.js">promises.js</a></h4>
<p>I&#39;ll be honest. I&#39;ve never written promise code in node before. Driven by
<a href="//jeditoolkit.com/2012/04/26/code-logic-not-mechanics.html#post">Gozalla&#39;s excellent post</a>
I concluded that everything should be a promise, and things that can&#39;t handle
promises should also be rewritten.</p>
<p>Take for example this particular line in the original:</p>
<pre><code class="lang-js">var previousId = file ? file.version : null;
</code></pre>
<p>If file is a promise, we can&#39;t use the ternary operator or the property
getter. Instead we need to write two helpers: a ternary operator helper and a
property getter helper:</p>
<pre><code class="lang-js">var previousIdP = p.ternary(fileP, p.get(fileP, &#39;version&#39;), null);
</code></pre>
<p>Unfortunately this gets out of hand quickly:</p>
<pre><code>var versionP = p.allObject({
    userAccountId: userAccount.id,
    blobId: blobIdP,
    creatorId: userAccount.id,
    previousId: previousIdP,
    ...
});
versionP = p.set(versionP, p.allObject({
    id: fn.call(Version.createHash, versionP)
}));
// Even if Version.insert has been lifted to take promise arguments, it returns
// a promise and therefore we cannot call execWithinP. We have to wait for the
// promise  to resolve to invoke the function.
var versionInsert = p.eventuallyCall(
    Version.insert(versionP), &#39;execWithinP&#39;, tx);
var versionIdP = p.get(versionP, &#39;id&#39;);
</code></pre><p>So I decided to write a less aggressive version, <code>promiseish.js</code></p>
<p>note: I used <a href="//github.com/cujojs/when">when</a> because i liked its function
lifting API better than Q&#39;s</p>
<h4 id="-promiseish-js-github-com-spion-async-compare-blob-blog-examples-promiseish-js-and-promiseishq-js-github-com-spion-async-compare-blob-blog-examples-promiseishq-js-"><a href="//github.com/spion/async-compare/blob/blog/examples/promiseish.js">promiseish.js</a> and <a href="//github.com/spion/async-compare/blob/blog/examples/promiseishQ.js">promiseishQ.js</a></h4>
<p>Nothing fancy here, just some <code>.then()</code> chaining. In fact it feels less complex
than the <code>promise.js</code> version, where I felt like I was trying to fight the
language all the time.</p>
<p>The second file <code>promiseishQ.js</code> uses <a href="//github.com/kriskowal/q">Q</a> instead of
<a href="//github.com/cujojs/when">when</a>. No big difference there.</p>
<h4 id="-fibrous-js-github-com-spion-async-compare-blob-blog-examples-fibrous-js-"><a href="//github.com/spion/async-compare/blob/blog/examples/fibrous.js">fibrous.js</a></h4>
<p><a href="//github.com/goodeggs/fibrous">Fibrous</a> is a fibers library that creates
&quot;sync&quot; methods out of your async ones, which you can then run in a fiber.</p>
<p>So if for example you had:</p>
<pre><code>fs.readFile(file, function(err, data){ ... });
</code></pre><p>Fibrous would generate a version that returns a future, suspends the running
fiber and resumes execution when the value becomes available.</p>
<pre><code>var data = fs.sync.readFile(file);
</code></pre><p>I also needed to wrap the entire upload function:</p>
<pre><code>fibrous(function upload() { ... })
</code></pre><p>This felt very similar to the generators version above but with <code>sync</code> instead
of <code>yield</code> to indicate the methods that will yield. The one benefit I can think
of is that it feels more natural for chaining - less parenthesis are needed.</p>
<pre><code>somefn.sync(arg).split(&#39;/&#39;)
// vs
(yield somefn(arg, resume)).split(&#39;/&#39;)
</code></pre><p>Major drawback: this will never be available outside of node.js or without
native modules.</p>
<p>Library: <a href="//github.com/goodeggs/fibrous">fibrous</a></p>
<h4 id="-suspend-js-github-com-spion-async-compare-blob-blog-examples-suspend-js-and-genny-js-github-com-spion-async-compare-blob-blog-examples-extra-promises-js-"><a href="//github.com/spion/async-compare/blob/blog/examples/suspend.js">suspend.js</a> and <a href="//github.com/spion/async-compare/blob/blog/examples-extra/promises.js">genny.js</a></h4>
<p><a href="https://github.com/jmar777/suspend">suspend</a> and
<a href="http://github.com/spion/genny">genny</a> are generator-based solutions that can
work directly with node-style functions.</p>
<p>I&#39;m biased here since I wrote genny. I still think that this is objectively the
best way to use generators in node. Just replace the callback with a placeholder
generator-resuming function, then yield that. Comes back to you with the value.</p>
<p>Kudos to <a href="//github.com/jmar777">jmar777</a> for realizing that you don&#39;t need
to actually yield anything and can resume the generator using the placeholder
callback instead.</p>
<p>Both suspend and genny use generators roughly the same way. The resulting code
is very clean, very straightforward and completely devoid of callbacks.</p>
<h4 id="-qasync-js-github-com-spion-async-compare-blob-blog-examples-qasync-js-"><a href="//github.com/spion/async-compare/blob/blog/examples/qasync.js">qasync.js</a></h4>
<p>Q provides two methods that allow you to use generators: <code>Q.spawn</code> and
<code>Q.async</code>. In both cases the generator yields promises and in turn receives
resolved values.</p>
<p>The code didn&#39;t feel very different from genny and suspend. Its slightly less
complicated: you can yield the promise instead of placing the provided resume
function at every point where a callback is needed.</p>
<p>Caveat: as always with promises you will need to wrap all callback-based
functions.</p>
<p>Library: <a href="//github.com/kriskowal/q">Q</a></p>
<h4 id="-co-js-github-com-spion-async-compare-blob-blog-examples-co-js-and-gens-js-github-com-spion-async-compare-blob-blog-examples-gens-js-"><a href="//github.com/spion/async-compare/blob/blog/examples/co.js">co.js</a> and <a href="//github.com/spion/async-compare/blob/blog/examples/gens.js">gens.js</a></h4>
<p><a href="//github.com/Raynos/gens">Gens</a> and <a href="//github.com/visionmedia/co">co</a> are
generator-based libraries. Both can work by yielding thunk-style functions:
that is, functions that take a single argument which is a node style callback
in the format <code>function (err, result)</code></p>
<p>The code looks roughly the same as qasync.js</p>
<p>The problem is, thunks still require wrapping. The recommended way to wrap node
style functions is to use <code>co.wrap</code> for co and <code>fn.bind</code> for gens - so thats
what I did.</p>
<h4 id="-streamline-js-github-com-spion-async-compare-blob-blog-examples-src-streamline-_js-"><a href="//github.com/spion/async-compare/blob/blog/examples/src-streamline._js">streamline.js</a></h4>
<p>Uses <a href="http://github.com/Sage/streamlinejs">streamlinejs</a> CPS transformer and
works very much like co and qasync, except without needing to write yield
all the time.</p>
<p>Caveat: you will need to compile the file in order to use it. Also, even
though it looks like valid JavaScript, it isn&#39;t JavaScript. Superficially, it
has the same syntax, but it has very different semantics, particularly when
it comes to the <code>_</code> keyword, which acts like <code>yield</code> and <code>resume</code> combined in
one.</p>
<p>The code however is really simple and straightforward: infact it has the lowest
complexity.</p>
<p><a name="complexity"></a></p>
<h3 id="complexity">Complexity</h3>
<p>To measure complexity I took the number of tokens in the source code found by
Esprima&#39;s lexer (comments excluded). The idea is taken from
<a href="http://www.paulgraham.com/power.html">Paul Graham&#39;s essay <em>Succinctness is Power</em></a></p>
<p>I decided to allow all callback wrapping to happen in a separate file: In a
large system, the wrapped layer will probably be a small part of the code.</p>
<p>Results:</p>
<table>
<thead>
<tr>
<th style="text-align:left">name</th>
<th style="text-align:right">tokens</th>
<th style="text-align:right">complexity</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">src-streamline._js</td>
<td style="text-align:right">302</td>
<td style="text-align:right">1.00</td>
</tr>
<tr>
<td style="text-align:left">co.js</td>
<td style="text-align:right">304</td>
<td style="text-align:right">1.01</td>
</tr>
<tr>
<td style="text-align:left">qasync.js</td>
<td style="text-align:right">314</td>
<td style="text-align:right">1.04</td>
</tr>
<tr>
<td style="text-align:left">fibrous.js</td>
<td style="text-align:right">317</td>
<td style="text-align:right">1.05</td>
</tr>
<tr>
<td style="text-align:left">suspend.js</td>
<td style="text-align:right">331</td>
<td style="text-align:right">1.10</td>
</tr>
<tr>
<td style="text-align:left">genny.js</td>
<td style="text-align:right">339</td>
<td style="text-align:right">1.12</td>
</tr>
<tr>
<td style="text-align:left">gens.js</td>
<td style="text-align:right">341</td>
<td style="text-align:right">1.13</td>
</tr>
<tr>
<td style="text-align:left">catcher.js</td>
<td style="text-align:right">392</td>
<td style="text-align:right">1.30</td>
</tr>
<tr>
<td style="text-align:left">promiseishQ.js</td>
<td style="text-align:right">396</td>
<td style="text-align:right">1.31</td>
</tr>
<tr>
<td style="text-align:left">promiseish.js</td>
<td style="text-align:right">411</td>
<td style="text-align:right">1.36</td>
</tr>
<tr>
<td style="text-align:left">original.js</td>
<td style="text-align:right">421</td>
<td style="text-align:right">1.39</td>
</tr>
<tr>
<td style="text-align:left">async.js</td>
<td style="text-align:right">442</td>
<td style="text-align:right">1.46</td>
</tr>
<tr>
<td style="text-align:left">promises.js</td>
<td style="text-align:right">461</td>
<td style="text-align:right">1.53</td>
</tr>
<tr>
<td style="text-align:left">flattened.js</td>
<td style="text-align:right">473</td>
<td style="text-align:right">1.57</td>
</tr>
<tr>
<td style="text-align:left">flattened-noclosure.js</td>
<td style="text-align:right">595</td>
<td style="text-align:right">1.97</td>
</tr>
<tr>
<td style="text-align:left">flattened-class-ctx.js</td>
<td style="text-align:right">674</td>
<td style="text-align:right">2.23</td>
</tr>
<tr>
<td style="text-align:left">flattened-class.js</td>
<td style="text-align:right">718</td>
<td style="text-align:right">2.38</td>
</tr>
<tr>
<td style="text-align:left">rx.js</td>
<td style="text-align:right">935</td>
<td style="text-align:right">3.10</td>
</tr>
</tbody>
</table>
<p>Streamline and co have the lowest complexity. Fibrous, qasync, suspend, genny
and gens are roughly comparable.</p>
<p>Catcher is comparable with the normal promise solutions. Both are roughly
comparable to the original version with callbacks, but there is some
improvement as the error handling is consolidated to one place.</p>
<p>It seems that flattening the callback pyramid increases the complexity a little
bit. However, arguably the readability of the flattened version is improved.</p>
<p>Using caolan&#39;s async in this particular case doesn&#39;t seem to yield much
improvement. Its complexity however is lower than the flattened version because
it consolidates error handling.</p>
<p>Going promises-all-the-way as Gozala suggests also increases the complexity
because we&#39;re fighting the language all the time.</p>
<p>The rx.js sample is still a work in progress - it can be made much better.</p>
<p><a name="performance-time-and-memory"></a></p>
<h3 id="performance-time-and-memory-">Performance (time and memory)</h3>
<p>All external methods are mocked using <code>setTimeout</code> to simulate waiting for I/O.</p>
<p>There are two variables that control the test:</p>
<ul>
<li>\(n\) - the number of parallel &quot;upload requests&quot;</li>
<li>\(t\) - average wait time per async I/O operation</li>
</ul>
<p>For the first test, I set the time for every async operation to 1ms then
ran every solution for \(n \in \lbrace 100, 500, 1000, 1500, 2000 \rbrace \).</p>
<p>note: hover over the legend to highlight the item on the chart.</p>
<p><div id="perf-time-1" class="plot">
</div></p>
<script type="text/javascript">

window.perfCPUBound =
[
{ label: 'async.js',
    data:
     [ [ 100, 17 ],
       [ 500, 39 ],
       [ 1000, 65 ],
       [ 1500, 95 ],
       [ 2000, 119 ] ] },
{ label: 'catcher.js',
    data:
     [ [ 100, 17 ],
       [ 500, 29 ],
       [ 1000, 44 ],
       [ 1500, 58 ],
       [ 2000, 72 ] ] },
  { label: 'co.js',
    data:
     [ [ 100, 24 ],
       [ 500, 62 ],
       [ 1000, 96 ],
       [ 1500, 157 ],
       [ 2000, 233 ] ] },
  { label: 'dst-co-traceur.js',
    data:
     [ [ 100, 28 ],
       [ 500, 86 ],
       [ 1000, 200 ],
       [ 1500, 271 ],
       [ 2000, 324 ] ] },
  { label: 'dst-genny-traceur.js',
    data:
     [ [ 100, 25 ],
       [ 500, 76 ],
       [ 1000, 157 ],
       [ 1500, 244 ],
       [ 2000, 286 ] ] },
  { label: 'dst-qasync-traceur.js',
    data:
     [ [ 100, 112 ],
       [ 500, 510 ],
       [ 1000, 1108 ],
       [ 1500, 1713 ],
       [ 2000, 2316 ] ] },
  { label: 'dst-stratifiedjs-014.js',
    data:
     [ [ 100, 40 ],
       [ 500, 131 ],
       [ 1000, 224 ],
       [ 1500, 280 ],
       [ 2000, 433 ] ] },
  { label: 'dst-streamline.js',
    data:
     [ [ 100, 19 ],
       [ 500, 37 ],
       [ 1000, 54 ],
       [ 1500, 74 ],
       [ 2000, 97 ] ] },
  { label: 'dst-suspend-traceur.js',
    data:
     [ [ 100, 21 ],
       [ 500, 56 ],
       [ 1000, 117 ],
       [ 1500, 217 ],
       [ 2000, 263 ] ] },
  { label: 'flattened-class.js',
    data:
     [ [ 100, 16 ],
       [ 500, 29 ],
       [ 1000, 40 ],
       [ 1500, 47 ],
       [ 2000, 58 ] ] },
  { label: 'flattened.js',
    data:
     [ [ 100, 15 ],
       [ 500, 31 ],
       [ 1000, 44 ],
       [ 1500, 55 ],
       [ 2000, 64 ] ] },
  { label: 'flattened-noclosure.js',
    data:
     [ [ 100, 16 ],
       [ 500, 28 ],
       [ 1000, 42 ],
       [ 1500, 53 ],
       [ 2000, 64 ] ] },
  { label: 'genny.js',
    data:
     [ [ 100, 19 ],
       [ 500, 48 ],
       [ 1000, 79 ],
       [ 1500, 147 ],
       [ 2000, 150 ] ] },
  { label: 'gens.js',
    data:
     [ [ 100, 19 ],
       [ 500, 39 ],
       [ 1000, 63 ],
       [ 1500, 94 ],
       [ 2000, 119 ] ] },
  { label: 'original.js',
    data:
     [ [ 100, 15 ],
       [ 500, 28 ],
       [ 1000, 43 ],
       [ 1500, 56 ],
       [ 2000, 65 ] ] },
  { label: 'promiseish.js',
    data:
     [ [ 100, 97 ],
       [ 500, 463 ],
       [ 1000, 809 ],
       [ 1500, 1155 ],
       [ 2000, 1652 ] ] },
  { label: 'promiseishQ.js',
    data:
     [ [ 100, 93 ],
       [ 500, 540 ],
       [ 1000, 1145 ],
       [ 1500, 1789 ],
       [ 2000, 2287 ] ] },
  { label: 'promises.js',
    data:
     [ [ 100, 222 ],
       [ 500, 1241 ],
       [ 1000, 2284 ],
       [ 1500, 3836 ],
       [ 2000, 5861 ] ] },
  { label: 'qasync.js',
    data:
     [ [ 100, 79 ],
       [ 500, 489 ],
       [ 1000, 962 ],
       [ 1500, 1587 ],
       [ 2000, 2104 ] ] },
  { label: 'rx.js',
    data:
     [ [ 100, 37 ],
       [ 500, 147 ],
       [ 1000, 232 ],
       [ 1500, 329 ],
       [ 2000, 490 ] ] },
  { label: 'suspend.js',
    data:
     [ [ 100, 16 ],
       [ 500, 28 ],
       [ 1000, 50 ],
       [ 1500, 66 ],
       [ 2000, 83 ] ] } ]

  .concat(

[ { label: 'dst-streamline-fibers.js',
    data:
     [ [ 100, 25 ],
       [ 500, 147 ],
       [ 1000, 519 ],
       [ 1500, 1256 ],
       [ 2000, 2393 ] ] },
  { label: 'fibrous.js',
    data:
     [ [ 100, 89 ],
       [ 500, 442 ],
       [ 1000, 1159 ],
       [ 1500, 2228 ],
       [ 2000, 3755 ] ] } ]

    );

window.addEventListener('load', function() {
    $.plot('#perf-time-1', perfCPUBound, {legend: { position: 'nw' }});
});
</script>

<p>Wow. Promises seem really, really slow. Fibers are also slow, with time
complexity \( O(n^2) \). Everything else seems to be much faster.</p>
<p><strong>Update (Dec 20 2013)</strong>: Promises not slow anymore. PetkaAntonov wrote
Bluebird, which is faster than almost everything else and very low on
memory usage. For more info read <em><a href="/posts/why-i-am-switching-to-promises.html">Why I am switching to
Promises</a></em></p>
<p>Lets try removing all those promises and fibers to see whats down there.</p>
<p><div id="perf-time-2" class="plot">
</div></p>
<script type="text/javascript">


window.addEventListener('load', function() {
    $.plot('#perf-time-2', perfCPUBound.filter(function(item) {
        return !/(promise|qasync|fibrous|fiber)/.test(item.label)
    }), {legend: { position: 'nw' }})
});
</script>

<p>Ah, much better.</p>
<p>The original and flattened solutions are the fastest, as they use vanilla
callbacks, with the fastest flattened solution being flattened-class.js.</p>
<p>suspend is the fastest generator based solution. It incurred minimal
overhead of about 60% running time. Its also roughly comparable with
streamlinejs (when in raw callbacks mode).</p>
<p>caolan&#39;s async adds some measurable overhead (its about 2 times slower than
the original versions). Its also somewhat slower than the fastest generator
based solution.</p>
<p>genny is about 3 times slower. This is because it adds some protection
guarantees: it makes sure that callback-calling functions behave and call the
callback only once. It also provides a mechanism to enable better stack traces
when errors are encountered.</p>
<p>The slowest of the generator bunch is co, but not by much. There is nothing
intrinsically slow about it though: the slowness is probably caused by <code>co.wrap</code>
which creates a new arguments array on every invocation of the wrapped function.</p>
<p>All generator solutions become about 2 times slower when compiled with
<a href="//github.com/google/traceur-compiler/">Google Traceur</a>, an ES6 to ES5 compiler
which we need to run generators code without the <code>--harmony</code> switch or in
browsers.</p>
<p>Finally we have rx.js which is about 10 times slower than the original.</p>
<p>However, this test is a bit unrealistic.</p>
<p>Most async operations take much longer than 1 millisecond to complete,
especially when the load is as high as thousands of requests per second.
As a result, performance is I/O bound - why measure things as if it were
CPU-bound?</p>
<p>So lets make the average time needed for an async operation depend on the
number of parallel calls to <code>upload()</code>.</p>
<p>On my machine redis can be queried about 40 000 times per second; node&#39;s
&quot;hello world&quot; http server can serve up to 10 000 requests per second;
postgresql&#39;s pgbench can do 300 mixed or 15 000 select transactions per second.</p>
<p>Given all that, I decided to go with 10 000 requests per second - it looks like
a reasonable (rounded) mean.</p>
<p>Each I/O operation will take 10 ms on average when there are 100 running in
parallel and 1000 ms when there are 10 000 running in parallel. Makes much more
sense.</p>
<p><div id="perf-time-3" class="plot">
</div></p>
<script type="text/javascript">
window.perfIOBound =

[ { label: 'async.js',
    data:
     [ [ 100, 92 ],
       [ 500, 388 ],
       [ 1000, 791 ],
       [ 1500, 1155 ],
       [ 2000, 1518 ] ] },
{ label: 'catcher.js',
    data:
     [ [ 100, 78 ],
       [ 500, 351 ],
       [ 1000, 699 ],
       [ 1500, 1081 ],
       [ 2000, 1427 ] ] },
  { label: 'co.js',
    data:
     [ [ 100, 98 ],
       [ 500, 424 ],
       [ 1000, 821 ],
       [ 1500, 1222 ],
       [ 2000, 1661 ] ] },
  { label: 'dst-co-traceur.js',
    data:
     [ [ 100, 91 ],
       [ 500, 467 ],
       [ 1000, 922 ],
       [ 1500, 1347 ],
       [ 2000, 1744 ] ] },
  { label: 'dst-genny-traceur.js',
    data:
     [ [ 100, 89 ],
       [ 500, 398 ],
       [ 1000, 778 ],
       [ 1500, 1205 ],
       [ 2000, 1533 ] ] },
  { label: 'dst-qasync-traceur.js',
    data:
     [ [ 100, 113 ],
       [ 500, 495 ],
       [ 1000, 1125 ],
       [ 1500, 1757 ],
       [ 2000, 2301 ] ] },
  { label: 'dst-stratifiedjs-014.js',
    data:
     [ [ 100, 117 ],
       [ 500, 437 ],
       [ 1000, 820 ],
       [ 1500, 1216 ],
       [ 2000, 1592 ] ] },
  { label: 'dst-streamline.js',
    data:
     [ [ 100, 79 ],
       [ 500, 350 ],
       [ 1000, 699 ],
       [ 1500, 1070 ],
       [ 2000, 1444 ] ] },
  { label: 'dst-suspend-traceur.js',
    data:
     [ [ 100, 82 ],
       [ 500, 352 ],
       [ 1000, 687 ],
       [ 1500, 1038 ],
       [ 2000, 1374 ] ] },
  { label: 'flattened-class.js',
    data:
     [ [ 100, 79 ],
       [ 500, 337 ],
       [ 1000, 687 ],
       [ 1500, 1049 ],
       [ 2000, 1424 ] ] },
  { label: 'flattened.js',
    data:
     [ [ 100, 76 ],
       [ 500, 346 ],
       [ 1000, 699 ],
       [ 1500, 1059 ],
       [ 2000, 1440 ] ] },
  { label: 'flattened-noclosure.js',
    data:
     [ [ 100, 80 ],
       [ 500, 340 ],
       [ 1000, 697 ],
       [ 1500, 1060 ],
       [ 2000, 1425 ] ] },
  { label: 'genny.js',
    data:
     [ [ 100, 77 ],
       [ 500, 357 ],
       [ 1000, 723 ],
       [ 1500, 1130 ],
       [ 2000, 1521 ] ] },
  { label: 'gens.js',
    data:
     [ [ 100, 84 ],
       [ 500, 353 ],
       [ 1000, 719 ],
       [ 1500, 1100 ],
       [ 2000, 1471 ] ] },
  { label: 'original.js',
    data:
     [ [ 100, 74 ],
       [ 500, 341 ],
       [ 1000, 695 ],
       [ 1500, 1060 ],
       [ 2000, 1431 ] ] },
  { label: 'promiseish.js',
    data:
     [ [ 100, 90 ],
       [ 500, 504 ],
       [ 1000, 908 ],
       [ 1500, 1234 ],
       [ 2000, 1730 ] ] },
  { label: 'promiseishQ.js',
    data:
     [ [ 100, 94 ],
       [ 500, 565 ],
       [ 1000, 1166 ],
       [ 1500, 1778 ],
       [ 2000, 2519 ] ] },
  { label: 'promises.js',
    data:
     [ [ 100, 229 ],
       [ 500, 1233 ],
       [ 1000, 2284 ],
       [ 1500, 3860 ],
       [ 2000, 5919 ] ] },
  { label: 'qasync.js',
    data:
     [ [ 100, 88 ],
       [ 500, 487 ],
       [ 1000, 950 ],
       [ 1500, 1604 ],
       [ 2000, 2155 ] ] },
  { label: 'rx.js',
    data:
     [ [ 100, 102 ],
       [ 500, 419 ],
       [ 1000, 760 ],
       [ 1500, 1117 ],
       [ 2000, 1526 ] ] },
  { label: 'suspend.js',
    data:
     [ [ 100, 75 ],
       [ 500, 343 ],
       [ 1000, 677 ],
       [ 1500, 1032 ],
       [ 2000, 1422 ] ] } ]

.concat(
[ { label: 'dst-streamline-fibers.js',
    data:
     [ [ 100, 94 ],
       [ 500, 446 ],
       [ 1000, 925 ],
       [ 1500, 1353 ],
       [ 2000, 2348 ] ] },
  { label: 'fibrous.js',
    data:
     [ [ 100, 149 ],
       [ 500, 534 ],
       [ 1000, 1206 ],
       [ 1500, 2280 ],
       [ 2000, 4136 ] ] } ]

       );
window.addEventListener('load', function() {
    $.plot('#perf-time-3', perfIOBound, {legend: { position: 'nw' }})
});
</script>

<p><code>promises.js</code> and <code>fibrous.js</code> are still significantly slower. However all of
the other solutions are quite comparable now. Lets remove the worst two:</p>
<p><div id="perf-time-4" class="plot">
</div></p>
<script type="text/javascript">
window.addEventListener('load', function() {
    $.plot('#perf-time-4', perfIOBound.filter(function(item) {
        return !/(promises.js|fibrous.js)/.test(item.label)
    }), {legend: { position: 'nw' }});
});
</script>

<p>Everything is about the same now. Great! So in practice, you won&#39;t notice
the CPU overhead in I/O bound cases - even if you&#39;re using promises. And with
some of the generator libraries, the overhead becomes practically invisible.</p>
<p>Excellent. But what about memory usage? Lets chart that too!</p>
<p>Note: the y axis represents peak memory usage (in MB).</p>
<p><div id="perf-mem-1" class="plot">
</div></p>
<script type="text/javascript">

window.perfMEM =

[ { label: 'async.js',
    data:
     [ [ 100, 0.75 ],
       [ 500, 3.9140625 ],
       [ 1000, 7.359375 ],
       [ 1500, 10.93359375 ],
       [ 2000, 12.71875 ] ] },
{ label: 'catcher.js',
    data:
     [ [ 100, 0.91015625 ],
       [ 500, 3.34765625 ],
       [ 1000, 6.46484375 ],
       [ 1500, 9.1953125 ],
       [ 2000, 11.01171875 ] ] },
  { label: 'co.js',
    data:
     [ [ 100, 1.703125 ],
       [ 500, 6.5390625 ],
       [ 1000, 11.16796875 ],
       [ 1500, 17.43359375 ],
       [ 2000, 24.515625 ] ] },
  { label: 'dst-co-traceur.js',
    data:
     [ [ 100, 0.09765625 ],
       [ 500, 1.09765625 ],
       [ 1000, 11.08984375 ],
       [ 1500, 17.68359375 ],
       [ 2000, 22.8203125 ] ] },
  { label: 'dst-genny-traceur.js',
    data:
     [ [ 100, -0.43359375 ],
       [ 500, 1.7265625 ],
       [ 1000, 9.1484375 ],
       [ 1500, 16.30859375 ],
       [ 2000, 15.6796875 ] ] },
  { label: 'dst-qasync-traceur.js',
    data:
     [ [ 100, 10.33203125 ],
       [ 500, 59.55859375 ],
       [ 1000, 101.76953125 ],
       [ 1500, 135.78125 ],
       [ 2000, 190.25 ] ] },
  { label: 'dst-stratifiedjs-014.js',
    data:
     [ [ 100, 2.21875 ],
       [ 500, 11.32421875 ],
       [ 1000, 23.32421875 ],
       [ 1500, 38.71875 ],
       [ 2000, 48.015625 ] ] },
  { label: 'dst-streamline.js',
    data:
     [ [ 100, 1.5078125 ],
       [ 500, 3.76953125 ],
       [ 1000, 8.3359375 ],
       [ 1500, 10.25 ],
       [ 2000, 15.890625 ] ] },
  { label: 'dst-suspend-traceur.js',
    data:
     [ [ 100, -0.30078125 ],
       [ 500, 2.234375 ],
       [ 1000, 6.5390625 ],
       [ 1500, 11.125 ],
       [ 2000, 15.484375 ] ] },
  { label: 'flattened-class.js',
    data:
     [ [ 100, 0.53125 ],
       [ 500, 3.1171875 ],
       [ 1000, 4.484375 ],
       [ 1500, 7.96484375 ],
       [ 2000, 8.58984375 ] ] },
  { label: 'flattened.js',
    data:
     [ [ 100, 0.515625 ],
       [ 500, 3.25390625 ],
       [ 1000, 5.3203125 ],
       [ 1500, 8.78515625 ],
       [ 2000, 9.765625 ] ] },
  { label: 'flattened-noclosure.js',
    data:
     [ [ 100, 0.51953125 ],
       [ 500, 3.03515625 ],
       [ 1000, 4.8984375 ],
       [ 1500, 8.08203125 ],
       [ 2000, 8.87890625 ] ] },
  { label: 'genny.js',
    data:
     [ [ 100, 1.11328125 ],
       [ 500, 5.48046875 ],
       [ 1000, 11.6953125 ],
       [ 1500, 16.7890625 ],
       [ 2000, 21.2734375 ] ] },
  { label: 'gens.js',
    data:
     [ [ 100, 0.65625 ],
       [ 500, 3.91015625 ],
       [ 1000, 8.40234375 ],
       [ 1500, 11.921875 ],
       [ 2000, 15.95703125 ] ] },
  { label: 'original.js',
    data:
     [ [ 100, 0.63671875 ],
       [ 500, 3.1640625 ],
       [ 1000, 5.328125 ],
       [ 1500, 8.69921875 ],
       [ 2000, 9.54296875 ] ] },
  { label: 'promiseish.js',
    data:
     [ [ 100, 17.9140625 ],
       [ 500, 89.01171875 ],
       [ 1000, 117.94921875 ],
       [ 1500, 129.1640625 ],
       [ 2000, 231.34765625 ] ] },
  { label: 'promiseishQ.js',
    data:
     [ [ 100, 16.2421875 ],
       [ 500, 77.6953125 ],
       [ 1000, 98.296875 ],
       [ 1500, 135.9609375 ],
       [ 2000, 142.94140625 ] ] },
  { label: 'promises.js',
    data:
     [ [ 100, 42.97265625 ],
       [ 500, 121.71484375 ],
       [ 1000, 240.53515625 ],
       [ 1500, 359.94921875 ],
       [ 2000, 481.65625 ] ] },
  { label: 'qasync.js',
    data:
     [ [ 100, 11.796875 ],
       [ 500, 57.00390625 ],
       [ 1000, 97.4765625 ],
       [ 1500, 149.5078125 ],
       [ 2000, 163.91796875 ] ] },
  { label: 'rx.js',
    data:
     [ [ 100, 3.9375 ],
       [ 500, 21.34375 ],
       [ 1000, 40.43359375 ],
       [ 1500, 62.11328125 ],
       [ 2000, 62.05859375 ] ] },
  { label: 'suspend.js',
    data:
     [ [ 100, 0.71875 ],
       [ 500, 4.09375 ],
       [ 1000, 8.60546875 ],
       [ 1500, 10.1171875 ],
       [ 2000, 14.28125 ] ] } ]

  .concat(
  [ { label: 'dst-streamline-fibers.js',
    data:
     [ [ 100, 1.80078125 ],
       [ 500, 8.53125 ],
       [ 1000, 17.05859375 ],
       [ 1500, 27.98828125 ],
       [ 2000, 33.98671875 ] ] },
  { label: 'fibrous.js',
    data:
     [ [ 100, 7.05078125 ],
       [ 500, 28.5859375 ],
       [ 1000, 56.75390625 ],
       [ 1500, 84.2734375 ],
       [ 2000, 63.140625 ] ] } ]

       );

window.addEventListener('load', function() {
    $.plot('#perf-mem-1', perfMEM, {legend: { position: 'nw' },
yaxis: {min: 0}});
});
</script>

<p>Seems like promises also use a lot of memory, especially the extreme
implementation <code>promises.js</code>. <code>promiseish.js</code> as well as <code>qasync.js</code> are not
too far behind.</p>
<p><code>fibrous.js</code>, <code>rx.js</code> and <code>stratifiedjs</code> are somewhat better than the above,
however their memory usage is still over 5 times bigger than the original.</p>
<p>Lets remove the hogs and see what remains underneath.</p>
<p><div id="perf-mem-2" class="plot">
</div></p>
<script type="text/javascript">
window.addEventListener('load', function() {
    $.plot('#perf-mem-2', perfMEM.filter(function(item) {
        return !/(promises|promiseish|qasync|fibrous|rx.js|stratifiedjs)/
            .test(item.label)
    }), {legend: { position: 'nw' }, yaxis: {min: 0}});
});
</script>

<p>Streamline&#39;s fibers implementation uses 35MB while the rest use between
10MB and 25MB.</p>
<p>This is amazing. Generators (without promises) also have a low memory overhead,
even when compiled with traceur.</p>
<p>Streamline is also quite good in this category. It has very low overhead, both
in CPU and memory usage.</p>
<p>Its important to note that the testing method that I use is not statistically
sound. Its however good enough to be used to compare orders of magnitude, which
is fine considering the narrowly defined benchmark.</p>
<p>With that said, here is a table for 1000 parallel requests with 10 ms response
time for I/O operations (i.e. 100K IO / s)</p>
<table>
<thead>
<tr>
<th style="text-align:left">file</th>
<th style="text-align:right">time(ms)</th>
<th style="text-align:right">memory(MB)</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">suspend.js</td>
<td style="text-align:right">101</td>
<td style="text-align:right">8.62</td>
</tr>
<tr>
<td style="text-align:left">flattened-class.js</td>
<td style="text-align:right">111</td>
<td style="text-align:right">4.48</td>
</tr>
<tr>
<td style="text-align:left">flattened-noclosure.js</td>
<td style="text-align:right">124</td>
<td style="text-align:right">5.04</td>
</tr>
<tr>
<td style="text-align:left">flattened.js</td>
<td style="text-align:right">125</td>
<td style="text-align:right">5.18</td>
</tr>
<tr>
<td style="text-align:left">original.js</td>
<td style="text-align:right">130</td>
<td style="text-align:right">5.33</td>
</tr>
<tr>
<td style="text-align:left">async.js</td>
<td style="text-align:right">135</td>
<td style="text-align:right">7.36</td>
</tr>
<tr>
<td style="text-align:left">dst-streamline.js</td>
<td style="text-align:right">139</td>
<td style="text-align:right">8.34</td>
</tr>
<tr>
<td style="text-align:left">catcher.js</td>
<td style="text-align:right">140</td>
<td style="text-align:right">6.45</td>
</tr>
<tr>
<td style="text-align:left">dst-suspend-traceur.js</td>
<td style="text-align:right">142</td>
<td style="text-align:right">6.76</td>
</tr>
<tr>
<td style="text-align:left">gens.js</td>
<td style="text-align:right">149</td>
<td style="text-align:right">8.40</td>
</tr>
<tr>
<td style="text-align:left">genny.js</td>
<td style="text-align:right">161</td>
<td style="text-align:right">11.69</td>
</tr>
<tr>
<td style="text-align:left">co.js</td>
<td style="text-align:right">182</td>
<td style="text-align:right">11.14</td>
</tr>
<tr>
<td style="text-align:left">dst-genny-traceur.js</td>
<td style="text-align:right">250</td>
<td style="text-align:right">8.84</td>
</tr>
<tr>
<td style="text-align:left">dst-stratifiedjs-014.js</td>
<td style="text-align:right">267</td>
<td style="text-align:right">23.55</td>
</tr>
<tr>
<td style="text-align:left">dst-co-traceur.js</td>
<td style="text-align:right">284</td>
<td style="text-align:right">13.54</td>
</tr>
<tr>
<td style="text-align:left">rx.js</td>
<td style="text-align:right">295</td>
<td style="text-align:right">40.43</td>
</tr>
<tr>
<td style="text-align:left">dst-streamline-fibers.js</td>
<td style="text-align:right">526</td>
<td style="text-align:right">17.05</td>
</tr>
<tr>
<td style="text-align:left">promiseish.js</td>
<td style="text-align:right">825</td>
<td style="text-align:right">117.88</td>
</tr>
<tr>
<td style="text-align:left">qasync.js</td>
<td style="text-align:right">971</td>
<td style="text-align:right">98.39</td>
</tr>
<tr>
<td style="text-align:left">fibrous.js</td>
<td style="text-align:right">1159</td>
<td style="text-align:right">57.48</td>
</tr>
<tr>
<td style="text-align:left">promiseishQ.js</td>
<td style="text-align:right">1161</td>
<td style="text-align:right">96.47</td>
</tr>
<tr>
<td style="text-align:left">dst-qasync-traceur.js</td>
<td style="text-align:right">1195</td>
<td style="text-align:right">112.10</td>
</tr>
<tr>
<td style="text-align:left">promises.js</td>
<td style="text-align:right">2315</td>
<td style="text-align:right">240.39</td>
</tr>
</tbody>
</table>
<p><a name="debuggability"></a></p>
<h3 id="debuggability">Debuggability</h3>
<p>Having good performance is important. However, all the performance is worth
nothing if our code doesn&#39;t do what its supposed to. Debugging is therefore
at least as important as performance.</p>
<p>How can we measure debuggability? We can look at source maps support and
the generated stack traces.</p>
<p><a name="source-maps-support"></a></p>
<h4 id="source-maps-support">Source maps support</h4>
<p>I split this category into 5 levels:</p>
<ul>
<li><p><strong>level 1</strong>: no source maps, but needs them (wacky stack trace line numbers)</p>
</li>
<li><p><strong>level 2</strong>: no source maps and needs them sometimes (to view the original
code)</p>
<p>Streamline used to be in this category but now it does have source maps
support.</p>
</li>
<li><p><strong>level 3</strong>: has source maps and needs them always.</p>
<p>Nothing is in this category.</p>
</li>
<li><p><strong>level 4</strong>: has source maps and needs them sometimes</p>
<p>Generator libraries are in this category. When compiled with traceur (e.g.
for the browser) source maps are required and needed. If ES6 is available,
source maps are unnecessary.</p>
<p>Streamline is also in this category for another reason. With streamline,
you don&#39;t need source maps to get accurate stack traces. However, you will
need them if you want to read the original code (e.g. when debugging in
the browser).</p>
</li>
<li><p><strong>level 5</strong>: doesn&#39;t need source maps</p>
<p>Everything else is in this category. That&#39;s a bit unfair as fibers will never
work in a browser.</p>
</li>
</ul>
<p><a name="stack-trace-accuracy"></a></p>
<h4 id="stack-trace-accuracy">Stack trace accuracy</h4>
<p>This category also has 5 levels:</p>
<ul>
<li><p><strong>level 1</strong>: stack traces are missing</p>
<p><code>suspend</code>, <code>co</code> and <code>gens</code> are in this category. When an error happens in one
of the async functions, this is how the result looks like:</p>
<pre><code>Error: Error happened
  at null._onTimeout (/home/spion/Documents/tests/async-compare/lib/fakes.js:27:27)
  at Timer.listOnTimeout [as ontimeout] (timers.js:105:15)
</code></pre><p>No mention of the original file, <code>examples/suspend.js</code></p>
<p>Unfortunately, if you throw an error to a generator using
<code>iterator.throw(error)</code>, the last yield point will not be present in the
resulting stack trace. This means you will have no idea which line in your
generator is the offending one.</p>
<p>Regular exceptions that are not thrown using <code>iterator.throw</code> have complete
stack traces, so only yield points will suffer.</p>
<p>Some solutions that aren&#39;t generator based are also in this category, namely
<code>promiseish.js</code> and <code>async.js</code>. When a library handles errors for you, the
callback stack trace will not be preserved unless special care is taken to
preserve it. <code>async</code> and <code>when</code> don&#39;t do that.</p>
</li>
<li><p><strong>level 2</strong>: stack traces are correct with native modules</p>
<p>Bruno Jouhier&#39;s generator based solution <a href="//github.com/bjouhier/galaxy">galaxy</a>
is in this category. It has a native companion module called
<a href="//github.com/bjouhier/galaxy-stack">galaxy-stack</a> that implements long stack
traces without a performance penalty.</p>
<p>Note that galaxy-stack doesn&#39;t work with node v0.11.5</p>
</li>
<li><p><strong>level 3</strong>: stack traces are correct with a flag (adding a performance
penalty).</p>
<p>All Q-based solutions are here, even <code>qasync.js</code>, which uses generators. Q&#39;s
support for stack traces via <code>Q.longStackSupport = true;</code> is good:</p>
<pre><code>Error: Error happened
    at null._onTimeout (/home/spion/Documents/tests/async-compare/lib/fakes.js:27:27)
    at Timer.listOnTimeout [as ontimeout] (timers.js:105:15)
From previous event:
    at /home/spion/Documents/tests/async-compare/examples/qasync.js:41:18
    at GeneratorFunctionPrototype.next (native)
</code></pre><p>So, does this mean that its possible to add long stack traces support to a
callbacks-based generator library the way that Q does it?</p>
<p>Yes it does! Genny is in this category too:</p>
<pre><code>Error: Error happened
    at null._onTimeout (/home/spion/Documents/tests/async-compare/lib/fakes.js:27:27)
    at Timer.listOnTimeout [as ontimeout] (timers.js:105:15)
From generator:
    at upload (/home/spion/Documents/tests/async-compare/examples/genny.js:38:35)
</code></pre><p>However it incurs about 50-70% memory overhead and is about 6 times slower.</p>
<p>Catcher is also in this category, with 100% memory overhead and about
10 times slower.</p>
</li>
<li><p><strong>level 4</strong>: stack traces are correct but fragile</p>
<p>All the raw-callback solutions are in this category: original, flattened,
flattened-class, etc. At the moment, rx.js is in this category too.</p>
<p>As long as the callback functions are defined by your code everything will
be fine. However, the moment you introduce some wrapper that handles the
errors for you, your stack traces will break and will show functions from the
wrapper library instead.</p>
</li>
<li><p><strong>level 5</strong>: stack traces are always correct</p>
<p>Streamline and fibers are in this category. Streamline compiles the file in a
way that preserves line numbers, making stack traces correct in all cases.
Fibers also preserve the full call stack.</p>
</li>
</ul>
<p>Ah yes. A table.</p>
<table>
<thead>
<tr>
<th>name</th>
<th style="text-align:right">source maps</th>
<th style="text-align:right">stack traces</th>
<th style="text-align:right">total</th>
</tr>
</thead>
<tbody>
<tr>
<td>fibrous.js</td>
<td style="text-align:right">5</td>
<td style="text-align:right">5</td>
<td style="text-align:right">10</td>
</tr>
<tr>
<td>src-streamline._js</td>
<td style="text-align:right">4</td>
<td style="text-align:right">5</td>
<td style="text-align:right">9</td>
</tr>
<tr>
<td>original.js</td>
<td style="text-align:right">5</td>
<td style="text-align:right">4</td>
<td style="text-align:right">9</td>
</tr>
<tr>
<td>flattened*.js</td>
<td style="text-align:right">5</td>
<td style="text-align:right">4</td>
<td style="text-align:right">9</td>
</tr>
<tr>
<td>rx.js</td>
<td style="text-align:right">5</td>
<td style="text-align:right">4</td>
<td style="text-align:right">9</td>
</tr>
<tr>
<td>catcher.js</td>
<td style="text-align:right">5</td>
<td style="text-align:right">3</td>
<td style="text-align:right">8</td>
</tr>
<tr>
<td>promiseishQ.js</td>
<td style="text-align:right">5</td>
<td style="text-align:right">3</td>
<td style="text-align:right">8</td>
</tr>
<tr>
<td>qasync.js</td>
<td style="text-align:right">4</td>
<td style="text-align:right">3</td>
<td style="text-align:right">7</td>
</tr>
<tr>
<td>genny.js</td>
<td style="text-align:right">4</td>
<td style="text-align:right">3</td>
<td style="text-align:right">7</td>
</tr>
<tr>
<td>async.js</td>
<td style="text-align:right">5</td>
<td style="text-align:right">1</td>
<td style="text-align:right">6</td>
</tr>
<tr>
<td>promiseish.js</td>
<td style="text-align:right">5</td>
<td style="text-align:right">1</td>
<td style="text-align:right">6</td>
</tr>
<tr>
<td>promises.js</td>
<td style="text-align:right">5</td>
<td style="text-align:right">1</td>
<td style="text-align:right">6</td>
</tr>
<tr>
<td>suspend.js</td>
<td style="text-align:right">4</td>
<td style="text-align:right">1</td>
<td style="text-align:right">5</td>
</tr>
<tr>
<td>gens.js</td>
<td style="text-align:right">4</td>
<td style="text-align:right">1</td>
<td style="text-align:right">5</td>
</tr>
<tr>
<td>co.js</td>
<td style="text-align:right">4</td>
<td style="text-align:right">1</td>
<td style="text-align:right">5</td>
</tr>
</tbody>
</table>
<p>Generators are not exactly great. They&#39;re doing well enough thanks to qasync
and genny.</p>
<p><a name="conclusion"></a></p>
<h3 id="conclusion">Conclusion</h3>
<p>If this analysis left you even more confused than before, you&#39;re not alone. It
seems hard to make a decision even with all the data available.</p>
<p>My opinion is biased. I love generators, and I&#39;ve been
<a href="https://code.google.com/p/v8/issues/detail?id=2355#c2">pushing</a>
<a href="https://news.ycombinator.com/item?id=5419030">pretty hard</a> to direct the
attention of V8 developers to them (maybe a bit too hard). And its obvious
from the analysis above that they have good characteristics: low code
complexity, good performance.</p>
<p>More importantly, they will eventually become a part of everyday JavaScript
with no compilation (except for older browsers) or native modules required,
and the yield keyword is in principle as good indicator of async code as
callbacks are.</p>
<p>Unfortunately, the debugging story for generators is somewhat bad, especially
because of the missing stack traces for thrown errors. Fortunately, there are
solutions and workarounds, like those implemented by genny (obtrusive, reduces
performance) and galaxy (unobtrusive, but requires native modules).</p>
<p>But there are things that cannot be measured. How will the community accept
generators? Will people find it hard to decide whether to use them or not? Will
they be frowned upon when used in code published to npm?</p>
<p>I don&#39;t have the answers to these questions. I only have hunches. But they are
generally positive. Generators will play an important role in the future of
node.</p>
<hr>
<p>Special thanks to
<a href="//github.com/Raynos">Raynos</a>,
<a href="//github.com/maxogden">maxogden</a>,
<a href="//github.com/mikeal">mikeal</a>
and <a href="//github.com/DamonOehlman">damonoehlman</a>
for their input on the draft version of this analysis.</p>
<p>Thanks to <a href="//github.com/jmar777">jmar777</a> for making suspend</p>
]]></description>
            <link>https://spion.github.io/posts/analysis-generators-and-other-async-patterns-node.html</link>
            <guid isPermaLink="true">https://spion.github.io/posts/analysis-generators-and-other-async-patterns-node.html</guid>
            <pubDate>Fri, 09 Aug 2013 00:00:00 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[Google Docs on the iPad]]></title>
            <description><![CDATA[<iframe allowfullscreen="allowfullscreen" frameborder="0" height="360"
    src="https://www.youtube.com/embed/3fmfbAJcfKY" width="640"></iframe>

<p>This is Appser for Google Docs. It allows you to run the full, web version of Google Docs on the iPad. And by that we mean all of it, even editing presentations (a feature which was recently completely disabled on the iPad by Google)</p>
<p>And its available on the app store, right now, for free.
<a href="https://itunes.apple.com/us/app/appser-for-google-docs/id577825348?ls=1&amp;mt=8">Get Appser for Google Docs</a>.</p>
<p>At this point you&#39;re probably thinking: Wait, isn&#39;t this horribly problematic?
Google Docs isn&#39;t optimized for touch input at all - neither the code nor the UI
elements. Selecting text is weird. Scrolling is incompatible with dragging to
select text or to move stuff. Those image boxes have really tiny resizers. The
toolbar buttons and menus are not big enough. What about right click?</p>
<p>Is it actually usable?</p>
<p>Also, can I just connect a bigger monitor, some kind of a touchpad and a
keyboard and forget about touch input?</p>
<p>The answer to all questions is yes.</p>
<h2 id="scroll-vs-drag-and-text-selection">Scroll vs drag and text selection</h2>
<p>When Apple first designed the iPhone, it had a 3.5 inch screen with a resolution
of 480x320 pixels. While this was quite a lot compared to other phones of the
time, it was not even near desktop and laptop displays. Yet it had a browser
that provided an experience as close to desktop as possible. How did they
achieve this?</p>
<p>They made scrolling and zooming really easy. This meant that the user was able
to quickly zoom to compensate for the lacking resolution, then quickly pan to
show more on the limited screen real-estate. And so, web browsing was very
usable even on the first iPhone.</p>
<p>Fast forward to today, on the iPad, we have a 10 inch retina-resolution
display... and the same zoom and pan control.</p>
<p>Why?</p>
<p>Do we really need this? Its very problematic for web apps. They like to assume
that drag and drop works. They want to display a static chrome around a
scrolling document, so they also like to assume that overflow: scroll works.</p>
<p>As a result, we decided to make scrolling work by disabling single-finger
scrolling. Instead we made single-finger actions behave exactly like mouse
actions do in the browser and switched to two-finger scrolling, making it work
everywhere. This enables you to quickly select text and drag things around.</p>
<p>We also disabled zoom - web apps usually have their own document zooming
facilities and the iPad screen has plenty of resolution and size.</p>
<h2 id="what-about-tiny-buttons-and-controls-">What about tiny buttons and controls?</h2>
<p>This is where things got interesting. We knew that even though we enabled drag and drop, the controls were too tiny for touch use. To overcome this we developed a method called Magnet Touch. It automatically enlarges all touch targets. The enlargement is not visual so it simply makes smaller targets easier to hit. Its also smart - it tries very hard to avoid overlap with other targets. Best of all, you don&#39;t have to tell it anything about the targets - not even which elements are targets.</p>
<h2 id="and-right-click-">And right click?</h2>
<p>Long taps are the equivalent of right clicks. At the moment they trigger when you release the finger.</p>
<h2 id="what-happens-when-i-connect-an-external-screen-">What happens when I connect an external screen?</h2>
<p>Your iPad becomes a keyboard + touchpad combo and you get a mouse pointer on the screen. Neat huh? If you prefer a physical Bluetooth keyboard you can of course connect one at any time.</p>
<p>There are some limitations when editing text fields (like the username and password on the login screen) - they must be shown on the iPad temporarily while editing. We&#39;re working on that.</p>
<h2 id="conclusion">Conclusion</h2>
<p>What, you&#39;re still reading this? <a href="https://itunes.apple.com/us/app/appser-for-google-docs/id577825348?ls=1&amp;mt=8">Download the app and try it out</a>. We&#39;d also <a href="http://appser.docucalc.com/support">love to hear from you</a>. You can also <a href="http://appser.docucalc.com/">visit the official Appser website</a>, if you&#39;d like to find out more.</p>
]]></description>
            <link>https://spion.github.io/posts/make-most-webapps-work-on-ipad.html</link>
            <guid isPermaLink="true">https://spion.github.io/posts/make-most-webapps-work-on-ipad.html</guid>
            <pubDate>Thu, 13 Dec 2012 00:00:00 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[Introducing npmsearch]]></title>
            <description><![CDATA[<p>Node&#39;s package manager <a href="https://npmjs.org/">npm</a> is a wonderful tool.</p>
<p>It handles dependencies and versions the right way. It requires simple, easy to write package metadata. It uses a central registry (by default) which makes installing modules easier. The central registry is <a href="http://couchdb.apache.org/">CouchDB</a> which basically makes it completely transparent and available to everyone.</p>
<p>It does many things right.</p>
<p>But it doesn&#39;t do search that well.</p>
<pre><code>9134 % npm search orm
npm http GET https://registry.npmjs.org/-
/all/since?stale=update_after&amp;amp;startkey=1353539108378
npm http 200 https://registry.npmjs.org/-
/all/since?stale=update_after&amp;amp;startkey=1353539108378

NAME                  DESCRIPTION
2csv                  A pluggable file format converter into Co...
abnf                  Augmented Backus-Naur Form (ABNF) parsing.
accounting            number, money and currency parsing/formatt..
activerecord          An ORM that supports multiple database sys..
addressit             Freeform Street Address Parser
...
[snip]
...
</code></pre><p>What just happened here?</p>
<p>Here is what happened: npm search gave us all packages that contain the substring &quot;orm&quot;. Anywhere.</p>
<p>You might argue that this works well with bigger words. Its true that results are slightly better with bigger words but they&#39;re still not sorted in any meaningful way (alphabetically sorting search results isn&#39;t very meaningful)</p>
<pre><code>9144 % npm search mysql
NAME                  DESCRIPTION
Accessor_MySQL        A MySQL database wrapper, provide ...
any-db                Database-agnostic connection pool ...
autodafe              mvc framework for node with mysql ...
connect-mysql         a MySQL session store for connect ...
connect-mysql-session A MySQL session store for node.js ...
cormo                 ORM framework for Node.js...
...
[snip]
...
</code></pre><p>Hence one of the common activities to do when researching node modules is to go to the #node.js IRC channel and ask the people there for a <strong>good</strong> library that does X.</p>
<p>I decided to make a package that helps with this, called npmsearch. Its a command-line tool that allows you to search the npm registry by keywords and it sorts the results using relevance and the number of downloads that the package has.</p>
<p>Install it using npm:</p>
<pre><code>[sudo] npm install -g npmsearch
</code></pre><p>then use it from the command line:</p>
<pre><code>9147 % npmsearch mysql
* mysql (6 15862)
     A node.js driver for mysql. It is written in JavaScript, does
     not  require compiling, and is 100% MIT licensed.
     by Felix Geisendörfer &lt;felix@debuggable.com&gt;

* mongoose (2 28197)
     Mongoose MongoDB ODM
     by Guillermo Rauch &lt;guillermo@learnboost.com&gt;
     http://github.com/LearnBoost/mongoose.git

* patio (10 174)
     Patio query engine and ORM
     by Doug Martin &lt;undefined&gt;
     git@github.com:c2fo/patio.git

* mysql-libmysqlclient (5 1019)
     Binary MySQL bindings for Node.JS
     by Oleg Efimov &lt;efimovov@gmail.com&gt;
     https://github.com/Sannis/node-mysql-libmysqlclient.git

* db-mysql (3 918)
     MySQL database bindings for Node.JS

* sql (6 51)
     sql builder
     by brianc &lt;brian.m.carlson@gmail.com&gt;
     http://github.com/brianc/node-sql.git

* sequelize (2 2715)
     Multi dialect ORM for Node.JS
     by Sascha Depold
</code></pre><p>If you want to try it out without installing it,
<a href="http://npmsearch.docucalc.com/">you can try it online</a>, or you can
<a href="https://github.com/spion/npmsearch">visit the project page on github</a></p>
<p>The implemented keyword search is non-trivial: it applies the
<a href="http://tartarus.org/martin/PorterStemmer/">Porter Stemmer</a> to the keywords and
expands the set provided by you with statistically picked commonly co-occuring
keywords. (e.g. mongo will expand to mongo mongodb)</p>
<p>Results are sorted by a combined factor which incorporates keyword relevance
and &quot;half-lifed&quot; downloads. You can control the importance of each factor in
the sorting process using command-line options - and there are many:</p>
<ul>
<li>relevance - how big of a factor should keyword relevance be, default 2</li>
<li>downloads - how big of a factor is the number of downloads, default 0.25</li>
<li>halflife  - the halflife of downloads e.g. 7 means downloads that are 7
days old lose half of their value, default 30</li>
<li>limit     - number of results to display, default 7</li>
<li>freshness - update the database if older than &quot;freshness&quot; days, default 1.5</li>
</ul>
<p>I hope this will help fellow nodesters find their next favorite modules</p>
<p>Have fun!</p>
]]></description>
            <link>https://spion.github.io/posts/introducing-npmsearch.html</link>
            <guid isPermaLink="true">https://spion.github.io/posts/introducing-npmsearch.html</guid>
            <pubDate>Tue, 27 Nov 2012 00:00:00 GMT</pubDate>
        </item>
        <item>
            <title><![CDATA[Fixing Hacker News with a reputation system]]></title>
            <description><![CDATA[<p>There is a certain phenomenon that seems to happen in almost every online
community of user-generated content. A community is created: the initial users
define the values of this new community. After a while the community experiences
growth in numbers. As a result of that growth, users that joined before it feel
like its no longer the same community with the same values. The latest widely
discussed example seems to be
<a href="http://news.ycombinator.com/item?id=4396747">Hacker News</a>.</p>
<p><a href="http://news.ycombinator.com/item?id=4397542">Paul Graham responds</a> that the
reasons are mostly: a shift in values, increase of anonymity and the fact that
its easier to vote than to contribute content</p>
<blockquote>
<p><em>It&#39;s a genuine problem and has been growing gradually worse for a while. I think the cause is simply growth. When a good community grows, it becomes worse in two ways: (a) more recent arrivals don&#39;t have as much of whatever quality distinguished the original members, and (b) the large size of the group makes people behave worse, because there is more anonymity in a larger group.</em></p>
<p><em>I&#39;ve spent many hours over the past several years trying to understand and mitigate such problems. I&#39;ve come up with a bunch of tweaks that worked, and I have hopes I&#39;ll be able to come up with more.</em></p>
<p><em>The idea I&#39;m currently investigating, in case anyone is curious, is that
votes rather than comments may be the easiest place to attack this problem.
Although snarky comments themselves are the most obvious symptom, I suspect
that voting is on average dumber than commenting, because it requires so much
less work. So I&#39;m going to try to see if it&#39;s possible to identify people who
consistently upvote nasty comments and if so count their votes less.</em></p>
</blockquote>
<p>As online communities grow, the values of the group shift. The majority now may
or may not hold the same values as the majority before. The question is, how to
preserve the old values of the group with minimum side-effects?</p>
<p>As it happens, my master&#39;s thesis was an attempt to fix exactly this problem
and implement an improved voting system tailored specifically for communities
with user-submitted content. I won&#39;t provide a link to the thesis as its not
written in English, but I&#39;ll try to summarize the gist of it.</p>
<p>The voting system used in most communities today (democratic voting) is the one
most susceptible to value shift when significant growth occurs. Its
no surprise: democratic systems are designed to measure what the majority
values. When significant growth occurs, the majority changes and therefore what
they value also changes.</p>
<p>In contrast, previous moderator/editor based systems offer a strict filter on
content based on the more static values of the current set of editors. However,
it has the downside of being limited to what the editors are able to review and
publish.</p>
<p>I propose a hybrid feedback-loop based system. In this system people have
variable voting influence and editor-like individuals are given as a
&quot;reference point&quot; or exemplary users with maximum voting influence. The
system attempts to find out what they value and recognize it in others.</p>
<p>The system is largely based on
<a href="http://www.unik.no/people/josang/papers/JI2002-Bled.pdf">the beta reputation system</a>,
which is used to measure trust in online e-commerce communities.</p>
<p>Here is a short description of the system:</p>
<ul>
<li>Voting influence is not the same for all users: its not 1 (+1 or -1) for
everyone but in the range 0-1.</li>
<li>When a user votes for a content item, they also vote for the creator (or
submitter) of the content.</li>
<li>The voting influence of a user is calculated using the positive and negative
votes that he has received for his submissions.</li>
<li>Exemplary users always have a static maximum influence.</li>
</ul>
<p>Suppose we have a content item \(C\) submitted by the user \(U_c\). Now a voter
\(V\) comes to vote for it and clicks on the +1 button.</p>
<p>The voter has his own submissions for which he has received a total amount of
positive vote \(p_V\) and a total amount of negative vote \(n_V\). As a result,
his voting influence \(i_V\) is modified: its not +1 but calculated according to
the formula:</p>
<p>$$ i_V = f_W(p_V, n_V) $$</p>
<p>where \(f_W\) is the
<a href="http://evanmiller.org/how-not-to-sort-by-average-rating.html">lower bound of Wilson score confidence interval</a>.
While a simple average such as:</p>
<p>$$ i_V = \frac{p_V}{p_V + n_V} $$</p>
<p>might work when the number of positive and negative votes is large enough, its
not good enough when the number of votes is low. The Wilson score confidence
interval gives us a better, flexible balance between desired certainty in the
result and the result itself.</p>
<p>This vote in turn is received by the content item \(C\). Because its a positive
vote, the amount of positive vote \(p_C\) is changed for this content item</p>
<p>$$ p_C \leftarrow p_C + i_V $$</p>
<p>and as a result, it has a new rating</p>
<p>$$ r_c = f_W(p_c, n_c) $$</p>
<p>but the positive points \(p_U\) of the creator of the content item are also
changed:</p>
<p>$$ p_U \leftarrow p_U + i_V $$</p>
<p>and as a result the voting influence \(i_U\) of submitter is also changed:</p>
<p>$$ i_U = f_W(p_U, n_U) $$</p>
<p>or in other words, he has &quot;earned&quot; a bigger influence in the voting system by
submitting a well-rated content item.</p>
<p>This means that new members have no voting influence. As they submit content and
receive votes their influence may rise if the existing users with high influence
in the system consider their content to be good.</p>
<p>This is where the reference users \(R\) come in. Their influence is fixed to
always be 1</p>
<p>$$ i_R = 1 $$</p>
<p>Because of this, influence propagates through the system from them to other
users who submit content which is deemed high-quality by the reference users.
Those users in turn also change influence by voting for others and so forth.</p>
<p>Its also possible to scale down votes as they age. The two possible strategies
are to scale all \(p_X\) and \(n_X\) values daily, for all content items and all
users by multiplying them with a certain aging factor \(k_a\)</p>
<p>$$ p_X \leftarrow k_a p_X $$</p>
<p>$$ n_X \leftarrow k_a n_X $$</p>
<p>or to simply keep all positive and negative votes \(V_p\) and \(V_n\) in the
database and recalculate \(p_X\) and \(n_X\) according to the age of the votes
\(a_V\), for example:</p>
<p>$$ p<em>X = \sum</em>{\forall V_p} { i_V k_a^{a_V} } $$</p>
<p>$$ n<em>X = \sum</em>{\forall V_n} { i_V k_a^{a_V} } $$</p>
<p>One of the convenient aspects of this system is that its easy to test-drive. It
doesn&#39;t require more user action than simple democratic voting. It only requires
an administrator to specify some reference users at the start which seed and
then propagate influence throughout the system.</p>
<p>I tested this system on a forum dataset (details available on request) and found
that the system achieves around 50% reduction of difference from a moderator
only system compared to scores of a democratic system, even when the direct
voting of reference users is turned off for content items and only the indirect
(to other users) influence is counted. \((p &lt; 0.05)\)</p>
<p>What does a 50% reduction in the difference mean? Let the score of a content
item \(C\) be measured in 3 systems: democratic \(D\), reference-users-only
\(R\) and hybrid \(H\) with direct influence of reference users to content items
being turned off. By sorting the items according to those scores we can
calculate their ranks in the 3 systems: \(r_D\), \(r_R\) and \(r<em>H\) respectively.
 The value of the rank is in the range \(1\) to \(n\), where \(n\)
is total number of content items. The absolute difference between the democratic
ranking and the reference ranking \(d</em>{DR}\) is:</p>
<p>$$ d_{DR} = abs(r_D - r_R) $$</p>
<p>while the absolute difference between the hybrid ranking and the reference
ranking \(d_{HR}\) is:</p>
<p>$$ d_{HR} = abs(r_H - r_R) $$</p>
<p>and it turns out that on average,</p>
<p>$$ d<em>{HR} = 0.5 d</em>{DR} $$</p>
<p>The important downside of these results is that the people using the system were
not aware that points are calculated in a different way. The original votes were
given by people who knew that the system is democratic and acted accordingly. It
remains to be seen what the results would be if people are aware that their
voting influence depends on the way others vote for their submitted content.</p>
<p>I pondered starting a website similar to hacker news based on this voting and
scoring scheme, however starting a whole new news website is about much more
than just scoring algorithms (it requires reputation in the online comminty,
popularity and most importantly time, none of which I presently have in
sufficient amounts or know how to achieve). But hopefully, pg and the rest of
the hacker news team might find this scheme useful enough to somehow incorporate
it into the existing scoring system.</p>
]]></description>
            <link>https://spion.github.io/posts/fixing-hackernews-mathematical-approach.html</link>
            <guid isPermaLink="true">https://spion.github.io/posts/fixing-hackernews-mathematical-approach.html</guid>
            <pubDate>Tue, 21 Aug 2012 00:00:00 GMT</pubDate>
        </item>
    </channel>
</rss>