<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0, target-densityDpi=medium-dpi">
    <meta content="True" name="HandheldFriendly">
    <title>A hint of chaos</title>
    <link rel="shortcut icon" href="/ico/favicon.ico">
    <link rel="stylesheet" href="/styles/style.css">
    <link rel="stylesheet" href="http://google-code-prettify.googlecode.com/svn/trunk/src/prettify.css">
  </head>
  <body>
    <script src="//ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
    <div class="main">
      <div class="info-tiny"><a href="/"> <img src="https://secure.gravatar.com/avatar/d924ad2ac22af6216aadd2e9184616de?s=420"></a>
        <h1 class="first"><a href="/">Gorgi Kosev</a></h1>
        <p class="subtitle">code, music, math</p>
      </div>
      <div class="info"><a href="/"> <img src="https://secure.gravatar.com/avatar/d924ad2ac22af6216aadd2e9184616de?s=420"></a>
        <h1 class="first"> <a href="/">Gorgi Kosev</a></h1>
        <p class="subtitle">code, music, math</p>
        <p class="subtitle margin"><a href="http://twitter.com/spion">@spion</a></p>
      </div>
      <div class="rest">
<div class="post">
  <h1><a href="/posts/closures-are-unavoidable-in-node.html">Closures are unavoidable in node</a></h1>
  <div class="date">Fri Aug 23 2013</div>
  <div class="content"><p>A couple of weeks ago I wrote a <a href="/posts/analysis-generators-and-other-async-patterns-node.html">giant comparison of node.js async code 
patterns</a> that 
mostly focuses on the new generators feature in EcmaScript 6 (Harmony)</p>
<p>Among other implementations there were two callback versions: <a href="//github.com/spion/async-compare/blob/master/examples/original.js">original.js</a>, 
which contains nested callbacks, and <a href="//github.com/spion/async-compare/blob/master/examples/flattened.js">flattened.js</a>, which flattens the nesting a
little bit. Both make extensive use of JavaScript closures: every time
the benchmarked function is invoked, a lot of closures are created.</p>
<p>Then <a href="http://blog.trevnorris.com/2013/08/long-live-callbacks.html">Trevor Norris wrote</a>
that we should be avoiding closures when writing performance-sensitive code,
hinting that my benchmark may be an example of &quot;doing it wrong&quot;</p>
<p>I decided to try and write two more flattened variants. The idea is to 
minimize performance loss and memory usage by avoiding the creation of closures.</p>
<p>You can see the code here: 
<strong><a href="//github.com/spion/async-compare/blob/master/examples/flattened-class.js">flattened-class.js</a> and <a href="//github.com/spion/async-compare/blob/master/examples/flattened-noclosure.js">flattened-noclosure.js</a></strong></p>
<p>Of course, this made complexity skyrocket. Lets see what it did for performance.</p>
<p>Results are for 50 000 parallel invocations of the upload function, with 
simulated I/O operations that always take 1ms. Note: suspend is currently the 
fastest generator based library</p>
<table>
<thead>
<tr>
<th align="left">file</th>
<th align="right">time(ms)</th>
<th align="right">memory(MB)</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left"><a href="//github.com/spion/async-compare/blob/master/examples/flattened-class.js">flattened-class.js</a></td>
<td align="right">1398</td>
<td align="right">106.58</td>
</tr>
<tr>
<td align="left"><a href="//github.com/spion/async-compare/blob/master/examples/flattened.js">flattened.js</a></td>
<td align="right">1453</td>
<td align="right">110.19</td>
</tr>
<tr>
<td align="left"><a href="//github.com/spion/async-compare/blob/master/examples/flattened-noclosure.js">flattened-noclosure.js</a></td>
<td align="right">1574</td>
<td align="right">102.28</td>
</tr>
<tr>
<td align="left"><a href="//github.com/spion/async-compare/blob/master/examples/original.js">original.js</a></td>
<td align="right">1749</td>
<td align="right">124.96</td>
</tr>
<tr>
<td align="left"><a href="//github.com/spion/async-compare/blob/master/examples/suspend.js">suspend.js</a></td>
<td align="right">2701</td>
<td align="right">144.66</td>
</tr>
</tbody>
</table>
<p>No performance gains. Why?</p>
<p>Because this kind of code requires that results from previous callbacks are 
passed to the next callback. </p>
<p>Unfortunately, in node this means creating closures. There is no other option. </p>
<p>Node core functions only take callback functions. If we want to pass context 
with that callback function, we <em>have</em> to create a closure. And yeah, <code>bind</code>
is also a closure:</p>
<pre><code>function bind(fn, ctx) {
    return function bound() {
        return fn.apply(ctx, arguments);
    }
}</code></pre>
<p>Notice how <code>bound</code> is a closure over ctx and fn. You can&#39;t avoid it if you
want to pass context together with the function.</p>
<p>Now, if the low-level core API was also able to take a context argument, things 
could have been different. For example, instead of writing:</p>
<pre><code>fs.readFile(f, bind(this.afterFileRead, this));</code></pre>
<p>if we were able to write:</p>
<pre><code>fs.readFile(f, this.afterFileRead, this);</code></pre>
<p>then we would be able to write code that avoids closures and 
<a href="//github.com/spion/async-compare/blob/master/examples/flattened-class.js">flattened-class.js</a> could have been much faster. </p>
<p>But we can&#39;t do that.</p>
<p>What if we could though? </p>
<p>Since my fake I/O functions use <code>setTimeout</code>, I decided to take 
<a href="https://github.com/joyent/node/blob/master/lib/timers.js">timers.js</a> from node 
core and add context passing support to the <code>Timeout</code> class. The result was 
<a href="//github.com/spion/async-compare/blob/master/lib/timers-ctx.js">timers-ctx.js</a> 
which in turn resulted with <a href="//github.com/spion/async-compare/blob/master/examples/flattened-class-ctx.js">flattened-class-ctx.js</a></p>
<p>And here is how it performs:</p>
<table>
<thead>
<tr>
<th align="left">file</th>
<th align="right">time(ms)</th>
<th align="right">memory(MB)</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left"><a href="//github.com/spion/async-compare/blob/master/examples/flattened-class-ctx.js">flattened-class-ctx.js</a></td>
<td align="right">736</td>
<td align="right">58.11</td>
</tr>
<tr>
<td align="left"><a href="//github.com/spion/async-compare/blob/master/examples/flattened-class.js">flattened-class.js</a></td>
<td align="right">1062</td>
<td align="right">84.72</td>
</tr>
<tr>
<td align="left"><a href="//github.com/spion/async-compare/blob/master/examples/flattened.js">flattened.js</a></td>
<td align="right">1148</td>
<td align="right">88.00</td>
</tr>
<tr>
<td align="left"><a href="//github.com/spion/async-compare/blob/master/examples/original.js">original.js</a></td>
<td align="right">1426</td>
<td align="right">104.66</td>
</tr>
<tr>
<td align="left"><a href="//github.com/spion/async-compare/blob/master/examples/suspend.js">suspend.js</a></td>
<td align="right">2157</td>
<td align="right">139.97</td>
</tr>
</tbody>
</table>
<p>Yeah. That shaved off a couple of 100s of miliseconds more. </p>
<p>Is it worth it?</p>
<table>
<thead>
<tr>
<th align="left">name</th>
<th align="right">tokens</th>
<th align="right">complexity</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left"><a href="//github.com/spion/async-compare/blob/master/examples/suspend.js">suspend.js</a></td>
<td align="right">331</td>
<td align="right">1.10</td>
</tr>
<tr>
<td align="left"><a href="//github.com/spion/async-compare/blob/master/examples/original.js">original.js</a></td>
<td align="right">425</td>
<td align="right">1.41</td>
</tr>
<tr>
<td align="left"><a href="//github.com/spion/async-compare/blob/master/examples/flattened.js">flattened.js</a></td>
<td align="right">477</td>
<td align="right">1.58</td>
</tr>
<tr>
<td align="left"><a href="//github.com/spion/async-compare/blob/master/examples/flattened-class-ctx.js">flattened-class-ctx.js</a></td>
<td align="right">674</td>
<td align="right">2.23</td>
</tr>
</tbody>
</table>
<p>Maybe, maybe not. You decide.</p>
</div>
  <div class="comments"> 
    <hr/><a href="/posts/closures-are-unavoidable-in-node.html#_comments">comment or share</a>
  </div>
</div>
<div class="post">
  <h1><a href="/posts/analysis-generators-and-other-async-patterns-node.html">Analysis of generators and other async patterns in node</a></h1>
  <div class="date">Fri Aug 09 2013</div>
  <div class="content"><p>Table of contents:</p>
<ul>
<li><a href="#a-gentle-introduction-to-generators">A gentle introduction to generators</a></li>
<li><a href="#the-analysis">The analysis</a></li>
<li><a href="#the-examples">The examples</a></li>
<li><a href="#complexity">Complexity</a></li>
<li><a href="#performance-time-and-memory">Performance (time and memory)</a></li>
<li><a href="#debuggability">Debuggability</a><ul>
<li><a href="#source-maps-support">Source maps support</a></li>
<li><a href="#stack-trace-accuracy">Stack trace accuracy</a></li>
</ul>
</li>
<li><a href="#conclusion">Conclusion</a></li>
</ul>
<p>Async coding patterns are the subject of never-ending debates for us node.js 
developers. Everyone has their own favorite method or pet library as well as
strong feelings and opinions on all the other methods and libraries. Debates
can be heated: sometimes social pariahs may be declared or grave rolling 
may be induced.</p>
<p>The reason for this is that JavaScript in node never had any continuation 
mechanism to allow code to pause and resume across the event loop boundary. </p>
<p>Until now.</p>
<p><a name="a-gentle-introduction-to-generators"></a></p>
<h3>A gentle introduction to generators</h3>
<p><small>If you know how generators work, you can <a href="#skip">skip this</a>
    and continue to the analysis</small></p>
<p>Generators are a new feature of ES6. Normally they would be used for iteration.
Here is a generator that generates Fibonacci numbers. The example is taken from
the <a href="http://wiki.ecmascript.org/doku.php?id=harmony:generators">ECMAScript harmony wiki</a>:</p>
<pre><code class="lang-js">function* fibonacci() {
    let [prev, curr] = [0, 1];
    for (;;) {
        [prev, curr] = [curr, prev + curr];
        yield curr;
    }
}</code></pre>
<p>And here is how we iterate through this generator:</p>
<pre><code class="lang-js">for (n of fibonacci()) {
    // truncate the sequence at 1000
    if (n &gt; 1000) break;
    console.log(n);
}</code></pre>
<p>What happens behind the scene? </p>
<p>Generator functions are actually constructors of iterators. The returned 
iterator object has a <code>next()</code> method. We can invoke that method manually:</p>
<pre><code class="lang-js">var seq = fibonacci();
console.log(seq.next()); // 1
console.log(seq.next()); // 2 etc.</code></pre>
<p>When <code>next</code> is invoked, it starts the execution of the generator. The generator 
runs until it encounters a <code>yield</code> expression. Then it pauses and the execution
goes back to the code that called <code>next</code></p>
<p>So in a way, <code>yield</code> works similarly to <code>return</code>. But there is a big difference. 
If we call <code>next</code> on the generator again, the generator will resume from the 
point where it left off - from the last <code>yield</code> line. </p>
<p>In our example, the generator will resume to the top of the endless  <code>for</code> loop 
and calculate the next Fibonacci pair.</p>
<p>So how would we use this to write async code? </p>
<p>A great thing about the <code>next()</code> method is that it can also send values to the 
generator. Let&#39;s write a simple number generator that also collects the stuff 
it receives. When it gets two things it prints them using <code>console.log</code>:</p>
<pre><code class="lang-js">function* numbers() {
    var stuffIgot = [];
    for (var k = 0; k &lt; 2; ++k) {        
        var itemReceived = yield k;
        stuffIgot.push(itemReceived);
    }
    console.log(stuffIgot);
}</code></pre>
<p>This generator gives us 3 numbers using yield. Can we give something back?</p>
<p>Let&#39;s give two things to this generator:</p>
<pre><code class="lang-js">var iterator = numbers();
// Cant give anything the first time: need to get to a yield first.
console.log(iterator.next()); // logs 0
console.log(iterator.next(&#39;present&#39;)); // logs 1
fs.readFile(&#39;file.txt&#39;, function(err, resultFromAnAsyncTask) {
    console.log(iterator.next(resultFromAnAsyncTask)); // logs 2
});</code></pre>
<p>The generator will log the string <code>&#39;present&#39;</code> and the contents of <code>file.txt</code></p>
<p>Uh-oh.</p>
<p>Seems that we can keep the generator paused across the event loop boundary. </p>
<p>What if instead of numbers, we yielded some files to be read?</p>
<pre><code class="lang-js">function* files() {
    var results = [];
    for (var k = 0; k &lt; files.length; ++k) 
        results.push(yield files[k]);
    return results;
}</code></pre>
<p>We could process those file reading tasks asynchronously. </p>
<pre><code class="lang-js">var iterator = files();
function process(iterator, sendValue) {
    var fileTask = iterator.next(sendValue);
    fs.readFile(fileTask, function(err, res) {
        if (err) iterator.throw(err);
        else process(iterator, res);
    });
}
process(iterator);</code></pre>
<p>But from the generator&#39;s point of view, everything seems to be happening 
synchronously: it gives us the file using <code>yield</code>, then it waits to be resumed, 
then it receives the contents of the file and makes a push to the results 
array.</p>
<p>And there is also <code>generator.throw()</code>. It causes an exception to be thrown
from inside the generator. How cool is that?</p>
<p>With <code>next</code> and <code>throw</code> combined together, we can easily run async code. Here 
is an example from one of the earliest ES6 async generators library 
<a href="http://taskjs.org/">task.js</a>. </p>
<pre><code class="lang-js">spawn(function* () {
    var data = yield $.ajax(url);
    $(&#39;#result&#39;).html(data);
    var status = $(&#39;#status&#39;).html(&#39;Download complete.&#39;);
    yield status.fadeIn().promise();
    yield sleep(2000);
    status.fadeOut();
});</code></pre>
<p>This generator yields promises, which causes it to suspend execution. The <code>spawn</code> 
function that runs the generator takes those promises and waits until they&#39;re 
fulfilled. Then it resumes the generator by sending it the resulting value.</p>
<p>When used in this form, generators look a lot like classical threads. You spawn 
a thread, it issues blocking I/O calls using <code>yield</code>, then the code resumes 
execution from the point it left off.</p>
<p>There is one very important difference though. While threads can be suspended
involuntarily at any point by the operating systems, generators have to 
willingly suspend themselves using <code>yield</code>. This means that there is no danger 
of variables changing under our feet, except after a <code>yield</code>.</p>
<p>Generators go a step further with this: it&#39;s impossible to suspend execution
without using the <code>yield</code> keyword. In fact, if you want to call another 
generator you will have to write <code>yield* anotherGenerator(args)</code>. This means 
that suspend points are always visible in the code, just like they are when 
using callbacks.</p>
<p>Amazing stuff! So what does this mean? What is the reduction of code complexity? 
What are the performance characteristics of code using generators? Is debugging 
easy? What about environments that don&#39;t have ES6 support?</p>
<p>I decided to do a big comparison of all existing node async code patterns and
find the answers to these questions. </p>
<p><a name="skip"></a><a name="the-analysis"></a></p>
<h3>The analysis</h3>
<p>For the analysis, I took <code>file.upload</code>, a typical CRUD method extracted from<br><a href="http://doxbee.com">DoxBee</a> called when uploading files. It executes multiple 
queries to the database: a couple of selects, some inserts and one update. 
Lots of mixed sync / async action.</p>
<p>It looks like this: </p>
<pre><code>function upload(stream, idOrPath, tag, done) {
    var blob = blobManager.create(account);
    var tx = db.begin();
    function backoff(err) {
        tx.rollback();
        return done(new Error(err));
    }
    blob.put(stream, function (err, blobId) {
        if (err) return done(err);
        self.byUuidOrPath(idOrPath).get(function (err, file) {
            if (err) return done(err);
            var previousId = file ? file.version : null;
            var version = {
                userAccountId: userAccount.id,
                date: new Date(),
                blobId: blobId,
                creatorId: userAccount.id,
                previousId: previousId
            };
            version.id = Version.createHash(version);
            Version.insert(version).execWithin(tx, function (err) {
                if (err) return backoff(err);
                if (!file) {
                    var splitPath = idOrPath.split(&#39;/&#39;);
                    var fileName = splitPath[splitPath.length - 1];
                    var newId = uuid.v1();
                    self.createQuery(idOrPath, {
                        id: newId,
                        userAccountId: userAccount.id,
                        name: fileName,
                        version: version.id
                    }, function (err, q) {
                        if (err) return backoff(err);
                        q.execWithin(tx, function (err) {
                            afterFileExists(err, newId);
                        });

                    })
                }
                else return afterFileExists(null, file.id);
            });
            function afterFileExists(err, fileId) {
                if (err) return backoff(err);
                FileVersion.insert({fileId: fileId,versionId: version.id})
                    .execWithin(tx, function (err) {
                        if (err) return backoff(err);
                        File.whereUpdate({id: fileId}, {
                            version: version.id
                        }).execWithin(tx, function (err) {
                            if (err) return backoff(err);
                            tx.commit(done);
                        });
                })
            }
        });
    });
}</code></pre>
<p>Slightly pyramidal code full of callbacks.</p>
<p>This is how it looks like when written with generators:</p>
<pre><code>var genny = require(&#39;genny&#39;);
module.exports = genny.fn(function* upload(resume, stream, idOrPath, tag) {
    var blob = blobManager.create(account);
    var tx = db.begin();
    try {
        var blobId = yield blob.put(stream, resume()); 
        var file = yield self.byUuidOrPath(idOrPath).get(resume()); 
        var previousId = file ? file.version : null;
        var version = {
            userAccountId: userAccount.id,
            blobId: blobId,
            creatorId: userAccount.id,
            previousId: previousId
        };
        version.id = Version.createHash(version);
        yield Version.insert(version).execWithin(tx, resume());
        if (!file) {
            var splitPath = idOrPath.split(&#39;/&#39;);
            var fileName = splitPath[splitPath.length - 1];
            var newId = uuid.v1();
            var file = {
                id: newId,
                userAccountId: userAccount.id,
                name: fileName,
                version: version.id
            }
            var q = yield self.createQuery(idOrPath, file, resume());
            yield q.execWithin(tx, resume());
        }
        yield FileVersion.insert({fileId: file.id, versionId: version.id})
            .execWithin(tx, resume());
        yield File.whereUpdate({id: file.id}, {version: version.id})
            .execWithin(tx, resume()); 
        yield tx.commit(resume());
    } catch (e) {
        tx.rollback();
        throw e; 
    }
});</code></pre>
<p>Shorter, very straight-forward code and absolutely no nesting of callback
functions. Awesome.</p>
<p>Yet subjective adjectives are not very convincing. I want to have a measure of 
complexity, a number that tells me what I&#39;m actually saving. </p>
<p>I also want to know what the performance characteristics are - how much time
and memory would it take to execute a thousand of parallel invocations of this 
method? What about 2000 or 3000?</p>
<p>Also, what happens if an exception is thrown? Do I get a complete stack trace 
like in the original version?</p>
<p>I also wanted to compare the results with other alternatives, such as fibers,
streamlinejs and promises (without generators).</p>
<p>So I wrote a lot of different versions of this method, and I will share my 
personal impressions before giving you the results of the analysis</p>
<p><a name="the-examples"></a></p>
<h3>The examples</h3>
<p><strong><a href="//github.com/spion/async-compare/blob/master/examples/original.js">original.js</a></strong></p>
<p>The original solution, presented above. Vanilla callbacks. Slightly pyramidal. 
I consider it acceptable, if a bit mediocre.</p>
<p><strong><a href="//github.com/spion/async-compare/blob/master/examples/flattened.js">flattened.js</a></strong></p>
<p>Flattened variant of the original via named functions. Taking the advice from
<a href="http://callbackhell.com/">callback hell</a>, I flattened the pyramid a little 
bit. As I did that, I found that while the pyramid shrunk, the code actually 
grew.</p>
<p><strong><a href="//github.com/spion/async-compare/blob/master/examples/catcher.js">catcher.js</a></strong></p>
<p>I noticed that the first two vanilla solutions had a lot of common error 
handling code everywhere. So I wrote a tiny library called catcher.js which 
works very much like node&#39;s <code>domain.intercept</code>. This reduced the complexity
and the number of lines further, but the pyramidal looks remained.</p>
<p><strong><a href="//github.com/spion/async-compare/blob/master/examples/flattened-class.js">flattened-class.js</a>
and <a href="//github.com/spion/async-compare/blob/master/examples/flattened-noclosure.js">flattened-noclosure.js</a></strong></p>
<p>Driven by 
<a href="http://blog.trevnorris.com/2013/08/long-live-callbacks.html">a post that Trevor Norris wrote</a>,
I tried to write two more flattened variants. The idea is to minimize 
performance loss and memory usage by avoiding the creation of closures.</p>
<p>If you want to read more comments about this implementation, see
<a href="/posts/closures-are-unavoidable-in-node.html">this post</a></p>
<p>Of course, this made complexity skyrocket. </p>
<p>However, there were no significant performance gains. Why?</p>
<p>Because this kind of code requires that results from previous callbacks to be 
somehow passed to the next callbacks. </p>
<p>Unfortunately, in node this means creating closures. There is no other option. 
If the low-level api was also able to take context, e.g. instead of writing</p>
<pre><code>fs.readFile(f, this.afterFileRead.bind(this));</code></pre>
<p>if we were able to simply write </p>
<pre><code>fs.readFile(f, this.afterFileRead, this);</code></pre>
<p>then <code>flattened-class.js</code> could have been much faster. But node functions only
take callback functions, and if we want to pass context with that callback 
function we <em>have</em> to create closures. Even the implementation of bind is a 
closure, e.g.</p>
<pre><code>function bind(fn, ctx) {
    return function bound() {
        return fn.apply(ctx, arguments);
    }
}</code></pre>
<p>Notice the closure?</p>
<p><strong><a href="//github.com/spion/async-compare/blob/master/examples/promises.js">promises.js</a></strong></p>
<p>I&#39;ll be honest. I&#39;ve never written promise code in node before. Driven by 
<a href="//jeditoolkit.com/2012/04/26/code-logic-not-mechanics.html#post">Gozalla&#39;s excellent post</a>
I concluded that everything should be a promise, and things that can&#39;t handle 
promises should also be rewritten. </p>
<p>Take for example this particular line in the original:</p>
<pre><code class="lang-js">var previousId = file ? file.version : null;</code></pre>
<p>If file is a promise, we can&#39;t use the ternary operator or the property 
getter. Instead we need to write two helpers: a ternary operator helper and a 
property getter helper:</p>
<pre><code class="lang-js">var previousIdP = p.ternary(fileP, p.get(fileP, &#39;version&#39;), null);</code></pre>
<p>Unfortunately this gets out of hand quickly:</p>
<pre><code>var versionP = p.allObject({
    userAccountId: userAccount.id,
    blobId: blobIdP,
    creatorId: userAccount.id,
    previousId: previousIdP,
    ...
});
versionP = p.set(versionP, p.allObject({
    id: fn.call(Version.createHash, versionP)
}));
// Even if Version.insert has been lifted to take promise arguments, it returns 
// a promise and therefore we cannot call execWithinP. We have to wait for the
// promise  to resolve to invoke the function.
var versionInsert = p.eventuallyCall(
    Version.insert(versionP), &#39;execWithinP&#39;, tx);
var versionIdP = p.get(versionP, &#39;id&#39;);</code></pre>
<p>So I decided to write a less aggressive version, <code>promiseish.js</code></p>
<p>note: I used <a href="//github.com/cujojs/when">when</a> because i liked its function 
lifting API better than Q&#39;s</p>
<p><strong><a href="//github.com/spion/async-compare/blob/master/examples/promises.js">promiseish.js</a> 
and <a href="//github.com/spion/async-compare/blob/master/examples/promises.js">promiseishQ.js</a></strong></p>
<p>Nothing fancy here, just some <code>.then()</code> chaining. In fact it feels less complex
than the <code>promise.js</code> version, where I felt like I was trying to fight the
language all the time.</p>
<p>The second file <code>promiseishQ.js</code> uses <a href="//github.com/kriskowal/q">Q</a> instead of 
<a href="//github.com/cujojs/when">when</a>.</p>
<p><strong><a href="//github.com/spion/async-compare/blob/master/examples/co.js">co.js</a> 
and <a href="//github.com/spion/async-compare/blob/master/examples/gens.js">gens.js</a></strong></p>
<p><a href="//github.com/Raynos/gens">Gens</a> and <a href="//github.com/visionmedia/co">co</a> are 
generator-based libraries. Both can work by yielding thunk-style functions: 
that is, functions that take a single argument which is a node style callback 
in the format <code>function (err, result)</code></p>
<p>The problem is, thunks still require wrapping. The recommended way to wrap node 
style functions is to use <code>co.wrap</code> for co and <code>fn.bind</code> for gens - so thats 
what I did.</p>
<p><strong><a href="//github.com/spion/async-compare/blob/master/examples/suspend.js">suspend.js</a> 
and <a href="//github.com/spion/async-compare/blob/master/examples/promises.js">genny.js</a></strong></p>
<p><a href="https://github.com/jmar777/suspend">suspend</a> and 
<a href="http://github.com/spion/genny">genny</a> are generator-based solutions that can 
work directly with node-style functions.</p>
<p>I&#39;m biased here since I wrote genny. I still think that this is objectively the 
best way to use generators in node. Just replace the callback with a placeholder 
function <code>resume</code>, then yield that. Comes back to you with the value. </p>
<p>Kudos to <a href="//github.com/jmar777">jmar777</a> for realizing that you don&#39;t need
to actually yield anything and can resume the generator using the placeholder 
callback instead.</p>
<p>Both suspend and genny use generators roughly the same way. The resulting code 
is very clean, very straightforward and completely devoid of callbacks. </p>
<p><strong><a href="//github.com/spion/async-compare/blob/master/examples/fibrous.js">fibrous.js</a></strong></p>
<p><a href="//github.com/goodeggs/fibrous">Fibrous</a> is a fibers library that creates 
&quot;sync&quot; methods out of your async ones, which you can then run in a fiber. </p>
<p>So if for example you had:</p>
<pre><code>fs.readFile(file, function(err, data){ ... });</code></pre>
<p>Fibrous would generate a version that returns a future, suspends the running
fiber and resumes execution when the value becomes available.</p>
<pre><code>var data = fs.sync.readFile(file);</code></pre>
<p>I also needed to wrap the entire upload function: </p>
<pre><code>fibrous(function upload() { ... })</code></pre>
<p>This felt exactly like the generators version but with <code>sync</code> instead of 
<code>yield</code> to indicate the methods that will yield. The one benefit I can think 
of is that it feels more natural for chaining - less parenthesis are needed. </p>
<pre><code>somefn.sync(arg).split(&#39;/&#39;)
// vs
(yield somefn(arg, resume)).split(&#39;/&#39;)</code></pre>
<p>Major drawback: this will never be available outside of node.js or without 
native modules.</p>
<p>Library: <a href="//github.com/goodeggs/fibrous">fibrous</a></p>
<p><strong><a href="//github.com/spion/async-compare/blob/master/examples/qasync.js">qasync.js</a></strong></p>
<p>Generators with promises. Didn&#39;t feel very different than genny or suspend.
Its slightly less complicated: you can yield the promise instead of placing
the provided resume function at every point where a callback is needed. </p>
<p>Caveat: as always with promises you will need to wrap all callback-based 
functions.</p>
<p>Library: <a href="//github.com/kriskowal/q">Q</a></p>
<p><strong><a href="//github.com/spion/async-compare/blob/master/examples/src-streamline._js">streamline.js</a></strong></p>
<p>Uses <a href="http://github.com/Sage/streamlinejs">streamlinejs</a> CPS transformer and
works very much like suspend and genny, except without needing to write yield 
all the time.</p>
<p>Caveat: you will need to compile the file in order to use it. Also, even 
though it looks like valid JavaScript, it isn&#39;t JavaScript. Superficially, it 
has the same syntax, but it has very different semantics, particularly when
it comes to the <code>_</code> keyword, which acts like <code>yield</code> and <code>resume</code> combined in 
one.</p>
<p>The code however is really simple and straightforward.</p>
<p><a name="complexity"></a></p>
<h3>Complexity</h3>
<p>To measure complexity I took the number of tokens in the source code found by
Esprima&#39;s lexer (comments excluded). The idea is taken from
<a href="http://www.paulgraham.com/power.html">Paul Graham&#39;s essay <em>Succinctness is Power</em></a></p>
<p>Results:</p>
<table>
<thead>
<tr>
<th align="left">name</th>
<th align="right">tokens</th>
<th align="right">complexity</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">src-streamline._js</td>
<td align="right">302</td>
<td align="right">1.00</td>
</tr>
<tr>
<td align="left">co.js</td>
<td align="right">304</td>
<td align="right">1.01</td>
</tr>
<tr>
<td align="left">fibrous.js</td>
<td align="right">317</td>
<td align="right">1.05</td>
</tr>
<tr>
<td align="left">qasync.js</td>
<td align="right">320</td>
<td align="right">1.06</td>
</tr>
<tr>
<td align="left">suspend.js</td>
<td align="right">331</td>
<td align="right">1.10</td>
</tr>
<tr>
<td align="left">genny.js</td>
<td align="right">339</td>
<td align="right">1.12</td>
</tr>
<tr>
<td align="left">gens.js</td>
<td align="right">344</td>
<td align="right">1.14</td>
</tr>
<tr>
<td align="left">catcher.js</td>
<td align="right">392</td>
<td align="right">1.30</td>
</tr>
<tr>
<td align="left">promiseishQ.js</td>
<td align="right">402</td>
<td align="right">1.33</td>
</tr>
<tr>
<td align="left">promiseish.js</td>
<td align="right">415</td>
<td align="right">1.37</td>
</tr>
<tr>
<td align="left">original.js</td>
<td align="right">425</td>
<td align="right">1.41</td>
</tr>
<tr>
<td align="left">promises.js</td>
<td align="right">461</td>
<td align="right">1.53</td>
</tr>
<tr>
<td align="left">flattened.js</td>
<td align="right">477</td>
<td align="right">1.58</td>
</tr>
<tr>
<td align="left">flattened-noclosure.js</td>
<td align="right">599</td>
<td align="right">1.98</td>
</tr>
<tr>
<td align="left">flattened-class.js</td>
<td align="right">717</td>
<td align="right">2.37</td>
</tr>
<tr>
<td align="left">rx.js</td>
<td align="right">935</td>
<td align="right">3.10</td>
</tr>
</tbody>
</table>
<p>Streamline and co have the lowest complexity. Fibrous, qasync, suspend, genny 
and gens are roughly comparable. </p>
<p>Catcher is comparable with both promiseish solutions. The complexity when using 
promises is roughly comparable to the original version with callbacks, but 
there is some improvement.</p>
<p>Finally, it seems that going promises-all-the-way or flattening the callback 
pyramid are not worth it. Both only increase the complexity.</p>
<p>The rx.js sample is still a work in progress - it can be made much better. </p>
<p><a name="performance-time-and-memory"></a></p>
<h3>Performance (time and memory)</h3>
<p>To be able to execute the files, all external methods are mocked with 
setTimeout to simulate waiting for I/O. </p>
<p>There are two variables that control the test:</p>
<ul>
<li>\(n\) - the number of parallel &quot;upload requests&quot;</li>
<li>\(t\) - average wait time per async I/O operation</li>
</ul>
<p>For the first test, I set the time for every async operation \(t = 1ms\) then 
ran every solution for \(n \in \lbrace 100,500,1000,1500,2000 \rbrace \).</p>
<p>note: hover over the legend to highlight the item on the chart.</p>
<p><div id="perf-time-1" class="plot">
</div></p>
<script type="text/javascript">

window.perfCPUBound = 
[ { label: 'catcher.js',
    data: 
     [ [ 100, 17 ],
       [ 500, 29 ],
       [ 1000, 44 ],
       [ 1500, 58 ],
       [ 2000, 72 ] ] },
  { label: 'co.js',
    data: 
     [ [ 100, 24 ],
       [ 500, 62 ],
       [ 1000, 96 ],
       [ 1500, 157 ],
       [ 2000, 233 ] ] },
  { label: 'dst-co-traceur.js',
    data: 
     [ [ 100, 28 ],
       [ 500, 86 ],
       [ 1000, 200 ],
       [ 1500, 271 ],
       [ 2000, 324 ] ] },
  { label: 'dst-genny-traceur.js',
    data: 
     [ [ 100, 25 ],
       [ 500, 76 ],
       [ 1000, 157 ],
       [ 1500, 244 ],
       [ 2000, 286 ] ] },
  { label: 'dst-qasync-traceur.js',
    data: 
     [ [ 100, 112 ],
       [ 500, 510 ],
       [ 1000, 1108 ],
       [ 1500, 1713 ],
       [ 2000, 2316 ] ] },
  { label: 'dst-streamline.js',
    data: 
     [ [ 100, 19 ],
       [ 500, 37 ],
       [ 1000, 54 ],
       [ 1500, 74 ],
       [ 2000, 97 ] ] },
  { label: 'dst-suspend-traceur.js',
    data: 
     [ [ 100, 21 ],
       [ 500, 56 ],
       [ 1000, 117 ],
       [ 1500, 217 ],
       [ 2000, 263 ] ] },
  { label: 'flattened-class.js',
    data: 
     [ [ 100, 16 ],
       [ 500, 29 ],
       [ 1000, 40 ],
       [ 1500, 47 ],
       [ 2000, 58 ] ] },
  { label: 'flattened.js',
    data: 
     [ [ 100, 15 ],
       [ 500, 31 ],
       [ 1000, 44 ],
       [ 1500, 55 ],
       [ 2000, 64 ] ] },
  { label: 'flattened-noclosure.js',
    data: 
     [ [ 100, 16 ],
       [ 500, 28 ],
       [ 1000, 42 ],
       [ 1500, 53 ],
       [ 2000, 64 ] ] },
  { label: 'genny.js',
    data: 
     [ [ 100, 19 ],
       [ 500, 48 ],
       [ 1000, 79 ],
       [ 1500, 147 ],
       [ 2000, 150 ] ] },
  { label: 'gens.js',
    data: 
     [ [ 100, 19 ],
       [ 500, 39 ],
       [ 1000, 63 ],
       [ 1500, 94 ],
       [ 2000, 119 ] ] },
  { label: 'original.js',
    data: 
     [ [ 100, 15 ],
       [ 500, 28 ],
       [ 1000, 43 ],
       [ 1500, 56 ],
       [ 2000, 65 ] ] },
  { label: 'promiseish.js',
    data: 
     [ [ 100, 97 ],
       [ 500, 463 ],
       [ 1000, 809 ],
       [ 1500, 1155 ],
       [ 2000, 1652 ] ] },
  { label: 'promiseishQ.js',
    data: 
     [ [ 100, 93 ],
       [ 500, 540 ],
       [ 1000, 1145 ],
       [ 1500, 1789 ],
       [ 2000, 2287 ] ] },
  { label: 'promises.js',
    data: 
     [ [ 100, 222 ],
       [ 500, 1241 ],
       [ 1000, 2284 ],
       [ 1500, 3836 ],
       [ 2000, 5861 ] ] },
  { label: 'qasync.js',
    data: 
     [ [ 100, 79 ],
       [ 500, 489 ],
       [ 1000, 962 ],
       [ 1500, 1587 ],
       [ 2000, 2104 ] ] },
  { label: 'rx.js',
    data: 
     [ [ 100, 37 ],
       [ 500, 147 ],
       [ 1000, 232 ],
       [ 1500, 329 ],
       [ 2000, 490 ] ] },
  { label: 'suspend.js',
    data: 
     [ [ 100, 16 ],
       [ 500, 28 ],
       [ 1000, 50 ],
       [ 1500, 66 ],
       [ 2000, 83 ] ] } ]

  .concat(

[ { label: 'dst-streamline-fibers.js',
    data: 
     [ [ 100, 25 ],
       [ 500, 147 ],
       [ 1000, 519 ],
       [ 1500, 1256 ],
       [ 2000, 2393 ] ] },
  { label: 'fibrous.js',
    data: 
     [ [ 100, 89 ],
       [ 500, 442 ],
       [ 1000, 1159 ],
       [ 1500, 2228 ],
       [ 2000, 3755 ] ] } ]

    );

window.addEventListener('load', function() {
    $.plot('#perf-time-1', perfCPUBound, {legend: { position: 'nw' }});
});
</script>

<p>Wow. Promises seem really, really slow. Fibers are also slow, with time 
complexity \( O(n^2) \). Everything else seems to be much faster.</p>
<p>Lets try removing all those promises and fibers to see whats down there.</p>
<p><div id="perf-time-2" class="plot">
</div></p>
<script type="text/javascript">


window.addEventListener('load', function() {
    $.plot('#perf-time-2', perfCPUBound.filter(function(item) {
        return !/(promise|qasync|fibrous|fiber)/.test(item.label)
    }), {legend: { position: 'nw' }})
});
</script>

<p>Ah, much better. </p>
<p>The original and flattened solutions are the fastest, as they use vanilla 
callbacks, with the fastest flattened solution being flattened-class.js</p>
<p>suspend and gens are the fastest generator based solutions. They incurred minimal 
overhead of about 60-100% running time. They&#39;re also roughly comparable with 
streamlinejs (when in raw callbacks mode).</p>
<p>Genny is about 2 times slower. This is because it adds some protection 
guarantees: it makes sure that callback-calling function behaves and calls the 
callback only once and provides a mechanism to enable better stack traces
when errors are encountered.</p>
<p>The slowest of the generator bunch is co, but not by much. There is nothing 
intrinsically slow about it though: the slowness is probably caused by <code>co.wrap</code> 
which creates a new arguments array on every invocation of the wrapped function.</p>
<p>All generator solutions become about 2 times slower when compiled with 
<a href="//github.com/google/traceur-compiler/">Google Traceur</a>, an ES6 to ES5 compiler
which we need to run generators code without the <code>--harmony</code> switch or in 
browsers. Is roughly 2-3 times slower, which is great.</p>
<p>Finally we have rx.js which is about 10 times slower than the fastest solution.</p>
<p>Great. But isn&#39;t this a bit unrealistic?</p>
<p>Most async operations take much longer than 1 millisecond to complete, 
especially when the load is as high as thousands of requests per second.
As a result, performance is I/O bound - why measure things as if it were 
CPU-bound?</p>
<p>So lets increase the average time needed for an async operation to </p>
<p>$$ t = n \cdot 0.1 ms $$</p>
<p>This will make I/O operation time grow together with \(n\). Each will
take 10 ms on average when there are 100 running in parallel and 100 ms when 
there are 1000 running in parallel. Makes much more sense.</p>
<p><div id="perf-time-3" class="plot">
</div></p>
<script type="text/javascript">
window.perfIOBound = 

[ { label: 'catcher.js',
    data: 
     [ [ 100, 78 ],
       [ 500, 351 ],
       [ 1000, 699 ],
       [ 1500, 1081 ],
       [ 2000, 1427 ] ] },
  { label: 'co.js',
    data: 
     [ [ 100, 98 ],
       [ 500, 424 ],
       [ 1000, 821 ],
       [ 1500, 1222 ],
       [ 2000, 1661 ] ] },
  { label: 'dst-co-traceur.js',
    data: 
     [ [ 100, 91 ],
       [ 500, 467 ],
       [ 1000, 922 ],
       [ 1500, 1347 ],
       [ 2000, 1744 ] ] },
  { label: 'dst-genny-traceur.js',
    data: 
     [ [ 100, 89 ],
       [ 500, 398 ],
       [ 1000, 778 ],
       [ 1500, 1205 ],
       [ 2000, 1533 ] ] },
  { label: 'dst-qasync-traceur.js',
    data: 
     [ [ 100, 113 ],
       [ 500, 495 ],
       [ 1000, 1125 ],
       [ 1500, 1757 ],
       [ 2000, 2301 ] ] },
  { label: 'dst-streamline.js',
    data: 
     [ [ 100, 79 ],
       [ 500, 350 ],
       [ 1000, 699 ],
       [ 1500, 1070 ],
       [ 2000, 1444 ] ] },
  { label: 'dst-suspend-traceur.js',
    data: 
     [ [ 100, 82 ],
       [ 500, 352 ],
       [ 1000, 687 ],
       [ 1500, 1038 ],
       [ 2000, 1374 ] ] },
  { label: 'flattened-class.js',
    data: 
     [ [ 100, 79 ],
       [ 500, 337 ],
       [ 1000, 687 ],
       [ 1500, 1049 ],
       [ 2000, 1424 ] ] },
  { label: 'flattened.js',
    data: 
     [ [ 100, 76 ],
       [ 500, 346 ],
       [ 1000, 699 ],
       [ 1500, 1059 ],
       [ 2000, 1440 ] ] },
  { label: 'flattened-noclosure.js',
    data: 
     [ [ 100, 80 ],
       [ 500, 340 ],
       [ 1000, 697 ],
       [ 1500, 1060 ],
       [ 2000, 1425 ] ] },
  { label: 'genny.js',
    data: 
     [ [ 100, 77 ],
       [ 500, 357 ],
       [ 1000, 723 ],
       [ 1500, 1130 ],
       [ 2000, 1521 ] ] },
  { label: 'gens.js',
    data: 
     [ [ 100, 84 ],
       [ 500, 353 ],
       [ 1000, 719 ],
       [ 1500, 1100 ],
       [ 2000, 1471 ] ] },
  { label: 'original.js',
    data: 
     [ [ 100, 74 ],
       [ 500, 341 ],
       [ 1000, 695 ],
       [ 1500, 1060 ],
       [ 2000, 1431 ] ] },
  { label: 'promiseish.js',
    data: 
     [ [ 100, 90 ],
       [ 500, 504 ],
       [ 1000, 908 ],
       [ 1500, 1234 ],
       [ 2000, 1730 ] ] },
  { label: 'promiseishQ.js',
    data: 
     [ [ 100, 94 ],
       [ 500, 565 ],
       [ 1000, 1166 ],
       [ 1500, 1778 ],
       [ 2000, 2519 ] ] },
  { label: 'promises.js',
    data: 
     [ [ 100, 229 ],
       [ 500, 1233 ],
       [ 1000, 2284 ],
       [ 1500, 3860 ],
       [ 2000, 5919 ] ] },
  { label: 'qasync.js',
    data: 
     [ [ 100, 88 ],
       [ 500, 487 ],
       [ 1000, 950 ],
       [ 1500, 1604 ],
       [ 2000, 2155 ] ] },
  { label: 'rx.js',
    data: 
     [ [ 100, 102 ],
       [ 500, 419 ],
       [ 1000, 760 ],
       [ 1500, 1117 ],
       [ 2000, 1526 ] ] },
  { label: 'suspend.js',
    data: 
     [ [ 100, 75 ],
       [ 500, 343 ],
       [ 1000, 677 ],
       [ 1500, 1032 ],
       [ 2000, 1422 ] ] } ]

.concat(
[ { label: 'dst-streamline-fibers.js',
    data: 
     [ [ 100, 94 ],
       [ 500, 446 ],
       [ 1000, 925 ],
       [ 1500, 1353 ],
       [ 2000, 2348 ] ] },
  { label: 'fibrous.js',
    data: 
     [ [ 100, 149 ],
       [ 500, 534 ],
       [ 1000, 1206 ],
       [ 1500, 2280 ],
       [ 2000, 4136 ] ] } ]

       );
window.addEventListener('load', function() {
    $.plot('#perf-time-3', perfIOBound, {legend: { position: 'nw' }})
});
</script>

<p><code>promises.js</code> and <code>fibrous.js</code> are still significantly slower. However all of
the other solutions are quite comparable now . Lets remove the worst two:</p>
<p><div id="perf-time-4" class="plot">
</div></p>
<script type="text/javascript">
window.addEventListener('load', function() {
    $.plot('#perf-time-4', perfIOBound.filter(function(item) {
        return !/(promises.js|fibrous.js)/.test(item.label)
    }), {legend: { position: 'nw' }});
});
</script>

<p>Everything is about the same now. Great! So in practice, you won&#39;t notice 
the CPU overhead in I/O bound cases - even if you&#39;re using promises. And with 
some of the generator libraries, the overhead simply disappears.</p>
<p>Excellent. But what about memory usage? Lets chart that too!</p>
<p>Note: the y axis represents peak memory usage (in MB).</p>
<p><div id="perf-mem-1" class="plot">
</div></p>
<script type="text/javascript">

window.perfMEM = 

[ { label: 'catcher.js',
    data: 
     [ [ 100, 0.91015625 ],
       [ 500, 3.34765625 ],
       [ 1000, 6.46484375 ],
       [ 1500, 9.1953125 ],
       [ 2000, 11.01171875 ] ] },
  { label: 'co.js',
    data: 
     [ [ 100, 1.703125 ],
       [ 500, 6.5390625 ],
       [ 1000, 11.16796875 ],
       [ 1500, 17.43359375 ],
       [ 2000, 24.515625 ] ] },
  { label: 'dst-co-traceur.js',
    data: 
     [ [ 100, 0.09765625 ],
       [ 500, 1.09765625 ],
       [ 1000, 11.08984375 ],
       [ 1500, 17.68359375 ],
       [ 2000, 22.8203125 ] ] },
  { label: 'dst-genny-traceur.js',
    data: 
     [ [ 100, -0.43359375 ],
       [ 500, 1.7265625 ],
       [ 1000, 9.1484375 ],
       [ 1500, 16.30859375 ],
       [ 2000, 15.6796875 ] ] },
  { label: 'dst-qasync-traceur.js',
    data: 
     [ [ 100, 10.33203125 ],
       [ 500, 59.55859375 ],
       [ 1000, 101.76953125 ],
       [ 1500, 135.78125 ],
       [ 2000, 190.25 ] ] },
  { label: 'dst-streamline.js',
    data: 
     [ [ 100, 1.5078125 ],
       [ 500, 3.76953125 ],
       [ 1000, 8.3359375 ],
       [ 1500, 10.25 ],
       [ 2000, 15.890625 ] ] },
  { label: 'dst-suspend-traceur.js',
    data: 
     [ [ 100, -0.30078125 ],
       [ 500, 2.234375 ],
       [ 1000, 6.5390625 ],
       [ 1500, 11.125 ],
       [ 2000, 15.484375 ] ] },
  { label: 'flattened-class.js',
    data: 
     [ [ 100, 0.53125 ],
       [ 500, 3.1171875 ],
       [ 1000, 4.484375 ],
       [ 1500, 7.96484375 ],
       [ 2000, 8.58984375 ] ] },
  { label: 'flattened.js',
    data: 
     [ [ 100, 0.515625 ],
       [ 500, 3.25390625 ],
       [ 1000, 5.3203125 ],
       [ 1500, 8.78515625 ],
       [ 2000, 9.765625 ] ] },
  { label: 'flattened-noclosure.js',
    data: 
     [ [ 100, 0.51953125 ],
       [ 500, 3.03515625 ],
       [ 1000, 4.8984375 ],
       [ 1500, 8.08203125 ],
       [ 2000, 8.87890625 ] ] },
  { label: 'genny.js',
    data: 
     [ [ 100, 1.11328125 ],
       [ 500, 5.48046875 ],
       [ 1000, 11.6953125 ],
       [ 1500, 16.7890625 ],
       [ 2000, 21.2734375 ] ] },
  { label: 'gens.js',
    data: 
     [ [ 100, 0.65625 ],
       [ 500, 3.91015625 ],
       [ 1000, 8.40234375 ],
       [ 1500, 11.921875 ],
       [ 2000, 15.95703125 ] ] },
  { label: 'original.js',
    data: 
     [ [ 100, 0.63671875 ],
       [ 500, 3.1640625 ],
       [ 1000, 5.328125 ],
       [ 1500, 8.69921875 ],
       [ 2000, 9.54296875 ] ] },
  { label: 'promiseish.js',
    data: 
     [ [ 100, 17.9140625 ],
       [ 500, 89.01171875 ],
       [ 1000, 117.94921875 ],
       [ 1500, 129.1640625 ],
       [ 2000, 231.34765625 ] ] },
  { label: 'promiseishQ.js',
    data: 
     [ [ 100, 16.2421875 ],
       [ 500, 77.6953125 ],
       [ 1000, 98.296875 ],
       [ 1500, 135.9609375 ],
       [ 2000, 142.94140625 ] ] },
  { label: 'promises.js',
    data: 
     [ [ 100, 42.97265625 ],
       [ 500, 121.71484375 ],
       [ 1000, 240.53515625 ],
       [ 1500, 359.94921875 ],
       [ 2000, 481.65625 ] ] },
  { label: 'qasync.js',
    data: 
     [ [ 100, 11.796875 ],
       [ 500, 57.00390625 ],
       [ 1000, 97.4765625 ],
       [ 1500, 149.5078125 ],
       [ 2000, 163.91796875 ] ] },
  { label: 'rx.js',
    data: 
     [ [ 100, 3.9375 ],
       [ 500, 21.34375 ],
       [ 1000, 40.43359375 ],
       [ 1500, 62.11328125 ],
       [ 2000, 62.05859375 ] ] },
  { label: 'suspend.js',
    data: 
     [ [ 100, 0.71875 ],
       [ 500, 4.09375 ],
       [ 1000, 8.60546875 ],
       [ 1500, 10.1171875 ],
       [ 2000, 14.28125 ] ] } ]

  .concat(     
  [ { label: 'dst-streamline-fibers.js',
    data: 
     [ [ 100, 1.80078125 ],
       [ 500, 8.53125 ],
       [ 1000, 17.05859375 ],
       [ 1500, 27.98828125 ],
       [ 2000, 34.38671875 ] ] },
  { label: 'fibrous.js',
    data: 
     [ [ 100, 7.05078125 ],
       [ 500, 28.5859375 ],
       [ 1000, 56.75390625 ],
       [ 1500, 84.2734375 ],
       [ 2000, 63.140625 ] ] } ]

       );

window.addEventListener('load', function() {
    $.plot('#perf-mem-1', perfMEM, {legend: { position: 'nw' }, 
yaxis: {min: 0}});
});
</script>

<p>Seems like promises also use a lot of memory, especially the extreme 
implementation <code>promises.js</code>. <code>promiseish.js</code> as well as <code>qasync.js</code> are not
too far behind.</p>
<p><code>fibrous.js</code> and <code>rx.js</code> are somewhat better than the above, however their 
memory usage is still over 5 times bigger than the original.</p>
<p>Lets remove the hogs and see what remains underneath.</p>
<p><div id="perf-mem-2" class="plot">
</div></p>
<script type="text/javascript">
window.addEventListener('load', function() {
    $.plot('#perf-mem-2', perfMEM.filter(function(item) {
        return !/(promises|promiseish|qasync|fibrous|rx.js)/.test(item.label)
    }), {legend: { position: 'nw' }, yaxis: {min: 0}});
});
</script>

<p>Streamline&#39;s fibers implementation uses 35MB while the rest use between
10MB and 25MB.</p>
<p>This is amazing. Generators (without promises) also have a pretty low memory 
overhead, even when compiled with traceur.</p>
<p>Streamline is also quite good in this category. It has very low overhead, both 
in CPU and memory usage. But suspend is comparable even when run non-natively 
with the traceur compiler.</p>
<p>Finally, its important to note that the testing method that I use is not 
statistically sound. Its however good enough to be used to compare orders of
magnitude, which is fine considering the narrowly defined micro-benchmark being
tested.</p>
<p>With that said, here is a table for 1000 parallel requests, 10 ms per I/O 
operation:</p>
<table>
<thead>
<tr>
<th align="left">file</th>
<th align="right">time(ms)</th>
<th align="right">memory(MB)</th>
</tr>
</thead>
<tbody>
<tr>
<td align="left">suspend.js</td>
<td align="right">101</td>
<td align="right">8.62</td>
</tr>
<tr>
<td align="left">flattened-class.js</td>
<td align="right">111</td>
<td align="right">4.48</td>
</tr>
<tr>
<td align="left">flattened-noclosure.js</td>
<td align="right">124</td>
<td align="right">5.04</td>
</tr>
<tr>
<td align="left">flattened.js</td>
<td align="right">125</td>
<td align="right">5.18</td>
</tr>
<tr>
<td align="left">original.js</td>
<td align="right">130</td>
<td align="right">5.33</td>
</tr>
<tr>
<td align="left">dst-streamline.js</td>
<td align="right">139</td>
<td align="right">8.34</td>
</tr>
<tr>
<td align="left">catcher.js</td>
<td align="right">140</td>
<td align="right">6.45</td>
</tr>
<tr>
<td align="left">dst-suspend-traceur.js</td>
<td align="right">142</td>
<td align="right">6.76</td>
</tr>
<tr>
<td align="left">gens.js</td>
<td align="right">149</td>
<td align="right">8.40</td>
</tr>
<tr>
<td align="left">genny.js</td>
<td align="right">161</td>
<td align="right">11.69</td>
</tr>
<tr>
<td align="left">co.js</td>
<td align="right">182</td>
<td align="right">11.14</td>
</tr>
<tr>
<td align="left">dst-genny-traceur.js</td>
<td align="right">250</td>
<td align="right">8.84</td>
</tr>
<tr>
<td align="left">dst-co-traceur.js</td>
<td align="right">284</td>
<td align="right">13.54</td>
</tr>
<tr>
<td align="left">rx.js</td>
<td align="right">295</td>
<td align="right">40.43</td>
</tr>
<tr>
<td align="left">dst-streamline-fibers.js</td>
<td align="right">526</td>
<td align="right">17.05</td>
</tr>
<tr>
<td align="left">promiseish.js</td>
<td align="right">825</td>
<td align="right">117.88</td>
</tr>
<tr>
<td align="left">qasync.js</td>
<td align="right">971</td>
<td align="right">98.39</td>
</tr>
<tr>
<td align="left">promiseishQ.js</td>
<td align="right">1161</td>
<td align="right">96.47</td>
</tr>
<tr>
<td align="left">dst-qasync-traceur.js</td>
<td align="right">1195</td>
<td align="right">112.10</td>
</tr>
<tr>
<td align="left">promises.js</td>
<td align="right">2315</td>
<td align="right">240.39</td>
</tr>
<tr>
<td align="left">fibrous.js</td>
<td align="right">1159</td>
<td align="right">57.48</td>
</tr>
</tbody>
</table>
<p><a name="debuggability"></a></p>
<h3>Debuggability</h3>
<p>Having good performance is important. However, all the performance is worth 
nothing if our code doesn&#39;t do what its supposed to. Debugging is therefore
at least as important as performance.</p>
<p>How can we measure debuggability? We can look at source maps support and
the generated stack traces.</p>
<p><a name="source-maps-support"></a></p>
<h4>Source maps support</h4>
<p>This has a couple of levels itself:</p>
<ul>
<li><p><strong>level 1</strong>: no source maps, but needs them (wacky stack trace line numbers)</p>
</li>
<li><p><strong>level 2</strong>: no source maps and needs them sometimes (to view the original
code)</p>
<p>Streamline used to be in this category but now it does have source maps 
support.</p>
</li>
<li><p><strong>level 3</strong>: has source maps and needs them always.</p>
<p>Nothing is in this category.</p>
</li>
<li><p><strong>level 4</strong>: has source maps and needs them sometimes</p>
<p>Generator libraries are in this category. When compiled with traceur (e.g. 
for the browser) source maps are required and needed. If ES6 is available, 
source maps are unnecessary.</p>
<p>Streamline is also in this category for another reason. With streamline,
you don&#39;t need source maps to get accurate stack traces. However, you will
need them if you want to read the original code (e.g. when debugging in 
the browser).</p>
</li>
<li><p><strong>level 5</strong>: doesn&#39;t need source maps</p>
<p>Everything else is in this category. That&#39;s a bit unfair as fibers will never 
work in a browser.</p>
</li>
</ul>
<p><a name="stack-trace-accuracy"></a></p>
<h4>Stack trace accuracy</h4>
<ul>
<li><p><strong>level 1</strong>: stack traces are missing</p>
<p><code>suspend</code>, <code>co</code> and <code>gens</code> are in this category. When an error happens in one 
of the async functions, this is how the result looks like:  </p>
<pre><code>Error: Error happened
  at null._onTimeout (/home/spion/Documents/tests/async-compare/lib/fakes.js:27:27)
  at Timer.listOnTimeout [as ontimeout] (timers.js:105:15)</code></pre>
<p>No mention of the original file, <code>examples/suspend.js</code></p>
<p>Unfortunately, if you throw an error to a generator using 
<code>iterator.throw(error)</code>, the last yield point will not be present in the 
resulting stack trace. This means you will have no idea which line in your 
generator is the offending one.</p>
<p>Regular exceptions that are not thrown using <code>iterator.throw</code> have complete 
stack traces, so only yield points will suffer.</p>
</li>
<li><p><strong>level 2</strong>: stack traces are never correct</p>
<p>In this category: <code>promiseish.js</code>. Unfortunately if special care is not taken 
to preserve a stack trace, it will not be preserved and the promise library
<code>when</code> doesn&#39;t do that.</p>
</li>
<li><p><strong>level 3</strong>: stack traces are correct with native modules</p>
<p>Bruno Jouhier&#39;s generator based solution <a href="//github.com/bjouhier/galaxy">galaxy</a> 
is in this category. It has a native companion module called 
<a href="//github.com/bjouhier/galaxy-stack">galaxy-stack</a> that implements long stack
traces without a performance penalty. </p>
<p>Note that galaxy-stack doesn&#39;t work with node v0.11.5</p>
</li>
<li><p><strong>level 4</strong>: stack traces are correct with a flag (adding a performance 
penalty).</p>
<p>All Q-based solutions are here, even <code>qasync.js</code>, which uses generators. Q&#39;s
support for stack traces via <code>Q.longStackSupport = true;</code> is good:</p>
<pre><code>Error: Error happened
    at null._onTimeout (/home/spion/Documents/tests/async-compare/lib/fakes.js:27:27)
    at Timer.listOnTimeout [as ontimeout] (timers.js:105:15)
From previous event:
    at /home/spion/Documents/tests/async-compare/examples/qasync.js:41:18
    at GeneratorFunctionPrototype.next (native)</code></pre>
<p>So, does this mean that its possible to add long stack traces support to a 
callbacks-based generator library the way that Q does it? </p>
<p>Yes it does! Genny is in this category too:</p>
<pre><code>Error: Error happened
    at null._onTimeout (/home/spion/Documents/tests/async-compare/lib/fakes.js:27:27)
    at Timer.listOnTimeout [as ontimeout] (timers.js:105:15)
From generator:
    at upload (/home/spion/Documents/tests/async-compare/examples/genny.js:38:35)</code></pre>
<p>However it incurs about 50-70% memory overhead and is about 6 times slower.</p>
<p>Catcher is also in this category, with 100% memory overhead and about 
10 times slower.</p>
</li>
<li><p><strong>level 5</strong>: stack traces are always correct</p>
<p>Streamline and both the original and flattened solutions are in this 
category. Streamline compiles the file in a way that preserves line numbers,
making stack traces correct in all cases. Fibers also include the offending
line in the stack trace.</p>
</li>
</ul>
<p>Ah yes. A table.</p>
<table>
<thead>
<tr>
<th>name</th>
<th align="right">source maps</th>
<th align="right">stack traces</th>
<th align="right">total</th>
</tr>
</thead>
<tbody>
<tr>
<td>original.js</td>
<td align="right">5</td>
<td align="right">5</td>
<td align="right">10</td>
</tr>
<tr>
<td>flattened.js</td>
<td align="right">5</td>
<td align="right">5</td>
<td align="right">10</td>
</tr>
<tr>
<td>fibrous.js</td>
<td align="right">5</td>
<td align="right">5</td>
<td align="right">10</td>
</tr>
<tr>
<td>src-streamline._js</td>
<td align="right">4</td>
<td align="right">5</td>
<td align="right">9</td>
</tr>
<tr>
<td>catcher.js</td>
<td align="right">5</td>
<td align="right">4</td>
<td align="right">9</td>
</tr>
<tr>
<td>promiseishQ.js</td>
<td align="right">5</td>
<td align="right">4</td>
<td align="right">9</td>
</tr>
<tr>
<td>qasync.js</td>
<td align="right">4</td>
<td align="right">4</td>
<td align="right">8</td>
</tr>
<tr>
<td>genny.js</td>
<td align="right">4</td>
<td align="right">4</td>
<td align="right">8</td>
</tr>
<tr>
<td>promiseish.js</td>
<td align="right">5</td>
<td align="right">2</td>
<td align="right">7</td>
</tr>
<tr>
<td>promises.js</td>
<td align="right">5</td>
<td align="right">1</td>
<td align="right">6</td>
</tr>
<tr>
<td>suspend.js</td>
<td align="right">4</td>
<td align="right">1</td>
<td align="right">5</td>
</tr>
<tr>
<td>gens.js</td>
<td align="right">4</td>
<td align="right">1</td>
<td align="right">5</td>
</tr>
<tr>
<td>co.js</td>
<td align="right">4</td>
<td align="right">1</td>
<td align="right">5</td>
</tr>
</tbody>
</table>
<p>Generators are not exactly the best here, but they&#39;re doing well enough thanks
to qasync and genny.</p>
<p>Here is the report from an automated test script that compares the reported 
error line with the actual error line:</p>
<table>
<thead>
<tr>
<th>file</th>
<th align="right">actual line</th>
<th align="right">rep line</th>
<th align="right">distance</th>
</tr>
</thead>
<tbody>
<tr>
<td>catcher.js</td>
<td align="right">38</td>
<td align="right">38</td>
<td align="right">0</td>
</tr>
<tr>
<td>qasync.js</td>
<td align="right">39</td>
<td align="right">39</td>
<td align="right">0</td>
</tr>
<tr>
<td>genny.js</td>
<td align="right">38</td>
<td align="right">38</td>
<td align="right">0</td>
</tr>
<tr>
<td>fibrous.js</td>
<td align="right">38</td>
<td align="right">38</td>
<td align="right">0</td>
</tr>
<tr>
<td>dst-streamline-fibers.js</td>
<td align="right">36</td>
<td align="right">35</td>
<td align="right">1</td>
</tr>
<tr>
<td>dst-streamline.js</td>
<td align="right">37</td>
<td align="right">36</td>
<td align="right">1</td>
</tr>
<tr>
<td>original.js</td>
<td align="right">50</td>
<td align="right">51</td>
<td align="right">1</td>
</tr>
<tr>
<td>flattened.js</td>
<td align="right">61</td>
<td align="right">64</td>
<td align="right">3</td>
</tr>
<tr>
<td>promiseishQ.js</td>
<td align="right">49</td>
<td align="right">52</td>
<td align="right">3</td>
</tr>
</tbody>
</table>
<p><a name="conclusion"></a></p>
<h3>Conclusion</h3>
<p>If this essay left you even more confused than before, you&#39;re not alone. It
seems hard to make a decision even with all the data available.</p>
<p>My opinion is biased. I love generators, and I&#39;ve been 
<a href="https://code.google.com/p/v8/issues/detail?id=2355#c2">pushing</a>
<a href="https://news.ycombinator.com/item?id=5419030">pretty hard</a> to direct the 
attention of V8 developers to them (maybe a bit too hard). And its obvious 
from the analysis above that they have good characteristics: low code 
complexity, good performance, acceptable debuggability.</p>
<p>More importantly, they will eventually become a part of everyday JavaScript 
with no compilation (except for older browsers) or native modules required, 
and the yield keyword is in principle as good indicator of async code as 
callbacks are.</p>
<p>But there are things that cannot be measured. How will the community accept 
generators? Will people find it hard to decide whether to use them or not? Will 
they be frowned upon when used in code published to npm?</p>
<p>I don&#39;t have the answers to these questions. I only have hunches. But they are 
generally positive. Generators will play an important role in the future of 
node.</p>
<hr>
<p>Special thanks to 
<a href="//github.com/Raynos">Raynos</a>, 
<a href="//github.com/maxogden">maxogden</a>, 
<a href="//github.com/mikeal">mikeal</a>
and <a href="//github.com/DamonOehlman">damonoehlman</a>
for their input on the draft version of this essay.</p>
<p>Thanks to <a href="//github.com/jmar777">jmar777</a> for making suspend</p>
<script src="/scripts/jquery.flot.js"></script>
<script src="/scripts/jquery.flot.highlightSeries.js"></script></div>
  <div class="comments"> 
    <hr/><a href="/posts/analysis-generators-and-other-async-patterns-node.html#_comments">comment or share</a>
  </div>
</div>
<div class="post">
  <h1><a href="/posts/make-most-webapps-work-on-ipad.html">Google Docs on the iPad</a></h1>
  <div class="date">Thu Dec 13 2012</div>
  <div class="content"><iframe allowfullscreen="allowfullscreen" frameborder="0" height="360" 
    src="http://www.youtube.com/embed/3fmfbAJcfKY" width="640"></iframe>

<p>This is Appser for Google Docs. It allows you to run the full, web version of Google Docs on the iPad. And by that we mean all of it, even editing presentations (a feature which was recently completely disabled on the iPad by Google)</p>
<p>And its available on the app store, right now, for free. 
<a href="https://itunes.apple.com/us/app/appser-for-google-docs/id577825348?ls=1&amp;mt=8">Get Appser for Google Docs</a>.</p>
<p>At this point you&#39;re probably thinking: Wait, isn&#39;t this horribly problematic? 
Google Docs isn&#39;t optimized for touch input at all - neither the code nor the UI
elements. Selecting text is weird. Scrolling is incompatible with dragging to 
select text or to move stuff. Those image boxes have really tiny resizers. The 
toolbar buttons and menus are not big enough. What about right click?</p>
<p>Is it actually usable?</p>
<p>Also, can I just connect a bigger monitor, some kind of a touchpad and a 
keyboard and forget about touch input?</p>
<p>The answer to all questions is yes.</p>
<h2>Scroll vs drag and text selection</h2>
<p>When Apple first designed the iPhone, it had a 3.5 inch screen with a resolution
of 480x320 pixels. While this was quite a lot compared to other phones of the 
time, it was not even near desktop and laptop displays. Yet it had a browser 
that provided an experience as close to desktop as possible. How did they 
achieve this?</p>
<p>They made scrolling and zooming really easy. This meant that the user was able 
to quickly zoom to compensate for the lacking resolution, then quickly pan to 
show more on the limited screen real-estate. And so, web browsing was very 
usable even on the first iPhone.</p>
<p>Fast forward to today, on the iPad, we have a 10 inch retina-resolution 
display... and the same zoom and pan control.</p>
<p>Why?</p>
<p>Do we really need this? Its very problematic for web apps. They like to assume 
that drag and drop works. They want to display a static chrome around a 
scrolling document, so they also like to assume that overflow: scroll works.</p>
<p>As a result, we decided to make scrolling work by disabling single-finger 
scrolling. Instead we made single-finger actions behave exactly like mouse 
actions do in the browser and switched to two-finger scrolling, making it work 
everywhere. This enables you to quickly select text and drag things around.</p>
<p>We also disabled zoom - web apps usually have their own document zooming 
facilities and the iPad screen has plenty of resolution and size.</p>
<h2>What about tiny buttons and controls?</h2>
<p>This is where things got interesting. We knew that even though we enabled drag and drop, the controls were too tiny for touch use. To overcome this we developed a method called Magnet Touch. It automatically enlarges all touch targets. The enlargement is not visual so it simply makes smaller targets easier to hit. Its also smart - it tries very hard to avoid overlap with other targets. Best of all, you don&#39;t have to tell it anything about the targets - not even which elements are targets.</p>
<h2>And right click?</h2>
<p>Long taps are the equivalent of right clicks. At the moment they trigger when you release the finger.</p>
<h2>What happens when I connect an external screen?</h2>
<p>Your iPad becomes a keyboard + touchpad combo and you get a mouse pointer on the screen. Neat huh? If you prefer a physical Bluetooth keyboard you can of course connect one at any time.</p>
<p>There are some limitations when editing text fields (like the username and password on the login screen) - they must be shown on the iPad temporarily while editing. We&#39;re working on that.</p>
<h2>Conclusion</h2>
<p>What, you&#39;re still reading this? <a href="https://itunes.apple.com/us/app/appser-for-google-docs/id577825348?ls=1&amp;mt=8">Download the app and try it out</a>. We&#39;d also <a href="http://appser.docucalc.com/support">love to hear from you</a>. You can also <a href="http://appser.docucalc.com/">visit the official Appser website</a>, if you&#39;d like to find out more.</p>
</div>
  <div class="comments"> 
    <hr/><a href="/posts/make-most-webapps-work-on-ipad.html#_comments">comment or share</a>
  </div>
</div>
<div class="post">
  <h1><a href="/posts/introducing-npmsearch.html">Introducing npmsearch</a></h1>
  <div class="date">Tue Nov 27 2012</div>
  <div class="content"><p>Node&#39;s package manager <a href="https://npmjs.org/">npm</a> is a wonderful tool.</p>
<p>It handles dependencies and versions the right way. It requires simple, easy to write package metadata. It uses a central registry (by default) which makes installing modules easier. The central registry is <a href="http://couchdb.apache.org/">CouchDB</a> which basically makes it completely transparent and available to everyone.</p>
<p>It does many things right.</p>
<p>But it doesn&#39;t do search that well.</p>
<pre><code>9134 % npm search orm
npm http GET https://registry.npmjs.org/-
/all/since?stale=update_after&amp;amp;startkey=1353539108378
npm http 200 https://registry.npmjs.org/-
/all/since?stale=update_after&amp;amp;startkey=1353539108378

NAME                  DESCRIPTION             
2csv                  A pluggable file format converter into Co...
abnf                  Augmented Backus-Naur Form (ABNF) parsing.
accounting            number, money and currency parsing/formatt..
activerecord          An ORM that supports multiple database sys..
addressit             Freeform Street Address Parser
...
[snip]
...</code></pre>
<p>What just happened here?</p>
<p>Here is what happened: npm search gave us all packages that contain the substring &quot;orm&quot;. Anywhere.</p>
<p>You might argue that this works well with bigger words. Its true that results are slightly better with bigger words but they&#39;re still not sorted in any meaningful way (alphabetically sorting search results isn&#39;t very meaningful)</p>
<pre><code>9144 % npm search mysql
NAME                  DESCRIPTION            
Accessor_MySQL        A MySQL database wrapper, provide ...
any-db                Database-agnostic connection pool ...
autodafe              mvc framework for node with mysql ...
connect-mysql         a MySQL session store for connect ...
connect-mysql-session A MySQL session store for node.js ... 
cormo                 ORM framework for Node.js...
...
[snip]
...</code></pre>
<p>Hence one of the common activities to do when researching node modules is to go to the #node.js IRC channel and ask the people there for a <strong>good</strong> library that does X.</p>
<p>I decided to make a package that helps with this, called npmsearch. Its a command-line tool that allows you to search the npm registry by keywords and it sorts the results using relevance and the number of downloads that the package has.</p>
<p>Install it using npm:</p>
<pre><code>[sudo] npm install -g npmsearch</code></pre>
<p>then use it from the command line:</p>
<pre><code>9147 % npmsearch mysql
* mysql (6 15862)
     A node.js driver for mysql. It is written in JavaScript, does 
     not  require compiling, and is 100% MIT licensed.
     by Felix Geisendrfer &lt;felix@debuggable.com&gt;

* mongoose (2 28197)
     Mongoose MongoDB ODM
     by Guillermo Rauch &lt;guillermo@learnboost.com&gt;
     http://github.com/LearnBoost/mongoose.git

* patio (10 174)
     Patio query engine and ORM
     by Doug Martin &lt;undefined&gt;
     git@github.com:c2fo/patio.git

* mysql-libmysqlclient (5 1019)
     Binary MySQL bindings for Node.JS
     by Oleg Efimov &lt;efimovov@gmail.com&gt;
     https://github.com/Sannis/node-mysql-libmysqlclient.git

* db-mysql (3 918)
     MySQL database bindings for Node.JS

* sql (6 51)
     sql builder
     by brianc &lt;brian.m.carlson@gmail.com&gt;
     http://github.com/brianc/node-sql.git

* sequelize (2 2715)
     Multi dialect ORM for Node.JS
     by Sascha Depold </code></pre>
<p>If you want to try it out without installing it, 
<a href="http://npmsearch.docucalc.com/">you can try it online</a>, or you can 
<a href="https://github.com/spion/npmsearch">visit the project page on github</a></p>
<p>The implemented keyword search is non-trivial: it applies the 
<a href="http://tartarus.org/martin/PorterStemmer/">Porter Stemmer</a> to the keywords and
expands the set provided by you with statistically picked commonly co-occuring 
keywords. (e.g. mongo will expand to mongo mongodb)</p>
<p>Results are sorted by a combined factor which incorporates keyword relevance 
and &quot;half-lifed&quot; downloads. You can control the importance of each factor in 
the sorting process using command-line options - and there are many:</p>
<ul>
<li>relevance - how big of a factor should keyword relevance be, default 2</li>
<li>downloads - how big of a factor is the number of downloads, default 0.25</li>
<li>halflife  - the halflife of downloads e.g. 7 means downloads that are 7 
days old lose half of their value, default 30</li>
<li>limit     - number of results to display, default 7</li>
<li>freshness - update the database if older than &quot;freshness&quot; days, default 1.5</li>
</ul>
<p>I hope this will help fellow nodesters find their next favorite modules</p>
<p>Have fun!</p>
</div>
  <div class="comments"> 
    <hr/><a href="/posts/introducing-npmsearch.html#_comments">comment or share</a>
  </div>
</div>
<div class="post">
  <h1><a href="/posts/fixing-hackernews-mathematical-approach.html">Fixing Hacker News: A mathematical approach</a></h1>
  <div class="date">Tue Aug 21 2012</div>
  <div class="content"><p>There is a certain phenomenon that seems to happen in almost every online 
community of user-generated content. A community is created: the initial users 
define the values of this new community. After a while the community experiences 
growth in numbers. As a result of that growth, users that joined before it feel 
like its no longer the same community with the same values. The latest widely 
discussed example seems to be 
<a href="http://news.ycombinator.com/item?id=4396747">Hacker News</a>.</p>
<p><a href="http://news.ycombinator.com/item?id=4397542">Paul Graham responds</a> that the 
reasons are mostly: a shift in values, increase of anonymity and the fact that
its easier to vote than to contribute content:</p>
<blockquote>
<p><em>It&#39;s a genuine problem and has been growing gradually worse for a while. I think the cause is simply growth. When a good community grows, it becomes worse in two ways: (a) more recent arrivals don&#39;t have as much of whatever quality distinguished the original members, and (b) the large size of the group makes people behave worse, because there is more anonymity in a larger group.</em></p>
<p><em>I&#39;ve spent many hours over the past several years trying to understand and mitigate such problems. I&#39;ve come up with a bunch of tweaks that worked, and I have hopes I&#39;ll be able to come up with more.</em></p>
<p><em>The idea I&#39;m currently investigating, in case anyone is curious, is that 
votes rather than comments may be the easiest place to attack this problem. 
Although snarky comments themselves are the most obvious symptom, I suspect 
that voting is on average dumber than commenting, because it requires so much
less work. So I&#39;m going to try to see if it&#39;s possible to identify people who
consistently upvote nasty comments and if so count their votes less.</em></p>
</blockquote>
<p>As online communities grow, the values of the group shift. The majority now may 
or may not hold the same values as the majority before. The question is, how to
preserve the old values of the group with minimum side-effects? </p>
<p>As it happens, my master&#39;s thesis was an attempt to fix exactly this problem 
mathematically and implement an improved voting system tailored specifically 
for communities with user-submitted content. I won&#39;t provide a link to the 
thesis as its not written in English, but I&#39;ll try to summarize the gist of it.</p>
<p>The voting system used in most communities today (democratic voting) is the one 
most susceptible to value shift when significant growth occurs. Its 
no surprise: democratic systems are designed to measure what the majority 
values. When significant growth occurs, the majority changes and therefore what 
they value also changes.</p>
<p>In contrast, previous moderator/editor based systems offer a strict filter on 
content based on the more static values of the current set of editors. However, 
it has the downside of being limited to what the editors are able to review and 
publish.</p>
<p>I propose a hybrid feedback-loop based system. In this system people have 
variable voting influence and editor-like individuals are given as a 
&quot;reference point&quot; or exemplary users with maximum voting influence. The 
system attempts to find out what they value and recognize it in others.</p>
<p>The system is based on the mathematics described in 
<a href="http://www.unik.no/people/josang/papers/JI2002-Bled.pdf">the beta reputation system</a>,
which is a system for measuring trust in online e-commerce communities.</p>
<p>Here is a short description of the system:</p>
<ul>
<li>Voting influence is not the same for all users: its not 1 (+1 or -1) for 
everyone but in the range 0-1.</li>
<li>When a user votes for a content item, they also vote for the creator (or 
submitter) of the content.</li>
<li>The voting influence of a user is calculated using the positive and negative 
votes that he has received for his submissions.</li>
<li>Exemplary users always have a static maximum influence.</li>
</ul>
<p>Suppose we have a content item \(C\) submitted by the user \(U_c\). Now a voter 
\(V\) comes to vote for it and clicks on the +1 button.</p>
<p>The voter has his own submissions for which he has received a total amount of 
positive vote \(p_V\) and a total amount of negative vote \(n_V\). As a result, 
his voting influence \(i_V\) is modified: its not +1 but calculated according to
the formula:</p>
<p>$$ i_V = f_W(p_V, n_V) $$</p>
<p>where \(f_W\) is the 
<a href="http://evanmiller.org/how-not-to-sort-by-average-rating.html">lower bound of Wilson score confidence interval</a>. 
While a simple average such as:</p>
<p>$$ i_V = \frac{p_V}{p_V + n_V} $$</p>
<p>might work when the number of positive and negative votes is large enough, its 
not good enough when the number of votes is low. The Wilson score confidence 
interval gives us a better, flexible balance between desired certainty in the 
result and the result itself.</p>
<p>This vote in turn is received by the content item \(C\). Because its a positive 
vote, the amount of positive vote \(p_C\) is changed for this content item</p>
<p>$$ p_C \leftarrow p_C + i_V $$</p>
<p>and as a result, it has a new rating</p>
<p>$$ r_c = f_W(p_c, n_c) $$</p>
<p>but the positive points \(p_U\) of the creator of the content item are also 
changed:</p>
<p>$$ p_U \leftarrow p_U + i_V $$</p>
<p>and as a result the voting influence \(i_U\) of submitter is also changed:</p>
<p>$$ i_U = f_W(p_U, n_U) $$</p>
<p>or in other words, he has &quot;earned&quot; a bigger influence in the voting system by
submitting a well-rated content item.</p>
<p>This means that new members have no voting influence. As they submit content and
receive votes their influence may rise if the existing users with high influence 
in the system consider their content to be good.</p>
<p>This is where the reference users \(R\) come in. Their influence is fixed to 
always be 1</p>
<p>$$ i_R = 1 $$</p>
<p>Because of this, influence propagates through the system from them to other 
users who submit content which is deemed high-quality by the reference users. 
Those users in turn also change influence by voting for others and so forth.</p>
<p>Its also possible to scale down votes as they age. The two possible strategies 
are to scale all \(p_X\) and \(n_X\) values daily, for all content items and all
users by multiplying them with a certain aging factor \(k_a\)</p>
<p>$$ p_X \leftarrow k_a p_X $$</p>
<p>$$ n_X \leftarrow k_a n_X $$</p>
<p>or to simply keep all positive and negative votes \(V_p\) and \(V_n\) in the 
database and recalculate \(p_X\) and \(n_X\) according to the age of the votes 
\(a_V\), for example:</p>
<p>$$ p_X = \sum_{\forall V_p} { i_V k_a^{a_V} } $$</p>
<p>$$ n_X = \sum_{\forall V_n} { i_V k_a^{a_V} } $$</p>
<p>One of the convenient aspects of this system is that its easy to test-drive. It 
doesn&#39;t require more user action than simple democratic voting. It only requires
an administrator to specify some reference users at the start which seed and 
then propagate influence throughout the system.</p>
<p>I tested this system on a forum dataset (details available on request) and found
that the system achieves around 50% reduction of difference from a moderator 
only system compared to scores of a democratic system, even when the direct 
voting of reference users is turned off for content items and only the indirect 
(to other users) influence is counted. \((p &lt; 0.05)\)</p>
<p>What does a 50% reduction in the difference mean? Let the score of a content 
item \(C\) be measured in 3 systems: democratic \(D\), reference-users-only 
\(R\) and hybrid \(H\) with direct influence of reference users to content items
being turned off. By sorting the items according to those scores we can 
calculate their ranks in the 3 systems: \(r_D\), \(r_R\) and \(r_H\) 
respectively. The value of the rank is in the range \(1\) to \(n\), where \(n\) 
is total number of content items. The absolute difference between the democratic
ranking and the reference ranking \(d_{DR}\) is:</p>
<p>$$ d_{DR} = abs(r_D - r_R) $$</p>
<p>while the absolute difference between the hybrid ranking and the reference 
ranking \(d_{HR}\) is:</p>
<p>$$ d_{HR} = abs(r_H - r_R) $$</p>
<p>and it turns out that on average,</p>
<p>$$ d_{HR} = 0.5 d_{DR} $$</p>
<p>The important downside of these results is that the people using the system were
not aware that points are calculated in a different way. The original votes were
given by people who knew that the system is democratic and acted accordingly. It
remains to be seen what the results would be if people are aware that their 
voting influence depends on the way others vote for their submitted content.</p>
<p>I pondered starting a website similar to hacker news based on this voting and 
scoring scheme, however starting a whole new news website is about much more 
than just scoring algorithms (it requires reputation in the online comminty, 
popularity and most importantly time, none of which I presently have in 
sufficient amounts or know how to achieve). But hopefully, pg and the rest of 
the hacker news team might find this scheme useful enough to somehow incorporate
it into the existing scoring system.</p>
</div>
  <div class="comments"> 
    <hr/><a href="/posts/fixing-hackernews-mathematical-approach.html#_comments">comment or share</a>
  </div>
</div>
<hr/>
<h1>Older posts</h1>
<h4 class="post"><a href="/posts/let-it-snow.html">Let it snow</a></h4>
<h4 class="post"><a href="/posts/why-native-sucks-html5-rocks-porting-windows-8.html">Why native sucks and HTML5 rocks: porting</a></h4>
<h4 class="post"><a href="/posts/intuitive-javascript-array-filtering-function-pt2.html">Intuitive JavaScript array filtering function pt2</a></h4>
<h4 class="post"><a href="/posts/intuitive-javascript-array-filtering-function.html">Intuitive JavaScript array filtering function pt1</a></h4>
<h4 class="post"><a href="/posts/amateur-lasse-gjertsen.html">Amateur - Lasse Gjertsen</a></h4></div>
    </div>
    <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script src="http://google-code-prettify.googlecode.com/svn/trunk/src/prettify.js"></script>
    <script>
      $('code').addClass('prettyprint');
      prettyPrint();
    </script>
    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
      
      ga('create', 'UA-42872676-1', 'spion.github.io');
      ga('send', 'pageview');
      
    </script>
  </body>
</html>